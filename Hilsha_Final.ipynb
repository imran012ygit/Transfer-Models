{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imran012x/Transfer-Models/blob/main/Hilsha_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 1: Colab-Setup\n"
      ],
      "metadata": {
        "id": "k1SaijPsU_az"
      },
      "id": "k1SaijPsU_az"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive_path = '/content/drive'\n",
        "\n",
        "if os.path.exists(drive_path) and os.path.ismount(drive_path):\n",
        "    print(\"Google Drive is already connected ‚úÖ\")\n",
        "else:\n",
        "    drive.mount(drive_path)\n",
        "    print(\"Google Drive connection done ‚úÖ\")\n",
        "\n",
        "\n",
        "# # Upload a file\n",
        "# uploaded = files.upload()\n",
        "# # Get the file name\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "# print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "\n",
        "# import zipfile\n",
        "# import os\n",
        "# # with zipfile.ZipFile('/content/drive/MyDrive/Hilsha/data_fish_224_11k.zip', 'r') as zip_ref:\n",
        "# #     zip_ref.extractall('')\n",
        "# with zipfile.ZipFile('/content/drive/MyDrive/Hilsha/data_fish_org_8407.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo8MKzRhJHqC",
        "outputId": "562bac9e-5c05-4963-8ece-e5a9610fec48"
      },
      "id": "Bo8MKzRhJHqC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive is already connected ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 2: Import & Config & Env Setup"
      ],
      "metadata": {
        "id": "HiFbgREXbGzn"
      },
      "id": "HiFbgREXbGzn"
    },
    {
      "cell_type": "code",
      "source": [
        "#1. IMPORTS AND INITIAL SETUP\n",
        "# ================================================================================================================================\n",
        "# Purpose: Import all required libraries and set up warnings to suppress unnecessary messages.\n",
        "\n",
        "\n",
        "\n",
        "!pip install pytorch-gradcam optuna captum -q  # Uncomment if running in a new environment\n",
        "\n",
        "\n",
        "import sys\n",
        "import numpy\n",
        "import pandas\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"python_version: {sys.version.split()[0]}\")\n",
        "print(f\"numpy_version: {numpy.__version__}\")\n",
        "print(f\"pandas_version: {pandas.__version__}\")\n",
        "print(f\"seaborn_version: {sns.__version__}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Standard Library\n",
        "# ============================================================\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import zipfile\n",
        "import logging\n",
        "import random\n",
        "import warnings\n",
        "import traceback\n",
        "import logging\n",
        "import subprocess\n",
        "import threading\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "from threading import Lock\n",
        "import multiprocessing as mp\n",
        "from itertools import combinations\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Tuple, Dict, Any, Optional\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ============================================================\n",
        "# Data Handling & Utilities\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# Visualization\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# ============================================================\n",
        "# System & Resource Monitoring\n",
        "# ============================================================\n",
        "import psutil\n",
        "import pynvml\n",
        "\n",
        "# ============================================================\n",
        "# Machine Learning\n",
        "# ============================================================\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, f1_score, accuracy_score,\n",
        "    precision_score, recall_score, roc_curve, auc\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Imbalanced data handling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ============================================================\n",
        "# Deep Learning - PyTorch\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# ============================================================\n",
        "# Augmentation\n",
        "# ============================================================\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Explainable AI (XAI)\n",
        "# ============================================================\n",
        "\n",
        "import torch.autograd as autograd\n",
        "from captum.attr import LRP\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "import optuna.logging\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Hyperparameter Optimization\n",
        "# ============================================================\n",
        "try:\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Warning: Optuna not available. Using default hyperparameters.\")\n",
        "\n",
        "\n",
        "\n",
        "#For DeprecationWarning / FutureWarning specifically:\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
        "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Hide all pip warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "# ---\n",
        "# 2. CONFIGURATION\n",
        "# ================================================================================================================================\n",
        "# Purpose: Define configuration settings and initialize the environment.\n",
        "\n",
        "class Config:\n",
        "\n",
        "\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/Hilsha'\n",
        "\n",
        "    # Dataset parameters\n",
        "    NUM_CLASSES = 5\n",
        "    CLASS_NAMES = ['Ilish', 'Chandana', 'Sardin', 'Sardinella', 'Punctatus']\n",
        "    INPUT_SIZE = 224\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32 #Will Change Dynamically\n",
        "    DATALOADER_NUM_WORKERS = 1 #Will Change Dynamically\n",
        "    # Dynamically adjust batch size and workers\n",
        "    EPOCHS = 40\n",
        "    PIN_MEMORY = True\n",
        "    USE_MIXED_PRECISION = True #True\n",
        "    COMPILE_MODEL = True\n",
        "    PATIENCE = 4\n",
        "    LEARNING_RATE = 1e-5\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    OPTUNA_TRIALS = 100\n",
        "    OPTUNA_EPOCHS = 10\n",
        "\n",
        "    # Models to train\n",
        "    # MODELS = ['resnet50','efficientnet_b0','mobilenet_v3_large','vgg16', 'densenet121']\n",
        "    MODELS = [\n",
        "        'resnet50',\n",
        "        'efficientnet_b0'\n",
        "        # # 'mobilenet_v3_large',\n",
        "        # 'vgg16',\n",
        "        # 'densenet121',\n",
        "        # 'inception_v3',\n",
        "        # 'vit_b_16',\n",
        "        # 'convnext_base',\n",
        "        # 'regnet_y_32gf'\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Ensemble methods\n",
        "    ENSEMBLE_METHODS = ['simple_average', 'weighted_average', 'confidence_based', 'learnable_weighted']\n",
        "\n",
        "    # Device\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    SEED = 42\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup random seeds, directories, and dynamically adjust batch size and workers\"\"\"\n",
        "\n",
        "    os.environ['PYTHONHASHSEED'] = str(Config.SEED)  # For hash seed reproducibility\n",
        "    random.seed(Config.SEED)\n",
        "    np.random.seed(Config.SEED)\n",
        "    torch.manual_seed(Config.SEED)\n",
        "    torch.cuda.manual_seed_all(Config.SEED)  # For multi-GPU if applicable\n",
        "    #Guard for GPU determinism (optional, but helpful if you want exact reproducibility across runs):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "    torch.manual_seed(Config.SEED)\n",
        "    np.random.seed(Config.SEED)\n",
        "\n",
        "    directories = [\n",
        "        Config.OUTPUT_DIR,\n",
        "        f\"{Config.OUTPUT_DIR}/models\",\n",
        "        f\"{Config.OUTPUT_DIR}/visualizations\",\n",
        "        f\"{Config.OUTPUT_DIR}/reports\",\n",
        "        f\"{Config.OUTPUT_DIR}/xai_visualizations\"\n",
        "    ]\n",
        "\n",
        "    for directory in directories:\n",
        "        Path(directory).mkdir(parents=True, exist_ok=True)#With exist_ok=True:Python will not raise an error if already exists.Or else raise a FileExistsError\n",
        "        #& parents=True ‚Üí creates all missing parent directories in the path.\n",
        "\n",
        "    # Ensure all output directories exist\n",
        "    os.makedirs(f\"{Config.OUTPUT_DIR}/best_model\", exist_ok=True)\n",
        "    os.makedirs(f\"{Config.OUTPUT_DIR}/model_results\", exist_ok=True)\n",
        "    os.makedirs(f\"{Config.OUTPUT_DIR}/kfold_results\", exist_ok=True)\n",
        "    os.makedirs(f\"{Config.OUTPUT_DIR}/visualizations\", exist_ok=True)\n",
        "\n",
        "    print(f\"Using device: {Config.DEVICE}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"Dynamic BATCH_SIZE: {Config.BATCH_SIZE}, DATALOADER_NUM_WORKERS: {Config.DATALOADER_NUM_WORKERS}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    seed = Config.SEED + worker_id\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tVhsrE2bL5w",
        "outputId": "7c2f9fd9-bebf-4ddf-8b0e-3f8230b57f10"
      },
      "id": "5tVhsrE2bL5w",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python_version: 3.12.11\n",
            "numpy_version: 1.26.4\n",
            "pandas_version: 2.2.2\n",
            "seaborn_version: 0.13.2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 3: Pre-processing & Save"
      ],
      "metadata": {
        "id": "M9g_LvnMZ1Rs"
      },
      "id": "M9g_LvnMZ1Rs"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Check GPU availability\n",
        "# print(\"GPU Available:\", torch.cuda.is_available())\n",
        "# print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n",
        "\n",
        "# # Define fish classes and dataset paths\n",
        "# fish_classes = ['ilish', 'chandana', 'sardin', 'sardinella', 'punctatus'] #0,1,2,3,4\n",
        "# zipfile.ZipFile('/content/drive/MyDrive/Hilsha/data_fish_org_8407.zip').extractall('/content/.hidden_fish')\n",
        "# data_dir = '/content/.hidden_fish'\n",
        "\n",
        "# image_limits = {\n",
        "#     'ilish': 3000,\n",
        "#     'chandana': 1185,\n",
        "#     'sardin': 2899,\n",
        "#     'sardinella': 370,\n",
        "#     'punctatus': 953\n",
        "# }\n",
        "\n",
        "# # Settings\n",
        "# total_images = sum(image_limits.values())\n",
        "# batch_size = 100\n",
        "# num_threads = 4\n",
        "\n",
        "\n",
        "# # Output paths\n",
        "# output_dir = '/content/drive/MyDrive/Hilsha'\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# labels_file = os.path.join(output_dir, 'Y_labels.npy')\n",
        "# xdata_file = os.path.join(output_dir, 'X_data.npy')\n",
        "\n",
        "# save_lock = threading.Lock()  # for thread-safe writes -> Prevents race conditions when multiple threads write to the same list.\n",
        "\n",
        "# # Function to gather image paths\n",
        "# def get_image_paths(class_name, max_images):\n",
        "#     path = os.path.join(data_dir, class_name)\n",
        "#     files = sorted(os.listdir(path))\n",
        "#     random.shuffle(files)\n",
        "#     return [os.path.join(path, f) for f in files[:max_images]]\n",
        "\n",
        "# # Load and preprocess batch\n",
        "# def load_and_preprocess_batch(image_paths, start_idx, batch_size, class_idx):\n",
        "#     end_idx = min(start_idx + batch_size, len(image_paths))\n",
        "#     batch_paths = image_paths[start_idx:end_idx]\n",
        "#     batch_images = []\n",
        "\n",
        "#     for img_path in batch_paths:\n",
        "#         img = Image.open(img_path).resize((224, 224)).convert('RGB')\n",
        "#         img_tensor = torch.tensor(np.array(img), dtype=torch.uint8).permute(2, 0, 1)  # C x H x W\n",
        "#         batch_images.append(img_tensor)\n",
        "\n",
        "#     batch_tensor = torch.stack(batch_images)  # B x C x H x W\n",
        "#     batch_labels = np.full((len(batch_images),), class_idx, dtype=np.int32)\n",
        "#     return batch_tensor, batch_labels\n",
        "\n",
        "# # Process one batch and return tensors & labels (no file saving)\n",
        "# def process_batch(image_paths, start_idx, batch_size, class_idx):\n",
        "#     return load_and_preprocess_batch(image_paths, start_idx, batch_size, class_idx)\n",
        "\n",
        "# def preprocess_and_save_all(overwrite=True):\n",
        "#     if os.path.exists(labels_file) and os.path.exists(xdata_file) and not overwrite:\n",
        "#         print(\"Preprocessed data already exists. Set overwrite=True to reprocess.\")\n",
        "#         return\n",
        "\n",
        "#     all_images = []\n",
        "#     all_labels = []\n",
        "#     processed_count = 0\n",
        "\n",
        "#     for idx, class_name in enumerate(fish_classes):\n",
        "#         print(f\"\\nProcessing class: {class_name}\")\n",
        "#         image_paths = get_image_paths(class_name, image_limits[class_name])\n",
        "#         total_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
        "#         #It ensures ceiling division ‚Äî rounding up, not down.\n",
        "#         # Normal division: 103 / 20 = 5.15 ‚Üí floor division // 20 = 5 (‚ùå missing last 3 images)\n",
        "#         # This trick: (103 + 20 - 1) // 20 = 122 // 20 = 6 ‚úÖ\n",
        "\n",
        "#         with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "#             futures = []\n",
        "#             for start in range(0, len(image_paths), batch_size):\n",
        "#                 futures.append(executor.submit(process_batch, image_paths, start, batch_size, idx))\n",
        "\n",
        "#             for future in tqdm(as_completed(futures), total=total_batches, desc=class_name):#taqaddum (ÿ™ŸÇÿØŸëŸÖ) ‚Äì Arabic for \"progress\".\n",
        "#                 # futures: List of tasks (from ThreadPoolExecutor or ProcessPoolExecutor).\n",
        "#                 # as_completed(futures): Yields each future as it finishes (not in order).\n",
        "\n",
        "#                 batch_tensor, batch_labels = future.result()\n",
        "#                 with save_lock: #Locks this section so that only one thread can update the shared lists safely.\n",
        "#                     all_images.append(batch_tensor)\n",
        "#                     all_labels.append(batch_labels)\n",
        "#                     processed_count += batch_tensor.size(0)\n",
        "#                     print(f\"Processed batch with {batch_tensor.size(0)} images, total processed: {processed_count}/{total_images}\")\n",
        "#                 gc.collect()\n",
        "\n",
        "#     # Combine all tensors and labels\n",
        "#     X = torch.cat(all_images, dim=0).numpy()\n",
        "#     Y = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "#     # Save final arrays\n",
        "#     np.save(xdata_file, X, allow_pickle=False)#Malicious .npy -> import os;os.system(\"rm -rf /\")  # ‚Üê Dangerous command\n",
        "#     np.save(labels_file, Y, allow_pickle=False)\n",
        "\n",
        "#     print(f\"\\n‚úÖ Done! Saved {processed_count} images in {xdata_file}\")\n",
        "#     print(f\"X_data shape: {X.shape}, Y_labels shape: {Y.shape}\")\n",
        "\n",
        "#     if processed_count != total_images:\n",
        "#         raise ValueError(f\"Expected {total_images} images, but processed {processed_count}\")\n",
        "\n",
        "# # Run preprocessing and save directly to X_data.npy and Y_labels.npy\n",
        "# preprocess_and_save_all(overwrite=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "\n",
        "        self.images = self._preprocess_images(images)\n",
        "        self.labels = labels.astype(np.int64)\n",
        "        self.transform = transform #Here means: Medium,Heavy or Any\n",
        "\n",
        "    def _preprocess_images(self, images):\n",
        "        \"\"\"Preprocess images to ensure proper format and normalization\"\"\"\n",
        "        if images.max() > 1.5: #üëâ The threshold 1.5 is just a safe cutoff to distinguish between the two cases.\n",
        "            #Because some normalized images can have values slightly above 1.0 (e.g., after augmentations, rounding, or scaling bugs).\n",
        "            images = images.astype(np.float32) / 255.0\n",
        "\n",
        "        if len(images.shape) == 4 and images.shape[1] == 3: #If input is (batch, channels, height, width) ‚Üí convert to (batch, height, width, channels) (common for TensorFlow).\n",
        "            images = np.transpose(images, (0, 2, 3, 1))\n",
        "        return images.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:  #Applies an Albumentations transform pipeline (it returns a dict, so you take ['image']).\n",
        "            image = self.transform(image=image)['image'] #\n",
        "        else:\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        #With transform ‚Üí advanced augmentations.\n",
        "        #Without transform ‚Üí just convert to PyTorch format.\n",
        "\n",
        "\n",
        "        # Convert label to plain Python int to avoid CUDA tensor creation in workers.That wastes memory and slows down training.\n",
        "        if isinstance(label, np.ndarray):\n",
        "            label = int(label.item())\n",
        "        elif hasattr(label, 'item'):\n",
        "            label = int(label.item())\n",
        "        else:\n",
        "            label = int(label)\n",
        "\n",
        "\n",
        "        return image, label  # Plain Python int, not torch.tensor\n",
        "        # return image, torch.tensor(int(label), dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     image = self.images[idx]  # H x W x C\n",
        "    #     label = self.labels[idx]\n",
        "\n",
        "    #     # Ensure image has 3 channels\n",
        "    #     if image.ndim == 2:  # grayscale H x W\n",
        "    #         image = np.stack([image]*3, axis=-1)\n",
        "    #     elif image.shape[-1] == 4:  # RGBA\n",
        "    #         image = image[:, :, :3]\n",
        "\n",
        "    #     # Apply Albumentations transform if any\n",
        "    #     if self.transform:\n",
        "    #         image = self.transform(image=image)['image']  # may already be tensor\n",
        "\n",
        "    #     # Convert to PyTorch tensor C x H x W if it's a numpy array\n",
        "    #     if isinstance(image, np.ndarray):\n",
        "    #         image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
        "    #     elif isinstance(image, torch.Tensor) and image.ndim == 3 and image.shape[0] != 3:\n",
        "    #         # If transform returns H x W x C tensor, permute to C x H x W\n",
        "    #         image = image.permute(2, 0, 1).float()\n",
        "    #     # else assume it's already C x H x W\n",
        "\n",
        "    #     # Convert label to tensor\n",
        "    #     label = int(label) if not isinstance(label, torch.Tensor) else label.long()\n",
        "\n",
        "    #     return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # class MyClass:\n",
        "        #     def greet(self):\n",
        "        #         print(\"Hello!\")\n",
        "        # obj = MyClass()\n",
        "        # print(hasattr(obj, 'greet'))   # True, because obj has a method greet\n",
        "        # print(hasattr(obj, 'name'))    # False, no attribute called name\n",
        "        # # Using hasattr with .item()\n",
        "        # import torch\n",
        "        # x = torch.tensor(5)  # scalar tensor\n",
        "        # print(hasattr(x, 'item'))      # True\n",
        "        # print(x.item())                # 5\n",
        "\n",
        "        # return image, torch.tensor(label, dtype=torch.long)  # <-- ensure label is tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataManager:\n",
        "    @staticmethod  #In Python, @staticmethod is used to define a method that belongs to a class but doesn‚Äôt access self or cls.\n",
        "\n",
        "    # class DataManager:\n",
        "    # staticmethod\n",
        "    # def greet(name):\n",
        "    #     return f\"Hello, {name}!\"\n",
        "    # # Call without creating an instance\n",
        "    # print(DataManager.greet(\"Imran\"))  # Output: Hello, Imran!\n",
        "    # # Call with an instance\n",
        "    # dm = DataManager()\n",
        "    # print(dm.greet(\"Imran\"))           # Output: Hello, Imran!\n",
        "\n",
        "    # class MyClass:\n",
        "    #     count = 0\n",
        "\n",
        "    #     staticmethod\n",
        "    #     def greet(name):\n",
        "    #         return f\"Hello, {name}!\"\n",
        "\n",
        "    #     classmethod\n",
        "    #     def increment_count(cls):\n",
        "    #         cls.count += 1\n",
        "    #         return cls.count\n",
        "\n",
        "    # # Static method\n",
        "    # print(MyClass.greet(\"Imran\"))      # Hello, Imran!\n",
        "    # # Class method\n",
        "    # print(MyClass.increment_count())   # 1\n",
        "    # print(MyClass.increment_count())   # 2\n",
        "    #Static method ‚Üí independent of class/instance.\n",
        "    #Class method ‚Üí works with the class itself (cls), can modify class variables.\n",
        "\n",
        "\n",
        "    def get_transforms(is_training=True, augmentation_strength='medium'):\n",
        "        \"\"\"Get data transforms with configurable augmentation strength\"\"\"\n",
        "        if is_training:\n",
        "            if augmentation_strength == 'light':\n",
        "                return A.Compose([\n",
        "                    A.Resize(Config.INPUT_SIZE, Config.INPUT_SIZE),\n",
        "                    A.HorizontalFlip(p=0.3),\n",
        "                    A.RandomRotate90(p=0.3),\n",
        "                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "            elif augmentation_strength == 'heavy':\n",
        "                return A.Compose([\n",
        "                    A.Resize(Config.INPUT_SIZE, Config.INPUT_SIZE),\n",
        "                    A.HorizontalFlip(p=0.7),\n",
        "                    A.VerticalFlip(p=0.5),\n",
        "                    A.RandomRotate90(p=0.7),\n",
        "                    # A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, p=0.8),\n",
        "                    # A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.8),\n",
        "                    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "                    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
        "                    # A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=40, val_shift_limit=30, p=0.7),\n",
        "                    A.GaussianBlur(blur_limit=(3, 9), p=0.5),\n",
        "                    A.GaussNoise(var_limit=(10.0, 80.0), p=0.4),\n",
        "                    A.CoarseDropout(max_holes=12, max_height=25, max_width=25, p=0.5),\n",
        "                    A.ElasticTransform(p=0.3),\n",
        "                    A.GridDistortion(p=0.3),\n",
        "                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "                    A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.3),\n",
        "                    A.RandomRain(blur_value=3, p=0.2),\n",
        "                    A.ColorJitter(hue=0.1, p=0.5),\n",
        "\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "            else:  # medium\n",
        "                return A.Compose([\n",
        "                    A.Resize(Config.INPUT_SIZE, Config.INPUT_SIZE),\n",
        "                    A.HorizontalFlip(p=0.5),\n",
        "                    A.VerticalFlip(p=0.3),\n",
        "                    A.RandomRotate90(p=0.5),\n",
        "                    # A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.7),\n",
        "                    # A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n",
        "                    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.1),\n",
        "                    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
        "                    # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.6),\n",
        "                    A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
        "                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "                    A.CoarseDropout(max_holes=8, max_height=20, max_width=20, p=0.4),\n",
        "                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "        else:\n",
        "            return A.Compose([\n",
        "                A.Resize(Config.INPUT_SIZE, Config.INPUT_SIZE),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_and_balance_data():\n",
        "        \"\"\"Load data and apply SMOTE\"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # # Check GPU availability\n",
        "        # print(\"GPU Available:\", torch.cuda.is_available())\n",
        "        # print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n",
        "\n",
        "        # # Define fish classes and dataset paths\n",
        "        # fish_classes = ['ilish', 'chandana', 'sardin', 'sardinella', 'punctatus'] #0,1,2,3,4\n",
        "        # zipfile.ZipFile('/content/drive/MyDrive/Hilsha/data_fish_org_8407.zip').extractall('/content/.hidden_fish')\n",
        "        # data_dir = '/content/.hidden_fish'\n",
        "\n",
        "        # image_limits = {\n",
        "        #     'ilish': 3000,\n",
        "        #     'chandana': 1185,\n",
        "        #     'sardin': 2899,\n",
        "        #     'sardinella': 370,\n",
        "        #     'punctatus': 953\n",
        "        # }\n",
        "\n",
        "        # # Settings\n",
        "        # total_images = sum(image_limits.values())\n",
        "        # batch_size = 100\n",
        "        # num_threads = 4\n",
        "\n",
        "\n",
        "        # # Output paths\n",
        "        # output_dir = '/content/drive/MyDrive/Hilsha'\n",
        "        # os.makedirs(output_dir, exist_ok=True)\n",
        "        # labels_file = os.path.join(output_dir, 'Y_labels.npy')\n",
        "        # xdata_file = os.path.join(output_dir, 'X_data.npy')\n",
        "\n",
        "        # save_lock = threading.Lock()  # for thread-safe writes -> Prevents race conditions when multiple threads write to the same list.\n",
        "\n",
        "        # # Function to gather image paths\n",
        "        # def get_image_paths(class_name, max_images):\n",
        "        #     path = os.path.join(data_dir, class_name)\n",
        "        #     files = sorted(os.listdir(path))\n",
        "        #     random.shuffle(files)\n",
        "        #     return [os.path.join(path, f) for f in files[:max_images]]\n",
        "\n",
        "        # # Load and preprocess batch\n",
        "        # def load_and_preprocess_batch(image_paths, start_idx, batch_size, class_idx):\n",
        "        #     end_idx = min(start_idx + batch_size, len(image_paths))\n",
        "        #     batch_paths = image_paths[start_idx:end_idx]\n",
        "        #     batch_images = []\n",
        "\n",
        "        #     for img_path in batch_paths:\n",
        "        #         img = Image.open(img_path).resize((224, 224)).convert('RGB')\n",
        "        #         img_tensor = torch.tensor(np.array(img), dtype=torch.uint8).permute(2, 0, 1)  # C x H x W\n",
        "        #         batch_images.append(img_tensor)\n",
        "\n",
        "        #     batch_tensor = torch.stack(batch_images)  # B x C x H x W\n",
        "        #     batch_labels = np.full((len(batch_images),), class_idx, dtype=np.int32)\n",
        "        #     return batch_tensor, batch_labels\n",
        "\n",
        "        # # Process one batch and return tensors & labels (no file saving)\n",
        "        # def process_batch(image_paths, start_idx, batch_size, class_idx):\n",
        "        #     return load_and_preprocess_batch(image_paths, start_idx, batch_size, class_idx)\n",
        "\n",
        "        # def preprocess_and_save_all(overwrite=True):\n",
        "        #     if os.path.exists(labels_file) and os.path.exists(xdata_file) and not overwrite:\n",
        "        #         print(\"Preprocessed data already exists. Set overwrite=True to reprocess.\")\n",
        "        #         return\n",
        "\n",
        "        #     all_images = []\n",
        "        #     all_labels = []\n",
        "        #     processed_count = 0\n",
        "\n",
        "        #     for idx, class_name in enumerate(fish_classes):\n",
        "        #         print(f\"\\nProcessing class: {class_name}\")\n",
        "        #         image_paths = get_image_paths(class_name, image_limits[class_name])\n",
        "        #         total_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
        "        #         #It ensures ceiling division ‚Äî rounding up, not down.\n",
        "        #         # Normal division: 103 / 20 = 5.15 ‚Üí floor division // 20 = 5 (‚ùå missing last 3 images)\n",
        "        #         # This trick: (103 + 20 - 1) // 20 = 122 // 20 = 6 ‚úÖ\n",
        "\n",
        "        #         with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        #             futures = []\n",
        "        #             for start in range(0, len(image_paths), batch_size):\n",
        "        #                 futures.append(executor.submit(process_batch, image_paths, start, batch_size, idx))\n",
        "\n",
        "        #             for future in tqdm(as_completed(futures), total=total_batches, desc=class_name):#taqaddum (ÿ™ŸÇÿØŸëŸÖ) ‚Äì Arabic for \"progress\".\n",
        "        #                 # futures: List of tasks (from ThreadPoolExecutor or ProcessPoolExecutor).\n",
        "        #                 # as_completed(futures): Yields each future as it finishes (not in order).\n",
        "\n",
        "        #                 batch_tensor, batch_labels = future.result()\n",
        "        #                 with save_lock: #Locks this section so that only one thread can update the shared lists safely.\n",
        "        #                     all_images.append(batch_tensor)\n",
        "        #                     all_labels.append(batch_labels)\n",
        "        #                     processed_count += batch_tensor.size(0)\n",
        "        #                     print(f\"Processed batch with {batch_tensor.size(0)} images, total processed: {processed_count}/{total_images}\")\n",
        "        #                 gc.collect()\n",
        "\n",
        "        #     # Combine all tensors and labels\n",
        "        #     X = torch.cat(all_images, dim=0).numpy()\n",
        "        #     Y = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        #     # Save final arrays\n",
        "        #     np.save(xdata_file, X, allow_pickle=False)#Malicious .npy -> import os;os.system(\"rm -rf /\")  # ‚Üê Dangerous command\n",
        "        #     np.save(labels_file, Y, allow_pickle=False)\n",
        "\n",
        "        #     print(f\"\\n‚úÖ Done! Saved {processed_count} images in {xdata_file}\")\n",
        "        #     print(f\"X_data shape: {X.shape}, Y_labels shape: {Y.shape}\")\n",
        "\n",
        "        #     if processed_count != total_images:\n",
        "        #         raise ValueError(f\"Expected {total_images} images, but processed {processed_count}\")\n",
        "\n",
        "        # # Run preprocessing and save directly to X_data.npy and Y_labels.npy\n",
        "        # preprocess_and_save_all(overwrite=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # X = np.load(Config.DATA_FILE)\n",
        "        # Y = np.load(Config.LABELS_FILE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # # Your data path\n",
        "        # output_dir = '/content/drive/MyDrive/Hilsha'\n",
        "        # data_file = os.path.join(output_dir, 'X_data.npy')\n",
        "        # labels_file = os.path.join(output_dir, 'Y_labels.npy')\n",
        "\n",
        "        # # Readable size format\n",
        "        # def sizeof_fmt(num, suffix='B'):\n",
        "        #     for unit in ['', 'K', 'M', 'G', 'T']:\n",
        "        #         if abs(num) < 1024.0:\n",
        "        #             return f\"{num:3.2f} {unit}{suffix}\"\n",
        "        #         num /= 1024.0\n",
        "        #     return f\"{num:.2f} T{suffix}\"\n",
        "\n",
        "        # # Main loader\n",
        "        # def load_preprocessed_data(as_torch=True, normalize=True, to_device=None):\n",
        "        #     # Check file existence #cpu,cuda (CUDA stands for Compute Unified Device Architecture.)\n",
        "        #     for path in [data_file, labels_file]:\n",
        "        #         if not os.path.exists(path):\n",
        "        #             raise FileNotFoundError(f\"Missing: {path}\")\n",
        "\n",
        "        #     # Print file sizes\n",
        "        #     print(f\"üìÅ X_data.npy: {sizeof_fmt(os.path.getsize(data_file))}\")\n",
        "        #     print(f\"üìÅ Y_labels.npy: {sizeof_fmt(os.path.getsize(labels_file))}\")\n",
        "\n",
        "        #     # Load with mmap\n",
        "        #     X = np.load(data_file, mmap_mode='r')\n",
        "        #     Y = np.load(labels_file, mmap_mode='r')\n",
        "\n",
        "        #     print(f\"‚úÖ X shape: {X.shape}, dtype: {X.dtype}\")\n",
        "        #     print(f\"‚úÖ Y shape: {Y.shape}, dtype: {Y.dtype}\")\n",
        "\n",
        "        #     # Sanity check\n",
        "        #     if len(X) != len(Y):\n",
        "        #         raise ValueError(\"Mismatch between number of samples in X and Y\")\n",
        "\n",
        "        #     # Convert to torch\n",
        "        #     if as_torch:\n",
        "        #         X = torch.from_numpy(X)\n",
        "        #         Y = torch.from_numpy(Y)\n",
        "\n",
        "        #         if normalize and X.dtype == torch.uint8:\n",
        "        #             X = X.float() / 255.0\n",
        "\n",
        "        #         if to_device:\n",
        "        #             X = X.to(to_device)\n",
        "        #             Y = Y.to(to_device)\n",
        "\n",
        "        #         print(f\"üß† Torch tensors ready on {to_device or 'CPU'}\")\n",
        "\n",
        "        #     return X, Y\n",
        "\n",
        "        # # üîÅ Example call\n",
        "        # X, Y = load_preprocessed_data(\n",
        "        #     as_torch=True,\n",
        "        #     normalize=True,\n",
        "        #     to_device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # )\n",
        "\n",
        "        # print(f\"\\nOriginal data shape: {X.shape}\")\n",
        "        # # print(f\"Original class distribution: {np.bincount(Y)}\")\n",
        "        # class_dist = np.bincount(Y.cpu().numpy()) if torch.is_tensor(Y) else np.bincount(Y)\n",
        "        # print(f\"Original class distribution: {class_dist}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"Applying SMOTE for class balancing...\")\n",
        "        # X_flat = X.reshape(X.shape[0], -1)\n",
        "        # smote = SMOTE(random_state=Config.SEED, k_neighbors=min(5, np.bincount(Y).min()-1))\n",
        "        # X_balanced_flat, Y_balanced = smote.fit_resample(X_flat, Y)\n",
        "        # X_balanced = X_balanced_flat.reshape(-1, *X.shape[1:])\n",
        "        # print(f\"Balanced data shape: {X_balanced.shape}\")\n",
        "        # print(f\"Balanced class distribution: {np.bincount(Y_balanced)}\")\n",
        "        # return X_balanced, Y_balanced\n",
        "\n",
        "\n",
        "\n",
        "        # Remove SMOTE completely and use WeightedRandomSampler only\n",
        "        # Using WeightedRandomSampler instead of SMOTE\n",
        "        # Compute weights and create sampler during DataLoader, not here\n",
        "        # return X, Y\n",
        "        # # Example data\n",
        "        # X = torch.randn(100, 3, 32, 32)  # 100 images\n",
        "        # Y = torch.randint(0, 5, (100,))  # 5 classes, imbalanced\n",
        "        # # Compute class weights\n",
        "        # class_counts = torch.bincount(Y)\n",
        "        # class_weights = 1.0 / class_counts.float()\n",
        "        # sample_weights = class_weights[Y]  # assign weight to each sample\n",
        "        # # Create sampler\n",
        "        # sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        # # Create DataLoader\n",
        "        # dataset = TensorDataset(X, Y)\n",
        "        # loader = DataLoader(dataset, batch_size=16, sampler=sampler)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Applying SMOTE for class balancing...\")\n",
        "        # Apply SMOTE with reduced k_neighbors and combine with WeightedRandomSampler\n",
        "        X_flat = X.reshape(X.shape[0], -1)\n",
        "        smote = SMOTE(random_state=Config.SEED, k_neighbors=3, sampling_strategy='not majority')\n",
        "        # smote = SMOTE(random_state=Config.SEED, k_neighbors=2, sampling_strategy= 'auto')\n",
        "        X_balanced_flat, Y_balanced = smote.fit_resample(X_flat, Y)\n",
        "        X_balanced = X_balanced_flat.reshape(-1, *X.shape[1:])\n",
        "        # Ensures WeightedRandomSampler is still used in DataLoader\n",
        "        print(f\"Balanced data shape: {X_balanced.shape}\")\n",
        "        print(f\"Balanced class distribution: {np.bincount(Y_balanced)}\")\n",
        "        return X_balanced, Y_balanced\n",
        "        # Benefit: Using a smaller k_neighbors=3 reduces the risk of generating unnatural\n",
        "        # image artifacts, while sampling_strategy='not majority' balances classes more conservatively.\n",
        "        # Retaining WeightedRandomSampler in the DataLoader further ensures balanced sampling during\n",
        "        # training, maintaining smoothness and preventing accuracy drops by avoiding over-reliance\n",
        "        # on SMOTE-generated samples.\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"Applying SMOTE for class balancing...\")\n",
        "\n",
        "        # X_flat = X.cpu().numpy().reshape(X.shape[0], -1) if torch.is_tensor(X) else X.reshape(X.shape[0], -1)\n",
        "        # Y_np = Y.cpu().numpy() if torch.is_tensor(Y) else Y\n",
        "\n",
        "        # smote = SMOTE(random_state=Config.SEED, k_neighbors=3, sampling_strategy='not majority')\n",
        "        # X_balanced_flat, Y_balanced = smote.fit_resample(X_flat, Y_np)\n",
        "        # X_balanced = X_balanced_flat.reshape(-1, *X.shape[1:])\n",
        "\n",
        "        # print(f\"Balanced data shape: {X_balanced.shape}\")\n",
        "        # print(f\"Balanced class distribution: {np.bincount(Y_balanced)}\")\n",
        "        # return X_balanced, Y_balanced\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_data_loaders(X, Y, test_size=0.2, batch_size=None, augmentation_strength='medium'):\n",
        "\n",
        "\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=test_size, random_state=Config.SEED, stratify=Y)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=Config.SEED, stratify=y_temp)\n",
        "\n",
        "        print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "        print(f\"Using optimized batch size: {batch_size}\")\n",
        "\n",
        "\n",
        "\n",
        "        train_dataset = FishDataset(X_train, y_train,DataManager.get_transforms(True, augmentation_strength))\n",
        "        val_dataset = FishDataset(X_val, y_val, DataManager.get_transforms(False))\n",
        "        test_dataset = FishDataset(X_test, y_test, DataManager.get_transforms(False))\n",
        "\n",
        "\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        #compute_class_weight('balanced', ...) gives higher weight to minority classes.\n",
        "        sample_weights = [class_weights[y] for y in y_train]\n",
        "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "        # Samples with higher weights are more likely to be picked in each batch.\n",
        "        # replacement=True allows oversampling of minority classes. ‚úÖ\n",
        "\n",
        "\n",
        "        # Conditionally set prefetch_factor based on num_workers\n",
        "        prefetch_factor = 2 if Config.DATALOADER_NUM_WORKERS > 0 else None\n",
        "        pin_memory=Config.PIN_MEMORY if 'cuda' in Config.DEVICE else False\n",
        "        num_workers = Config.DATALOADER_NUM_WORKERS if torch.cuda.is_available() else 0\n",
        "        use_prefetch = num_workers > 0\n",
        "\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            # sampler=sampler, #Imbalanced dataset ‚Üí use sampler.Balanced dataset ‚Üí use shuffle=True.\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            prefetch_factor=2 if use_prefetch else None,  # Only use prefetch_factor when num_workers > 0\n",
        "            # persistent_workers=Config.DATALOADER_NUM_WORKERS > 0,\n",
        "            # persistent_workers=False,\n",
        "            worker_init_fn=worker_init_fn  # Add this\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            prefetch_factor=2 if use_prefetch else None,  # Only use prefetch_factor when num_workers > 0\n",
        "            # persistent_workers=Config.DATALOADER_NUM_WORKERS > 0,\n",
        "            # persistent_workers=False,\n",
        "            worker_init_fn=worker_init_fn  # Add this\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            prefetch_factor=2 if use_prefetch else None,  # Only use prefetch_factor when num_workers > 0\n",
        "            # persistent_workers=Config.DATALOADER_NUM_WORKERS > 0,\n",
        "            # persistent_workers=False, #False is slow but exact reproductivity ensures & workers reset each epoch).\n",
        "            worker_init_fn=worker_init_fn  # Add this\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader, (X_val, y_val), (X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "-UojpedIZ-Ra"
      },
      "id": "-UojpedIZ-Ra",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 4: Loading"
      ],
      "metadata": {
        "id": "MIZ9EajaYfPQ"
      },
      "id": "MIZ9EajaYfPQ"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FILE = '/content/drive/MyDrive/Hilsha/X_data.npy'\n",
        "LABEL_FILE = '/content/drive/MyDrive/Hilsha/Y_labels.npy'\n",
        "\n",
        "\n",
        "X = np.load(DATA_FILE)\n",
        "Y = np.load(LABEL_FILE)"
      ],
      "metadata": {
        "id": "EItOc1z5MANu"
      },
      "id": "EItOc1z5MANu",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 5:Data Visualization [From Processed Image]\n"
      ],
      "metadata": {
        "id": "xz03W0wETvEH"
      },
      "id": "xz03W0wETvEH"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import pandas as pd\n",
        "# from collections import Counter\n",
        "# import os\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# # Scientific plotting setup\n",
        "# plt.style.use('seaborn-v0_8')\n",
        "# sns.set_palette(\"husl\")\n",
        "# plt.rcParams['figure.dpi'] = 300\n",
        "# plt.rcParams['savefig.dpi'] = 300\n",
        "# plt.rcParams['font.size'] = 12\n",
        "# plt.rcParams['axes.titlesize'] = 14\n",
        "# plt.rcParams['axes.labelsize'] = 12\n",
        "# plt.rcParams['xtick.labelsize'] = 10\n",
        "# plt.rcParams['ytick.labelsize'] = 10\n",
        "# plt.rcParams['legend.fontsize'] = 10\n",
        "# plt.rcParams['figure.titlesize'] = 16\n",
        "\n",
        "# class FishDatasetNumpyAnalyzer:\n",
        "#     \"\"\"Comprehensive analysis suite for fish species dataset from NumPy arrays\"\"\"\n",
        "\n",
        "#     def __init__(self, X_data, Y_labels, output_dir='./fish_classification_results'):\n",
        "#         self.X_data = X_data\n",
        "#         self.Y_labels = Y_labels\n",
        "#         self.output_dir = output_dir\n",
        "#         self.create_output_dirs()\n",
        "\n",
        "#         # Dataset metadata\n",
        "#         self.n_samples = X_data.shape[0]\n",
        "#         self.image_shape = X_data.shape[1:]\n",
        "#         self.unique_labels = np.unique(Y_labels)\n",
        "#         self.n_classes = len(self.unique_labels)\n",
        "\n",
        "#         # Determine image format (channels first vs channels last)\n",
        "#         self.channels_first = self._detect_channels_first()\n",
        "\n",
        "#         # Create label mapping if labels are numeric\n",
        "#         if np.issubdtype(Y_labels.dtype, np.number):\n",
        "#             self.label_names = [f\"Species_{i}\" for i in self.unique_labels]\n",
        "#             self.label_to_name = dict(zip(self.unique_labels, self.label_names))\n",
        "#         else:\n",
        "#             self.label_names = self.unique_labels.tolist()\n",
        "#             self.label_to_name = dict(zip(self.unique_labels, self.label_names))\n",
        "\n",
        "#         print(f\"Dataset loaded: {self.n_samples} samples, {self.n_classes} classes\")\n",
        "#         print(f\"Image shape: {self.image_shape}\")\n",
        "#         print(f\"Data type: {X_data.dtype}\")\n",
        "#         print(f\"Channels first format: {self.channels_first}\")\n",
        "\n",
        "#     def _detect_channels_first(self):\n",
        "#         \"\"\"Detect if images are in channels-first format\"\"\"\n",
        "#         if len(self.image_shape) == 3:\n",
        "#             # If first dimension is small (1-4), likely channels first\n",
        "#             # If last dimension is small (1-4), likely channels last\n",
        "#             if self.image_shape[0] <= 4 and self.image_shape[0] < min(self.image_shape[1], self.image_shape[2]):\n",
        "#                 return True\n",
        "#             elif self.image_shape[2] <= 4 and self.image_shape[2] < min(self.image_shape[0], self.image_shape[1]):\n",
        "#                 return False\n",
        "#             else:\n",
        "#                 # Default assumption based on common formats\n",
        "#                 return self.image_shape[0] <= 4\n",
        "#         return False\n",
        "\n",
        "#     def _prepare_image_for_display(self, img):\n",
        "#         \"\"\"Convert image to proper format for matplotlib display\"\"\"\n",
        "#         if len(img.shape) == 3:\n",
        "#             if self.channels_first:\n",
        "#                 # Convert from (C, H, W) to (H, W, C)\n",
        "#                 img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "#             # Handle different channel counts\n",
        "#             if img.shape[2] == 1:  # Grayscale with channel dimension\n",
        "#                 img = img.squeeze(axis=2)\n",
        "#                 return img, 'gray'\n",
        "#             elif img.shape[2] == 3:  # RGB\n",
        "#                 return img, None\n",
        "#             elif img.shape[2] == 4:  # RGBA\n",
        "#                 return img[:, :, :3], None  # Drop alpha channel\n",
        "#             else:\n",
        "#                 # Multi-channel, use first channel as grayscale\n",
        "#                 return img[:, :, 0], 'gray'\n",
        "#         else:  # 2D grayscale\n",
        "#             return img, 'gray'\n",
        "\n",
        "#     def create_output_dirs(self):\n",
        "#         \"\"\"Create organized output directory structure\"\"\"\n",
        "#         dirs = [\n",
        "#             self.output_dir,\n",
        "#             f\"{self.output_dir}/figures\",\n",
        "#             f\"{self.output_dir}/statistics\",\n",
        "#             f\"{self.output_dir}/sample_images\",\n",
        "#             f\"{self.output_dir}/reports\"\n",
        "#         ]\n",
        "#         for dir_path in dirs:\n",
        "#             os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "#     def analyze_data_properties(self):\n",
        "#         \"\"\"Analyze basic properties of the loaded data\"\"\"\n",
        "#         print(\"Analyzing data properties...\")\n",
        "\n",
        "#         properties = {\n",
        "#             'dataset_size': self.n_samples,\n",
        "#             'n_classes': self.n_classes,\n",
        "#             'image_shape': self.image_shape,\n",
        "#             'channels_first': self.channels_first,\n",
        "#             'data_type': str(self.X_data.dtype),\n",
        "#             'data_range': {\n",
        "#                 'min': float(self.X_data.min()),\n",
        "#                 'max': float(self.X_data.max()),\n",
        "#                 'mean': float(self.X_data.mean()),\n",
        "#                 'std': float(self.X_data.std())\n",
        "#             },\n",
        "#             'class_distribution': dict(Counter(self.Y_labels)),\n",
        "#             'memory_usage_mb': self.X_data.nbytes / (1024 * 1024)\n",
        "#         }\n",
        "\n",
        "#         # Per-class statistics\n",
        "#         class_stats = {}\n",
        "#         for label in self.unique_labels:\n",
        "#             mask = self.Y_labels == label\n",
        "#             class_data = self.X_data[mask]\n",
        "#             class_stats[self.label_to_name[label]] = {\n",
        "#                 'count': int(np.sum(mask)),\n",
        "#                 'mean_intensity': float(class_data.mean()),\n",
        "#                 'std_intensity': float(class_data.std()),\n",
        "#                 'min_intensity': float(class_data.min()),\n",
        "#                 'max_intensity': float(class_data.max())\n",
        "#             }\n",
        "\n",
        "#         properties['class_statistics'] = class_stats\n",
        "#         self.data_properties = properties\n",
        "\n",
        "#         return properties\n",
        "\n",
        "#     def plot_class_distribution(self, figsize=(15, 8)):\n",
        "#         \"\"\"Visualize class distribution\"\"\"\n",
        "#         class_counts = Counter(self.Y_labels)\n",
        "#         class_names = [self.label_to_name[label] for label in class_counts.keys()]\n",
        "#         counts = list(class_counts.values())\n",
        "\n",
        "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "#         # Bar plot\n",
        "#         bars = ax1.bar(range(len(class_names)), counts, color='skyblue', alpha=0.7)\n",
        "#         ax1.set_title('Class Distribution', fontweight='bold')\n",
        "#         ax1.set_xlabel('Species')\n",
        "#         ax1.set_ylabel('Number of Samples')\n",
        "#         ax1.set_xticks(range(len(class_names)))\n",
        "#         ax1.set_xticklabels(class_names, rotation=45, ha='right')\n",
        "\n",
        "#         # Add value labels on bars\n",
        "#         for bar, count in zip(bars, counts):\n",
        "#             height = bar.get_height()\n",
        "#             ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "#                     f'{count}', ha='center', va='bottom')\n",
        "\n",
        "#         # Pie chart\n",
        "#         ax2.pie(counts, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "#         ax2.set_title('Class Distribution (%)', fontweight='bold')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(f'{self.output_dir}/figures/class_distribution.png', bbox_inches='tight')\n",
        "#         plt.show()\n",
        "\n",
        "#     def plot_sample_images(self, samples_per_class=5, figsize=(20, 12)):\n",
        "#         \"\"\"Display sample images from each class\"\"\"\n",
        "#         n_classes = len(self.unique_labels)\n",
        "\n",
        "#         fig, axes = plt.subplots(n_classes, samples_per_class, figsize=figsize)\n",
        "#         if n_classes == 1:\n",
        "#             axes = axes.reshape(1, -1)\n",
        "#         elif samples_per_class == 1:\n",
        "#             axes = axes.reshape(-1, 1)\n",
        "\n",
        "#         fig.suptitle('Sample Images by Class', fontsize=16, fontweight='bold')\n",
        "\n",
        "#         for i, label in enumerate(self.unique_labels):\n",
        "#             # Get indices for this class\n",
        "#             class_indices = np.where(self.Y_labels == label)[0]\n",
        "\n",
        "#             # Sample random images from this class\n",
        "#             if len(class_indices) >= samples_per_class:\n",
        "#                 sample_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n",
        "#             else:\n",
        "#                 sample_indices = class_indices\n",
        "\n",
        "#             for j in range(samples_per_class):\n",
        "#                 if j < len(sample_indices):\n",
        "#                     img = self.X_data[sample_indices[j]].copy()\n",
        "\n",
        "#                     # Prepare image for display\n",
        "#                     display_img, cmap = self._prepare_image_for_display(img)\n",
        "\n",
        "#                     # Normalize if needed\n",
        "#                     if display_img.max() > 1:\n",
        "#                         display_img = display_img.astype(float) / 255.0\n",
        "\n",
        "#                     axes[i, j].imshow(display_img, cmap=cmap)\n",
        "#                     axes[i, j].axis('off')\n",
        "\n",
        "#                     if j == 0:  # Label the first column with class names\n",
        "#                         axes[i, j].set_ylabel(self.label_to_name[label],\n",
        "#                                             rotation=90, fontsize=12, va='center')\n",
        "#                 else:\n",
        "#                     axes[i, j].axis('off')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(f'{self.output_dir}/figures/sample_images.png', bbox_inches='tight')\n",
        "#         plt.show()\n",
        "\n",
        "#     def plot_pixel_intensity_analysis(self, figsize=(20, 12)):\n",
        "#         \"\"\"Analyze pixel intensity distributions\"\"\"\n",
        "#         fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
        "#         fig.suptitle('Pixel Intensity Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "#         # Overall intensity distribution\n",
        "#         axes[0, 0].hist(self.X_data.flatten(), bins=100, alpha=0.7, color='blue', density=True)\n",
        "#         axes[0, 0].set_title('Overall Pixel Intensity Distribution')\n",
        "#         axes[0, 0].set_xlabel('Pixel Intensity')\n",
        "#         axes[0, 0].set_ylabel('Density')\n",
        "\n",
        "#         # Mean intensity per image\n",
        "#         mean_intensities = np.mean(self.X_data.reshape(self.n_samples, -1), axis=1)\n",
        "#         axes[0, 1].hist(mean_intensities, bins=50, alpha=0.7, color='green', density=True)\n",
        "#         axes[0, 1].set_title('Mean Intensity per Image')\n",
        "#         axes[0, 1].set_xlabel('Mean Intensity')\n",
        "#         axes[0, 1].set_ylabel('Density')\n",
        "\n",
        "#         # Standard deviation per image\n",
        "#         std_intensities = np.std(self.X_data.reshape(self.n_samples, -1), axis=1)\n",
        "#         axes[0, 2].hist(std_intensities, bins=50, alpha=0.7, color='red', density=True)\n",
        "#         axes[0, 2].set_title('Intensity Standard Deviation per Image')\n",
        "#         axes[0, 2].set_xlabel('Std Intensity')\n",
        "#         axes[0, 2].set_ylabel('Density')\n",
        "\n",
        "#         # Class-wise intensity comparison\n",
        "#         class_intensities = []\n",
        "#         class_labels = []\n",
        "#         for label in self.unique_labels:\n",
        "#             mask = self.Y_labels == label\n",
        "#             class_data = self.X_data[mask]\n",
        "#             class_mean_intensities = np.mean(class_data.reshape(np.sum(mask), -1), axis=1)\n",
        "#             class_intensities.extend(class_mean_intensities)\n",
        "#             class_labels.extend([self.label_to_name[label]] * len(class_mean_intensities))\n",
        "\n",
        "#         intensity_df = pd.DataFrame({\n",
        "#             'intensity': class_intensities,\n",
        "#             'class': class_labels\n",
        "#         })\n",
        "\n",
        "#         # Create boxplot data\n",
        "#         box_data = [intensity_df[intensity_df['class'] == name]['intensity'].values\n",
        "#                    for name in self.label_names]\n",
        "\n",
        "#         axes[1, 0].boxplot(box_data, labels=self.label_names)\n",
        "#         axes[1, 0].set_title('Mean Intensity by Class')\n",
        "#         axes[1, 0].set_ylabel('Mean Intensity')\n",
        "#         axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "#         # Average image intensity heatmap by class\n",
        "#         avg_images = np.zeros((len(self.unique_labels), *self.image_shape))\n",
        "#         for i, label in enumerate(self.unique_labels):\n",
        "#             mask = self.Y_labels == label\n",
        "#             avg_images[i] = np.mean(self.X_data[mask], axis=0)\n",
        "\n",
        "#         # Calculate average intensity across spatial dimensions\n",
        "#         if len(self.image_shape) == 3:\n",
        "#             if self.channels_first:\n",
        "#                 # Average across height and width for each channel\n",
        "#                 avg_intensities = np.mean(avg_images, axis=(2, 3))  # Shape: (n_classes, n_channels)\n",
        "#             else:\n",
        "#                 # Average across height and width for each channel\n",
        "#                 avg_intensities = np.mean(avg_images, axis=(1, 2))  # Shape: (n_classes, n_channels)\n",
        "#         else:\n",
        "#             # Grayscale images - average across spatial dimensions\n",
        "#             avg_intensities = np.mean(avg_images, axis=(1, 2))  # Shape: (n_classes,)\n",
        "#             avg_intensities = avg_intensities.reshape(-1, 1)  # Make it 2D for heatmap\n",
        "\n",
        "#         im = axes[1, 1].imshow(avg_intensities, cmap='viridis', aspect='auto')\n",
        "#         axes[1, 1].set_title('Average Intensity by Class')\n",
        "#         axes[1, 1].set_ylabel('Class Index')\n",
        "#         axes[1, 1].set_yticks(range(len(self.unique_labels)))\n",
        "#         axes[1, 1].set_yticklabels([self.label_to_name[label] for label in self.unique_labels])\n",
        "\n",
        "#         if len(self.image_shape) == 3:\n",
        "#             if self.channels_first:\n",
        "#                 n_channels = self.image_shape[0]\n",
        "#             else:\n",
        "#                 n_channels = self.image_shape[2]\n",
        "\n",
        "#             if n_channels > 1:\n",
        "#                 axes[1, 1].set_xlabel('Channel')\n",
        "#                 axes[1, 1].set_xticks(range(n_channels))\n",
        "#                 if n_channels == 3:\n",
        "#                     axes[1, 1].set_xticklabels(['R', 'G', 'B'])\n",
        "#                 else:\n",
        "#                     axes[1, 1].set_xticklabels([f'Ch{i}' for i in range(n_channels)])\n",
        "#         else:\n",
        "#             axes[1, 1].set_xlabel('Intensity')\n",
        "\n",
        "#         plt.colorbar(im, ax=axes[1, 1])\n",
        "\n",
        "#         # Channel analysis for color images\n",
        "#         if len(self.image_shape) == 3:\n",
        "#             if self.channels_first:\n",
        "#                 n_channels = self.image_shape[0]\n",
        "#                 channel_means = np.mean(self.X_data, axis=(0, 2, 3))  # Average over batch, height, width\n",
        "#             else:\n",
        "#                 n_channels = self.image_shape[2]\n",
        "#                 channel_means = np.mean(self.X_data, axis=(0, 1, 2))  # Average over batch, height, width\n",
        "\n",
        "#             if n_channels > 1:\n",
        "#                 colors = ['red', 'green', 'blue'] if n_channels == 3 else ['gray'] * n_channels\n",
        "#                 axes[1, 2].bar(range(n_channels), channel_means, color=colors)\n",
        "#                 axes[1, 2].set_title('Mean Intensity by Channel')\n",
        "#                 axes[1, 2].set_xlabel('Channel')\n",
        "#                 axes[1, 2].set_ylabel('Mean Intensity')\n",
        "#                 if n_channels == 3:\n",
        "#                     axes[1, 2].set_xticks(range(3))\n",
        "#                     axes[1, 2].set_xticklabels(['Red', 'Green', 'Blue'])\n",
        "#                 else:\n",
        "#                     axes[1, 2].set_xticks(range(n_channels))\n",
        "#                     axes[1, 2].set_xticklabels([f'Ch{i}' for i in range(n_channels)])\n",
        "#             else:\n",
        "#                 axes[1, 2].text(0.5, 0.5, 'Single Channel\\n(Grayscale)',\n",
        "#                                ha='center', va='center', transform=axes[1, 2].transAxes)\n",
        "#                 axes[1, 2].set_title('Channel Information')\n",
        "#         else:\n",
        "#             axes[1, 2].text(0.5, 0.5, 'Single Channel\\n(Grayscale)',\n",
        "#                            ha='center', va='center', transform=axes[1, 2].transAxes)\n",
        "#             axes[1, 2].set_title('Channel Information')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(f'{self.output_dir}/figures/pixel_intensity_analysis.png', bbox_inches='tight')\n",
        "#         plt.show()\n",
        "\n",
        "#     def perform_dimensionality_reduction(self, n_components=2, sample_size=5000):\n",
        "#         \"\"\"Perform PCA and t-SNE for visualization\"\"\"\n",
        "#         print(\"Performing dimensionality reduction...\")\n",
        "\n",
        "#         # Sample data if too large\n",
        "#         if self.n_samples > sample_size:\n",
        "#             indices = np.random.choice(self.n_samples, sample_size, replace=False)\n",
        "#             X_sample = self.X_data[indices]\n",
        "#             Y_sample = self.Y_labels[indices]\n",
        "#         else:\n",
        "#             X_sample = self.X_data\n",
        "#             Y_sample = self.Y_labels\n",
        "\n",
        "#         # Flatten images\n",
        "#         X_flat = X_sample.reshape(len(X_sample), -1)\n",
        "\n",
        "#         # Normalize data\n",
        "#         X_normalized = (X_flat - X_flat.mean()) / (X_flat.std() + 1e-8)\n",
        "\n",
        "#         # PCA\n",
        "#         print(\"Computing PCA...\")\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         X_pca = pca.fit_transform(X_normalized)\n",
        "\n",
        "#         # t-SNE\n",
        "#         print(\"Computing t-SNE...\")\n",
        "#         tsne = TSNE(n_components=n_components, random_state=42, perplexity=30)\n",
        "#         X_tsne = tsne.fit_transform(X_normalized)\n",
        "\n",
        "#         # Plot results\n",
        "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "#         # PCA plot\n",
        "#         scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1],\n",
        "#                               c=Y_sample, cmap='tab10', alpha=0.6)\n",
        "#         ax1.set_title(f'PCA Visualization\\nExplained Variance: {pca.explained_variance_ratio_.sum():.3f}')\n",
        "#         ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
        "#         ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
        "\n",
        "#         # Add colorbar for PCA\n",
        "#         cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
        "#         cbar1.set_label('Class')\n",
        "\n",
        "#         # t-SNE plot\n",
        "#         scatter2 = ax2.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
        "#                               c=Y_sample, cmap='tab10', alpha=0.6)\n",
        "#         ax2.set_title('t-SNE Visualization')\n",
        "#         ax2.set_xlabel('t-SNE 1')\n",
        "#         ax2.set_ylabel('t-SNE 2')\n",
        "\n",
        "#         # Add colorbar for t-SNE\n",
        "#         cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
        "#         cbar2.set_label('Class')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(f'{self.output_dir}/figures/dimensionality_reduction.png', bbox_inches='tight')\n",
        "#         plt.show()\n",
        "\n",
        "#         return pca, X_pca, X_tsne\n",
        "\n",
        "#     def create_data_summary_report(self):\n",
        "#         \"\"\"Create comprehensive data summary\"\"\"\n",
        "#         if not hasattr(self, 'data_properties'):\n",
        "#             self.analyze_data_properties()\n",
        "\n",
        "#         summary = f\"\"\"\n",
        "# FISH SPECIES DATASET - DATA ANALYSIS REPORT\n",
        "# {'='*60}\n",
        "\n",
        "# Dataset Overview:\n",
        "# - Total samples: {self.data_properties['dataset_size']:,}\n",
        "# - Number of classes: {self.data_properties['n_classes']}\n",
        "# - Image dimensions: {self.data_properties['image_shape']}\n",
        "# - Channels first format: {self.data_properties['channels_first']}\n",
        "# - Data type: {self.data_properties['data_type']}\n",
        "# - Memory usage: {self.data_properties['memory_usage_mb']:.1f} MB\n",
        "\n",
        "# Data Range:\n",
        "# - Minimum value: {self.data_properties['data_range']['min']:.3f}\n",
        "# - Maximum value: {self.data_properties['data_range']['max']:.3f}\n",
        "# - Mean value: {self.data_properties['data_range']['mean']:.3f}\n",
        "# - Standard deviation: {self.data_properties['data_range']['std']:.3f}\n",
        "\n",
        "# Class Distribution:\n",
        "# \"\"\"\n",
        "\n",
        "#         for class_name, stats in self.data_properties['class_statistics'].items():\n",
        "#             summary += f\"\\n{class_name}:\\n\"\n",
        "#             summary += f\"  - Sample count: {stats['count']:,}\\n\"\n",
        "#             summary += f\"  - Mean intensity: {stats['mean_intensity']:.3f}\\n\"\n",
        "#             summary += f\"  - Std intensity: {stats['std_intensity']:.3f}\\n\"\n",
        "#             summary += f\"  - Intensity range: {stats['min_intensity']:.3f} - {stats['max_intensity']:.3f}\\n\"\n",
        "\n",
        "#         # Save report\n",
        "#         with open(f'{self.output_dir}/reports/data_summary.txt', 'w') as f:\n",
        "#             f.write(summary)\n",
        "\n",
        "#         print(summary)\n",
        "#         return summary\n",
        "\n",
        "#     def run_complete_analysis(self, sample_size_dr=5000):\n",
        "#         \"\"\"Run complete analysis pipeline\"\"\"\n",
        "#         print(\"Starting comprehensive NumPy dataset analysis...\")\n",
        "#         print(\"=\" * 60)\n",
        "\n",
        "#         # Step 1: Analyze data properties\n",
        "#         self.analyze_data_properties()\n",
        "\n",
        "#         # Step 2: Create visualizations\n",
        "#         print(\"\\nGenerating visualizations...\")\n",
        "#         self.plot_class_distribution()\n",
        "#         self.plot_sample_images()\n",
        "#         self.plot_pixel_intensity_analysis()\n",
        "\n",
        "#         # Step 3: Dimensionality reduction\n",
        "#         pca, X_pca, X_tsne = self.perform_dimensionality_reduction(sample_size=sample_size_dr)\n",
        "\n",
        "#         # Step 4: Generate report\n",
        "#         print(\"\\nGenerating comprehensive report...\")\n",
        "#         self.create_data_summary_report()\n",
        "\n",
        "#         print(f\"\\nAnalysis complete! Results saved to: {self.output_dir}\")\n",
        "#         print(\"\\nGenerated files:\")\n",
        "#         print(\"- Figures: class_distribution.png, sample_images.png, pixel_intensity_analysis.png, dimensionality_reduction.png\")\n",
        "#         print(\"- Reports: data_summary.txt\")\n",
        "\n",
        "#         return pca, X_pca, X_tsne\n",
        "\n",
        "\n",
        "# # Usage example:\n",
        "# analyzer = FishDatasetNumpyAnalyzer(X_Loaded, Y_Loaded, output_dir='./fish_classification_results')\n",
        "# pca_model, X_pca_result, X_tsne_result = analyzer.run_complete_analysis(sample_size_dr=3000)\n",
        "# print(\"\\nDataset analysis completed successfully!\")\n",
        "# print(\"All visualizations and reports have been generated.\")"
      ],
      "metadata": {
        "id": "3GW4mxs4UgSQ"
      },
      "id": "3GW4mxs4UgSQ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 6: Model Architecture"
      ],
      "metadata": {
        "id": "EJcWrfAbXyhI"
      },
      "id": "EJcWrfAbXyhI"
    },
    {
      "cell_type": "code",
      "source": [
        "# class ModelFactory:\n",
        "#     @staticmethod\n",
        "#     def create_model(model_name, params=None, num_classes=Config.NUM_CLASSES):\n",
        "#         \"\"\"Create model with GPU optimizations\"\"\"\n",
        "#         if params is None:\n",
        "#             params = {}\n",
        "#         dropout_rate = params.get('dropout', 0.5)\n",
        "#         hidden_dim_multiplier = params.get('hidden_dim_multiplier', 0.5)\n",
        "\n",
        "#         # Create base model\n",
        "#         model = ModelFactory._create_base_model(model_name, params, num_classes, dropout_rate, hidden_dim_multiplier)\n",
        "\n",
        "#         # Apply GPU optimizations\n",
        "#         return ModelFactory._optimize_for_gpu(model)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _optimize_for_gpu(model):\n",
        "#         \"\"\"Apply GPU-specific optimizations\"\"\"\n",
        "#         if torch.cuda.is_available():\n",
        "#             # Convert to channels_last memory format for better GPU utilization\n",
        "#             model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "#             # Enable torch.compile for PyTorch 2.0+ (significant speedup)\n",
        "#             if hasattr(torch, 'compile') and torch.cuda.get_device_capability()[0] >= 7:\n",
        "#                 try:\n",
        "#                     model = torch.compile(model, mode='max-autotune', dynamic=True)\n",
        "#                 except Exception:\n",
        "#                     pass  # Fallback if compilation fails\n",
        "\n",
        "#             # Replace standard activations with more GPU-efficient ones\n",
        "#             ModelFactory._replace_activations(model)\n",
        "\n",
        "#         return model\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _replace_activations(model):\n",
        "#         \"\"\"Replace ReLU with more GPU-efficient activations\"\"\"\n",
        "#         for name, module in model.named_children():\n",
        "#             if isinstance(module, nn.ReLU):\n",
        "#                 # Replace with SiLU for better GPU utilization\n",
        "#                 setattr(model, name, nn.SiLU(inplace=True))\n",
        "#             elif len(list(module.children())) > 0:\n",
        "#                 ModelFactory._replace_activations(module)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _create_gpu_optimized_classifier(in_features, hidden_dim, num_classes, dropout_rate):\n",
        "#         \"\"\"Create GPU-optimized classifier with grouped operations\"\"\"\n",
        "#         return nn.Sequential(\n",
        "#             # Group operations for better GPU memory access\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(in_features, hidden_dim, bias=False),  # Remove bias (BatchNorm handles it)\n",
        "#             nn.BatchNorm1d(hidden_dim),\n",
        "#             nn.SiLU(inplace=True),  # More GPU-efficient than ReLU\n",
        "\n",
        "#             # Second layer with residual-like structure\n",
        "#             nn.Dropout(dropout_rate * 0.5),\n",
        "#             nn.Linear(hidden_dim, hidden_dim // 2, bias=False),\n",
        "#             nn.BatchNorm1d(hidden_dim // 2),\n",
        "#             nn.SiLU(inplace=True),\n",
        "\n",
        "#             # Final classification layer\n",
        "#             nn.Linear(hidden_dim // 2, num_classes)\n",
        "#         )\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _create_base_model(model_name, params, num_classes, dropout_rate, hidden_dim_multiplier):\n",
        "#         \"\"\"Create the base model architecture\"\"\"\n",
        "\n",
        "#         if model_name == 'resnet50':\n",
        "#             model = models.resnet50(weights='IMAGENET1K_V2')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"layer2\", \"layer3\", \"layer4\", \"fc\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.fc.in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.fc = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'efficientnet_b0':\n",
        "#             model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"_blocks.12\", \"_blocks.13\", \"_blocks.14\", \"_blocks.15\", \"_blocks.16\", \"classifier\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.classifier[1].in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.classifier = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'mobilenet_v3_large':\n",
        "#             model = models.mobilenet_v3_large(weights='IMAGENET1K_V2')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"features.10\", \"features.11\", \"features.12\", \"features.13\", \"classifier\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = 960\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.classifier = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'vgg16':\n",
        "#             model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"features.24\", \"features.26\", \"features.28\", \"classifier\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             hidden_dim = int(4096 * hidden_dim_multiplier)\n",
        "#             # Simplified classifier for better GPU utilization\n",
        "#             model.classifier = nn.Sequential(\n",
        "#                 nn.Dropout(dropout_rate),\n",
        "#                 nn.Linear(512 * 7 * 7, hidden_dim, bias=False),\n",
        "#                 nn.BatchNorm1d(hidden_dim),\n",
        "#                 nn.SiLU(inplace=True),\n",
        "#                 nn.Dropout(dropout_rate * 0.5),\n",
        "#                 nn.Linear(hidden_dim, hidden_dim // 2, bias=False),\n",
        "#                 nn.BatchNorm1d(hidden_dim // 2),\n",
        "#                 nn.SiLU(inplace=True),\n",
        "#                 nn.Linear(hidden_dim // 2, num_classes)\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'densenet121':\n",
        "#             model = models.densenet121(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"denseblock3\", \"denseblock4\", \"classifier\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.classifier.in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.classifier = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'inception_v3':\n",
        "#             model = models.inception_v3(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"Mixed_6\", \"Mixed_7a\", \"Mixed_7b\", \"Mixed_7c\", \"fc\", \"AuxLogits\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.fc.in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "\n",
        "#             # Main classifier\n",
        "#             model.fc = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#             # Auxiliary classifier (if exists)\n",
        "#             if hasattr(model, 'AuxLogits') and model.AuxLogits is not None:\n",
        "#                 aux_features = model.AuxLogits.fc.in_features\n",
        "#                 aux_hidden = int(aux_features * hidden_dim_multiplier)\n",
        "#                 model.AuxLogits.fc = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                     aux_features, aux_hidden, num_classes, dropout_rate\n",
        "#                 )\n",
        "\n",
        "#         elif model_name == 'vit_b_16':\n",
        "#             model = models.vit_b_16(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"encoder.layers.8\", \"encoder.layers.9\", \"encoder.layers.10\", \"encoder.layers.11\", \"heads\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.heads.head.in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.heads.head = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'convnext_base':\n",
        "#             model = models.convnext_base(weights='IMAGENET1K_V1')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"features.6\", \"features.7\", \"classifier\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.classifier[2].in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.classifier = nn.Sequential(\n",
        "#                 model.classifier[0],  # Keep the LayerNorm\n",
        "#                 model.classifier[1],  # Keep the Flatten\n",
        "#                 nn.Dropout(dropout_rate),\n",
        "#                 nn.Linear(num_features, hidden_dim, bias=False),\n",
        "#                 nn.BatchNorm1d(hidden_dim),\n",
        "#                 nn.SiLU(inplace=True),\n",
        "#                 nn.Dropout(dropout_rate * 0.5),\n",
        "#                 nn.Linear(hidden_dim, hidden_dim // 2, bias=False),\n",
        "#                 nn.BatchNorm1d(hidden_dim // 2),\n",
        "#                 nn.SiLU(inplace=True),\n",
        "#                 nn.Linear(hidden_dim // 2, num_classes)\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'regnet_y_32gf':\n",
        "#             model = models.regnet_y_32gf(weights='IMAGENET1K_V2')\n",
        "#             # Unfreeze more layers for better GPU utilization\n",
        "#             for name, param in model.named_parameters():\n",
        "#                 param.requires_grad = False\n",
        "#                 if any(layer in name for layer in [\"trunk_output.block3\", \"trunk_output.block4\", \"fc\"]):\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#             num_features = model.fc.in_features\n",
        "#             hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "#             model.fc = ModelFactory._create_gpu_optimized_classifier(\n",
        "#                 num_features, hidden_dim, num_classes, dropout_rate\n",
        "#             )\n",
        "\n",
        "#         elif model_name == 'cnn':\n",
        "#             class GPUOptimizedCNN(nn.Module):\n",
        "#                 def __init__(self, num_classes=5, dropout_rate=0.3, hidden_dim_multiplier=0.3):\n",
        "#                     super(GPUOptimizedCNN, self).__init__()\n",
        "\n",
        "#                     # GPU-optimized feature extractor with depthwise separable convolutions\n",
        "#                     self.features = nn.Sequential(\n",
        "#                         # Block 1 - Initial feature extraction\n",
        "#                         nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "#                         nn.BatchNorm2d(32),\n",
        "#                         nn.SiLU(inplace=True),\n",
        "\n",
        "#                         # Block 2 - Depthwise separable conv\n",
        "#                         nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32, bias=False),  # Depthwise\n",
        "#                         nn.Conv2d(32, 64, kernel_size=1, bias=False),  # Pointwise\n",
        "#                         nn.BatchNorm2d(64),\n",
        "#                         nn.SiLU(inplace=True),\n",
        "#                         nn.MaxPool2d(2, 2),\n",
        "\n",
        "#                         # Block 3 - More efficient computation\n",
        "#                         nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, groups=64, bias=False),  # Depthwise\n",
        "#                         nn.Conv2d(64, 128, kernel_size=1, bias=False),  # Pointwise\n",
        "#                         nn.BatchNorm2d(128),\n",
        "#                         nn.SiLU(inplace=True),\n",
        "#                         nn.MaxPool2d(2, 2),\n",
        "\n",
        "#                         # Block 4 - Final feature extraction\n",
        "#                         nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False),  # Depthwise\n",
        "#                         nn.Conv2d(128, 256, kernel_size=1, bias=False),  # Pointwise\n",
        "#                         nn.BatchNorm2d(256),\n",
        "#                         nn.SiLU(inplace=True),\n",
        "\n",
        "#                         # Global average pooling for efficiency\n",
        "#                         nn.AdaptiveAvgPool2d((1, 1))\n",
        "#                     )\n",
        "\n",
        "#                     # Efficient classifier\n",
        "#                     hidden_dim = max(128, int(256 * hidden_dim_multiplier))\n",
        "#                     self.classifier = nn.Sequential(\n",
        "#                         nn.Flatten(),\n",
        "#                         nn.Dropout(dropout_rate),\n",
        "#                         nn.Linear(256, hidden_dim, bias=False),\n",
        "#                         nn.BatchNorm1d(hidden_dim),\n",
        "#                         nn.SiLU(inplace=True),\n",
        "#                         nn.Dropout(dropout_rate * 0.5),\n",
        "#                         nn.Linear(hidden_dim, num_classes)\n",
        "#                     )\n",
        "\n",
        "#                     # Initialize weights properly\n",
        "#                     self._initialize_weights()\n",
        "\n",
        "#                 def _initialize_weights(self):\n",
        "#                     for m in self.modules():\n",
        "#                         if isinstance(m, nn.Conv2d):\n",
        "#                             nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "#                         elif isinstance(m, nn.Linear):\n",
        "#                             nn.init.xavier_uniform_(m.weight, gain=1.0)\n",
        "#                             if m.bias is not None:\n",
        "#                                 nn.init.zeros_(m.bias)\n",
        "#                         elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "#                             if m.weight is not None:\n",
        "#                                 nn.init.ones_(m.weight)\n",
        "#                             if m.bias is not None:\n",
        "#                                 nn.init.zeros_(m.bias)\n",
        "\n",
        "#                 def forward(self, x):\n",
        "#                     # Ensure channels_last format for GPU optimization\n",
        "#                     if x.device.type == 'cuda':\n",
        "#                         x = x.to(memory_format=torch.channels_last)\n",
        "\n",
        "#                     # Feature extraction\n",
        "#                     x = self.features(x)\n",
        "\n",
        "#                     # Classification\n",
        "#                     x = self.classifier(x)\n",
        "\n",
        "#                     return x\n",
        "\n",
        "#             model = GPUOptimizedCNN(num_classes=num_classes, dropout_rate=dropout_rate,\n",
        "#                                   hidden_dim_multiplier=hidden_dim_multiplier)\n",
        "\n",
        "#         else:\n",
        "#             raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "#         return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    # def create_model(model_name, num_classes=Config.NUM_CLASSES, dropout_rate=0.5, hidden_dim_multiplier=0.5):\n",
        "    def create_model(model_name, params=None, num_classes=Config.NUM_CLASSES, dropout_rate=0.5, hidden_dim_multiplier=0.5):\n",
        "        #Create model with configurable architecture\n",
        "        if params is None:\n",
        "            params = {}\n",
        "        dropout_rate = params.get('dropout', 0.5)\n",
        "        hidden_dim_multiplier = params.get('hidden_dim_multiplier', 0.5)\n",
        "\n",
        "        if model_name == 'resnet50':\n",
        "            model = models.resnet50(weights='IMAGENET1K_V2')\n",
        "            # Partial unfreeze for better accuracy: unfreeze layer4 and fc\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                # if \"layer4\" in name or \"fc\" in name:\n",
        "                if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            num_features = model.fc.in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.fc = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'efficientnet_b0':\n",
        "            model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: last blocks\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if \"_blocks.15\" in name or \"_blocks.16\" in name or \"classifier\" in name:\n",
        "                    param.requires_grad = True\n",
        "            num_features = model.classifier[1].in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'mobilenet_v3_large':\n",
        "            model = models.mobilenet_v3_large(weights='IMAGENET1K_V2')\n",
        "            # Partial unfreeze: last features\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if \"features.12\" in name or \"features.13\" in name or \"classifier\" in name:\n",
        "                    param.requires_grad = True\n",
        "            num_features = 960\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'vgg16':\n",
        "            model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: classifier and last features\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if \"classifier\" in name or \"features.28\" in name:\n",
        "                    param.requires_grad = True\n",
        "            hidden_dim = int(4096 * hidden_dim_multiplier)\n",
        "            model.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 7 * 7, 4096),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(4096, hidden_dim),\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'densenet121':\n",
        "            model = models.densenet121(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: denseblock4 and classifier\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if \"denseblock4\" in name or \"classifier\" in name:\n",
        "                    param.requires_grad = True\n",
        "            num_features = model.classifier.in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'inception_v3':\n",
        "            model = models.inception_v3(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: Mixed_7a, Mixed_7b, Mixed_7c and classifiers\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if any(layer in name for layer in [\"Mixed_7a\", \"Mixed_7b\", \"Mixed_7c\", \"fc\", \"AuxLogits\"]):\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            num_features = model.fc.in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "\n",
        "            # Main classifier\n",
        "            model.fc = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "            # Auxiliary classifier (if exists)\n",
        "            if hasattr(model, 'AuxLogits') and model.AuxLogits is not None:\n",
        "                aux_features = model.AuxLogits.fc.in_features\n",
        "                aux_hidden = int(aux_features * hidden_dim_multiplier)\n",
        "                model.AuxLogits.fc = nn.Sequential(\n",
        "                    nn.Dropout(dropout_rate),\n",
        "                    nn.Linear(aux_features, aux_hidden),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.BatchNorm1d(aux_hidden),\n",
        "                    nn.Dropout(dropout_rate / 2),\n",
        "                    nn.Linear(aux_hidden, num_classes)\n",
        "                )\n",
        "\n",
        "        elif model_name == 'vit_b_16':\n",
        "            model = models.vit_b_16(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: last encoder layers and head\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if any(layer in name for layer in [\"encoder.layers.10\", \"encoder.layers.11\", \"heads\"]):\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            num_features = model.heads.head.in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.heads.head = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'convnext_base':\n",
        "            model = models.convnext_base(weights='IMAGENET1K_V1')\n",
        "            # Partial unfreeze: last stages and classifier\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if any(layer in name for layer in [\"features.7\", \"classifier\"]):\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            num_features = model.classifier[2].in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.classifier = nn.Sequential(\n",
        "                model.classifier[0],  # Keep the LayerNorm\n",
        "                model.classifier[1],  # Keep the Flatten\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'regnet_y_32gf':\n",
        "            model = models.regnet_y_32gf(weights='IMAGENET1K_V2')\n",
        "            # Partial unfreeze: last trunk stage and fc\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "                if \"trunk_output\" in name or \"fc\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            num_features = model.fc.in_features\n",
        "            hidden_dim = int(num_features * hidden_dim_multiplier)\n",
        "            model.fc = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_features, hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout_rate / 2),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'cnn':\n",
        "            # class SimpleCNN(nn.Module):\n",
        "\n",
        "            class SimpleCNN(nn.Module):\n",
        "                def __init__(self, num_classes=5, dropout_rate=0.3, hidden_dim_multiplier=0.3):\n",
        "                    super(SimpleCNN, self).__init__()\n",
        "\n",
        "                    # More conservative feature extractor to prevent overfitting\n",
        "                    self.features = nn.Sequential(\n",
        "                        # Block 1 - Start small\n",
        "                        nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Dropout2d(0.1),  # Spatial dropout in conv layers\n",
        "                        nn.MaxPool2d(2, 2),  # 224 -> 112\n",
        "\n",
        "                        # Block 2\n",
        "                        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Dropout2d(0.15),\n",
        "                        nn.MaxPool2d(2, 2),  # 112 -> 56\n",
        "\n",
        "                        # Block 3\n",
        "                        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Dropout2d(0.2),\n",
        "                        nn.MaxPool2d(2, 2),  # 56 -> 28\n",
        "\n",
        "                        # Block 4 - Add one more conv before pooling\n",
        "                        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Dropout2d(0.25),\n",
        "                        nn.MaxPool2d(2, 2),  # 28 -> 14\n",
        "\n",
        "                        # Block 5 - Final feature extraction\n",
        "                        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Dropout2d(0.3),\n",
        "                        nn.AdaptiveAvgPool2d((7, 7))  # Fixed spatial size\n",
        "                    )\n",
        "\n",
        "                    # Calculate features after adaptive pooling\n",
        "                    conv_output_size = 256 * 7 * 7  # 12544\n",
        "\n",
        "                    # Much smaller hidden dimension to prevent overfitting\n",
        "                    hidden_dim = int(conv_output_size * hidden_dim_multiplier)\n",
        "                    hidden_dim = max(64, min(hidden_dim, 512))  # Smaller range\n",
        "\n",
        "                    # Simple but effective classifier\n",
        "                    self.classifier = nn.Sequential(\n",
        "                        nn.Dropout(dropout_rate),\n",
        "                        nn.Linear(conv_output_size, hidden_dim),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.BatchNorm1d(hidden_dim),\n",
        "                        nn.Dropout(dropout_rate * 0.5),\n",
        "                        nn.Linear(hidden_dim, num_classes)\n",
        "                    )\n",
        "\n",
        "                    # Initialize weights properly\n",
        "                    self._initialize_weights()\n",
        "\n",
        "                def _initialize_weights(self):\n",
        "                    for m in self.modules():\n",
        "                        if isinstance(m, nn.Conv2d):\n",
        "                            # Use smaller initialization for better gradient flow\n",
        "                            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                            if m.bias is not None:\n",
        "                                nn.init.zeros_(m.bias)\n",
        "                        elif isinstance(m, nn.Linear):\n",
        "                            # Smaller initialization for linear layers\n",
        "                            nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
        "                            if m.bias is not None:\n",
        "                                nn.init.zeros_(m.bias)\n",
        "                        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "                            if m.weight is not None:\n",
        "                                nn.init.ones_(m.weight)\n",
        "                            if m.bias is not None:\n",
        "                                nn.init.zeros_(m.bias)\n",
        "\n",
        "                def forward(self, x):\n",
        "                    # Feature extraction\n",
        "                    x = self.features(x)\n",
        "\n",
        "                    # Flatten\n",
        "                    x = torch.flatten(x, 1)\n",
        "\n",
        "                    # Classification with gradient clipping\n",
        "                    x = self.classifier(x)\n",
        "\n",
        "                    # Clip outputs to prevent extreme values\n",
        "                    x = torch.clamp(x, min=-10, max=10)\n",
        "\n",
        "                    return x\n",
        "\n",
        "            # # Example usage\n",
        "            # model = SimpleCNN(num_classes=num_classes, dropout_rate=dropout_rate, hidden_dim_multiplier=hidden_dim_multiplier)\n",
        "            # model = model.to(Config.DEVICE)  # Move to device right after creation\n",
        "            model = SimpleCNN(num_classes=num_classes, dropout_rate=dropout_rate, hidden_dim_multiplier=hidden_dim_multiplier)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "Pxu1Cn5CXx-B"
      },
      "id": "Pxu1Cn5CXx-B",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 7: Ensamble Model Architecture"
      ],
      "metadata": {
        "id": "HMjRUeNFW-m_"
      },
      "id": "HMjRUeNFW-m_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "# 6. ENSEMBLE METHODS\n",
        "# ================================================================================================================================\n",
        "# Purpose: Implement ensemble methods (simple, weighted, confidence-based, learnable).\n",
        "\n",
        "class EnsembleManager:\n",
        "    def __init__(self, models_dict, val_data):\n",
        "        self.models = models_dict\n",
        "        self.X_val, self.y_val = val_data\n",
        "        self.model_predictions = self._get_predictions()\n",
        "        self.histories = {}\n",
        "\n",
        "    def _get_predictions(self):\n",
        "        print(\"Getting model predictions for ensemble...\")\n",
        "        predictions = {}\n",
        "\n",
        "        val_dataset = FishDataset(self.X_val, self.y_val, DataManager.get_transforms(False))\n",
        "        val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_probs = []\n",
        "            all_losses = []\n",
        "            all_labels = []\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
        "                    outputs = model(images)\n",
        "                    probabilities = torch.softmax(outputs, dim=1)\n",
        "                    loss = criterion(outputs, labels).item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_probs.extend(probabilities.cpu().numpy())\n",
        "                    all_losses.append(loss)\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            accuracy = correct / total\n",
        "            f1 = f1_score(self.y_val, all_preds, average='macro')\n",
        "            avg_loss = np.mean(all_losses)\n",
        "\n",
        "            predictions[name] = {\n",
        "                'predictions': np.array(all_preds),\n",
        "                'probabilities': np.array(all_probs),\n",
        "                'loss': avg_loss,\n",
        "                'accuracy': accuracy,\n",
        "                'f1': f1,\n",
        "                'true_labels': np.array(all_labels)\n",
        "            }\n",
        "\n",
        "            print(f\"  {name}: F1 = {f1:.4f}, Acc = {accuracy:.4f}, Loss = {avg_loss:.4f}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def simple_average_ensemble(self, model_combo):\n",
        "        selected_probs = [self.model_predictions[name]['probabilities'] for name in model_combo]\n",
        "        avg_probs = np.mean(selected_probs, axis=0)\n",
        "        predictions = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(self.y_val, predictions)\n",
        "        f1 = f1_score(self.y_val, predictions, average='macro')\n",
        "        loss = np.mean([self.model_predictions[name]['loss'] for name in model_combo])\n",
        "\n",
        "        avg_probs = np.mean(selected_probs, axis=0) if selected_probs else np.zeros((len(self.y_val), Config.NUM_CLASSES))\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'loss': loss,\n",
        "            'predictions': predictions,\n",
        "            'models': model_combo,\n",
        "            # 'probabilities': avg_probs,\n",
        "            'probabilities': avg_probs if avg_probs.ndim == 2 else np.zeros((0, Config.NUM_CLASSES)),\n",
        "            'true_labels': self.y_val\n",
        "        }\n",
        "\n",
        "    def weighted_average_ensemble(self, model_combo):\n",
        "        weights = []\n",
        "        selected_probs = []\n",
        "\n",
        "        for name in model_combo:\n",
        "            f1 = self.model_predictions[name]['f1']\n",
        "            weights.append(f1)\n",
        "            selected_probs.append(self.model_predictions[name]['probabilities'])\n",
        "\n",
        "        weights = np.array(weights) / np.sum(weights)\n",
        "        weighted_probs = np.average(selected_probs, axis=0, weights=weights)\n",
        "        predictions = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(self.y_val, predictions)\n",
        "        f1 = f1_score(self.y_val, predictions, average='macro')\n",
        "        loss = np.average([self.model_predictions[name]['loss'] for name in model_combo], weights=weights)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'loss': loss,\n",
        "            'predictions': predictions,\n",
        "            'weights': weights,\n",
        "            'models': model_combo,\n",
        "            'probabilities': weighted_probs,\n",
        "            'true_labels': self.y_val\n",
        "        }\n",
        "\n",
        "    def confidence_based_ensemble(self, model_combo):\n",
        "        final_predictions = []\n",
        "        all_probs = []\n",
        "\n",
        "        for i in range(len(self.y_val)):\n",
        "            confidences = []\n",
        "            probs = []\n",
        "\n",
        "            for name in model_combo:\n",
        "                prob = self.model_predictions[name]['probabilities'][i]\n",
        "                confidence = np.max(prob)\n",
        "                confidences.append(confidence)\n",
        "                probs.append(prob)\n",
        "\n",
        "            confidences = np.array(confidences)\n",
        "            weights = confidences / np.sum(confidences) if np.sum(confidences) > 0 else np.ones(len(confidences)) / len(confidences)\n",
        "\n",
        "            final_prob = np.average(probs, axis=0, weights=weights)\n",
        "            final_predictions.append(np.argmax(final_prob))\n",
        "            all_probs.append(final_prob)\n",
        "\n",
        "        predictions = np.array(final_predictions)\n",
        "        accuracy = accuracy_score(self.y_val, predictions)\n",
        "        f1 = f1_score(self.y_val, predictions, average='macro')\n",
        "        loss = np.mean([self.model_predictions[name]['loss'] for name in model_combo])\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'loss': loss,\n",
        "            'predictions': predictions,\n",
        "            'models': model_combo,\n",
        "            'probabilities': np.array(all_probs),\n",
        "            'true_labels': self.y_val\n",
        "        }\n",
        "\n",
        "    def learnable_weighted_ensemble(self, model_combo, epochs=30):\n",
        "        print(f\"Training learnable weighted ensemble with {len(model_combo)} models...\")\n",
        "\n",
        "        selected_probs = []\n",
        "        for name in model_combo:\n",
        "            selected_probs.append(self.model_predictions[name]['probabilities'])\n",
        "\n",
        "        ensemble_input = np.stack(selected_probs, axis=1)\n",
        "\n",
        "        X_ensemble = torch.FloatTensor(ensemble_input).to(Config.DEVICE)\n",
        "        y_ensemble = torch.LongTensor(self.y_val).to(Config.DEVICE)\n",
        "\n",
        "        ensemble_model = LearnableWeightedEnsemble(\n",
        "            num_models=len(model_combo),\n",
        "            num_classes=Config.NUM_CLASSES\n",
        "        ).to(Config.DEVICE)\n",
        "\n",
        "        optimizer = optim.AdamW(ensemble_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        history = {'train_loss': [], 'train_acc': [], 'val_f1': []}\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            ensemble_model.train()\n",
        "            optimizer.zero_grad()\n",
        "            predictions, weights = ensemble_model(X_ensemble)\n",
        "            loss = criterion(predictions, y_ensemble)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            accuracy = accuracy_score(y_ensemble.cpu().numpy(), predictions.argmax(dim=1).cpu().numpy())\n",
        "            f1 = f1_score(y_ensemble.cpu().numpy(), predictions.argmax(dim=1).cpu().numpy(), average='macro')\n",
        "\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['train_acc'].append(accuracy)\n",
        "            history['val_f1'].append(f1)\n",
        "\n",
        "            print(f\"Ensemble Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, \"\n",
        "                  f\"Acc = {accuracy:.4f}, F1 = {f1:.4f}\")\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                torch.save(ensemble_model.state_dict(), f\"{Config.OUTPUT_DIR}/models/learnable_ensemble_{'+'.join(model_combo)}.pt\")\n",
        "\n",
        "        ensemble_model.load_state_dict(torch.load(f\"{Config.OUTPUT_DIR}/models/learnable_ensemble_{'+'.join(model_combo)}.pt\"))\n",
        "        ensemble_model.eval()\n",
        "        with torch.no_grad():\n",
        "            final_predictions, learned_weights = ensemble_model(X_ensemble)\n",
        "            predictions = final_predictions.argmax(dim=1).cpu().numpy()\n",
        "            probabilities = torch.softmax(final_predictions, dim=1).cpu().numpy()\n",
        "            avg_weights = learned_weights.mean(dim=0).cpu().numpy()\n",
        "\n",
        "        accuracy = accuracy_score(self.y_val, predictions)\n",
        "        f1 = f1_score(self.y_val, predictions, average='macro')\n",
        "        loss = np.mean([self.model_predictions[name]['loss'] for name in model_combo])\n",
        "\n",
        "        self.histories[f\"learnable_weighted_{'+'.join(model_combo)}\"] = history\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'loss': loss,\n",
        "            'predictions': predictions,\n",
        "            'models': model_combo,\n",
        "            'learned_weights': avg_weights,\n",
        "            'probabilities': probabilities,\n",
        "            'true_labels': self.y_val\n",
        "        }\n",
        "\n",
        "    def test_ensemble_combinations(self):\n",
        "        print(\"Testing ensemble combinations...\")\n",
        "\n",
        "        model_names = list(self.models.keys())\n",
        "        all_results = {}\n",
        "        best_result = None\n",
        "        best_score = 0\n",
        "\n",
        "        for size in range(2, min(len(model_names) + 1, 5)):\n",
        "            print(f\"Testing {size}-model combinations...\")\n",
        "\n",
        "            for combo in list(combinations(model_names, size))[:5]:\n",
        "                combo_name = f\"combo_{size}_{'+'.join(combo)}\"\n",
        "\n",
        "                for method_name in Config.ENSEMBLE_METHODS:\n",
        "                    full_name = f\"{combo_name}_{method_name}\"\n",
        "\n",
        "                    try:\n",
        "                        if method_name == 'simple_average':\n",
        "                            result = self.simple_average_ensemble(combo)\n",
        "                        elif method_name == 'weighted_average':\n",
        "                            result = self.weighted_average_ensemble(combo)\n",
        "                        elif method_name == 'confidence_based':\n",
        "                            result = self.confidence_based_ensemble(combo)\n",
        "                        elif method_name == 'learnable_weighted':\n",
        "                            result = self.learnable_weighted_ensemble(combo)\n",
        "\n",
        "                        # Verify result contains required keys\n",
        "                        required_keys = ['accuracy', 'f1', 'loss', 'predictions', 'models', 'probabilities', 'true_labels']\n",
        "                        if not all(key in result for key in required_keys):\n",
        "                            missing = [key for key in required_keys if key not in result]\n",
        "                            print(f\"  {full_name}: Missing keys {missing}\")\n",
        "                            continue\n",
        "                        # Ensure probabilities is 2D\n",
        "                        if 'probabilities' in result and (result['probabilities'].ndim != 2 or result['probabilities'].shape[1] != Config.NUM_CLASSES):\n",
        "                            result['probabilities'] = np.zeros((len(result['true_labels']), Config.NUM_CLASSES))\n",
        "\n",
        "                        all_results[full_name] = result\n",
        "                        print(f\"  {full_name}: F1 = {result['f1']:.4f}, Acc = {result['accuracy']:.4f}, \"\n",
        "                              f\"Loss = {result['loss']:.4f}, True Labels Shape = {result['true_labels'].shape}\")\n",
        "\n",
        "                        if result['f1'] > best_score:\n",
        "                            best_score = result['f1']\n",
        "                            best_result = (full_name, result)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  {full_name}: FAILED - {str(e)}\")\n",
        "\n",
        "        if best_result:\n",
        "            print(f\"\\n‚úì Best ensemble: {best_result[0]} (F1: {best_result[1]['f1']:.4f})\")\n",
        "        else:\n",
        "            print(\"\\nNo valid ensemble results generated.\")\n",
        "\n",
        "        return all_results, best_result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LEARNABLE WEIGHTED ENSEMBLE MODEL\n",
        "# ===============================================================================================================================\n",
        "# Purpose: Define a neural network for learning optimal ensemble weights.\n",
        "\n",
        "class LearnableWeightedEnsemble(nn.Module):\n",
        "    \"\"\"Ensemble model with per-class adaptive weights and attention\"\"\"\n",
        "    def __init__(self, num_models, num_classes, hidden_dim=128, num_heads=4):\n",
        "        super(LearnableWeightedEnsemble, self).__init__()\n",
        "        self.num_models = num_models\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Attention mechanism to learn relations between model predictions\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=num_classes, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        # Weight network outputs per-class weights for each model\n",
        "        self.weight_network = nn.Sequential(\n",
        "            nn.Linear(num_classes, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "            nn.Sigmoid()  # Per-class weight scaling\n",
        "        )\n",
        "\n",
        "        # Prediction head: combines weighted predictions + raw predictions\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(num_classes * (num_models + 1), hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_classes * 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, model_predictions):\n",
        "        \"\"\"\n",
        "        model_predictions: (batch, num_models, num_classes)\n",
        "        Returns:\n",
        "            final_predictions: logits for classification\n",
        "            weights: learned per-class weights for each model\n",
        "        \"\"\"\n",
        "        batch_size = model_predictions.size(0)\n",
        "\n",
        "        # --- Step 1: Attention over model predictions --- #The model looks at how predictions of different models relate to each other.\n",
        "        attn_output, _ = self.attention(model_predictions, model_predictions, model_predictions)\n",
        "        # shape: (batch, num_models, num_classes)\n",
        "\n",
        "\n",
        "        # --- Step 2: Per-class weights for each model ---\n",
        "        #Learns a weight for each model for each class.\n",
        "        #softmax ensures weights across models sum to 1 for each class.\n",
        "        #Basically: ‚ÄúFor class 0, I trust model 2 more; for class 1, I trust model 0 more.‚Äù\n",
        "        weights = self.weight_network(attn_output)  # (batch, num_models, num_classes)\n",
        "        weights = F.softmax(weights, dim=1)  # normalize over models\n",
        "\n",
        "\n",
        "        # --- Step 3: Weighted average across models ---\n",
        "        #Combines the models‚Äô predictions using the learned weights ‚Üí smarter than a plain average.\n",
        "        weighted_avg = torch.sum(model_predictions * weights, dim=1)  # (batch, num_classes)\n",
        "\n",
        "\n",
        "        # --- Step 4: Residual connection with raw predictions ---\n",
        "        #Combines the weighted average and all raw predictions.Gives the network more info to refine the final prediction.\n",
        "        flat_preds = model_predictions.view(batch_size, -1)  # (batch, num_models * num_classes)\n",
        "        final_input = torch.cat([weighted_avg, flat_preds], dim=1)  # (batch, num_classes + num_models*num_classes)\n",
        "\n",
        "\n",
        "        # --- Step 5: Final refined prediction ---\n",
        "        #A small feed-forward network refines the predictions.Output: (batch_size, num_classes) ‚Üí logits for each class.\n",
        "        final_predictions = self.prediction_head(final_input)  # (batch, num_classes)\n",
        "\n",
        "        return final_predictions, weights\n",
        "        #It learns which model is best for each class, combines their predictions smartly using attention, and produces a refined final prediction.\n",
        "\n",
        "    # def entropy_regularization(self, weights):\n",
        "    #     \"\"\"Encourage diverse weight usage (optional loss term).\"\"\"\n",
        "    #     # weights: (batch, num_models, num_classes)\n",
        "    #     entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=1)  # (batch, num_classes)\n",
        "    #     return torch.mean(entropy)\n"
      ],
      "metadata": {
        "id": "pWZlr59kXHue"
      },
      "id": "pWZlr59kXHue",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 8: üìä Optuna Trials [Hyper-parameter Tuning]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DgnPYu28n6Db"
      },
      "id": "DgnPYu28n6Db"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optimized for maximum GPU utilization and enhanced user experience\n",
        "from threading import Lock\n",
        "from termcolor import colored, cprint\n",
        "\n",
        "import warnings\n",
        "from optuna.exceptions import ExperimentalWarning\n",
        "# Suppress only ExperimentalWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# # Configure logging\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    #Initialize worker with different random seed\n",
        "    np.random.seed(torch.initial_seed() % 2**32 + worker_id)\n",
        "\n",
        "class Optuna_DataManager:\n",
        "\n",
        "    @staticmethod\n",
        "    def create_data_loaders(X, Y, train_batch_size=64, val_batch_size=128,\n",
        "                                    test_size=0.2, augmentation_strength='medium',\n",
        "                                    num_workers=8, pin_memory=True, persistent_workers=True):\n",
        "\n",
        "        # Split data strategically\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            X, Y, test_size=test_size, random_state=42, stratify=Y\n",
        "        )\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        # OPTIMIZATION: Delete intermediate variables immediately to save CPU RAM\n",
        "        del X_temp, y_temp\n",
        "        gc.collect()\n",
        "\n",
        "        cprint(f\"üìä Data Distribution:\", 'cyan', attrs=['bold'])\n",
        "        print(f\"   Train: {len(X_train):,} samples\")\n",
        "        print(f\"   Val:   {len(X_val):,} samples\")\n",
        "        print(f\"   Test:  {len(X_test):,} samples\")\n",
        "        print(f\"   Batch: Train={train_batch_size}, Val={val_batch_size}\")\n",
        "\n",
        "        # Create datasets with transforms (assuming these classes exist)\n",
        "        # You need to define these or import them\n",
        "        try:\n",
        "            # from your_data_module import FishDataset, DataManager  # Replace with actual imports\n",
        "            train_dataset = FishDataset(X_train, y_train, DataManager.get_transforms(True, augmentation_strength))\n",
        "            val_dataset = FishDataset(X_val, y_val, DataManager.get_transforms(False))\n",
        "            test_dataset = FishDataset(X_test, y_test, DataManager.get_transforms(False))\n",
        "        except ImportError:\n",
        "            raise ImportError(\"FishDataset and DataManager classes not found. Please ensure they are imported.\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error creating datasets: {e}\")\n",
        "\n",
        "        # OPTIMIZATION: More efficient class weight calculation to save memory\n",
        "        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "        class_weights = len(y_train) / (len(unique_classes) * class_counts)\n",
        "        class_weight_dict = dict(zip(unique_classes, class_weights))\n",
        "        sample_weights = [class_weight_dict[y] for y in y_train]\n",
        "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "        # Clean up intermediate weight calculations\n",
        "        del class_weights, class_weight_dict, sample_weights\n",
        "\n",
        "        # OPTIMIZATION: Reduce CPU workers to save RAM, prioritize GPU feeding\n",
        "        if num_workers is None:\n",
        "            if torch.cuda.is_available():\n",
        "                # More conservative worker count to save CPU RAM\n",
        "                num_workers = min(os.cpu_count() // 2, 8)  # Reduced from 16\n",
        "            else:\n",
        "                num_workers = 2  # Minimal for CPU-only\n",
        "        else:\n",
        "            # Cap the provided num_workers to save CPU RAM\n",
        "            num_workers = min(num_workers, os.cpu_count() // 2, 8)\n",
        "\n",
        "        # OPTIMIZATION: Reduce prefetch factor to save CPU memory\n",
        "        prefetch_factor = 2 if torch.cuda.is_available() else 1  # Reduced from 4\n",
        "\n",
        "        # Create GPU-optimized data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=train_batch_size,\n",
        "            sampler=sampler,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "            prefetch_factor=prefetch_factor,\n",
        "            persistent_workers=persistent_workers and num_workers > 0,\n",
        "            worker_init_fn=worker_init_fn,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=val_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "            prefetch_factor=prefetch_factor,\n",
        "            persistent_workers=persistent_workers and num_workers > 0,\n",
        "            worker_init_fn=worker_init_fn\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=val_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "            prefetch_factor=prefetch_factor,\n",
        "            persistent_workers=persistent_workers and num_workers > 0,\n",
        "            worker_init_fn=worker_init_fn\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader, (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "def setup_maximum_gpu_utilization() -> Tuple[int, float, float]:\n",
        "    \"\"\"Setup GPU optimizations with proper error handling\"\"\"\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    cprint(\"üöÄ SETTING UP MAXIMUM GPU UTILIZATION\", 'red', attrs=['bold'])\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    gpu_memory_gb = 0.0\n",
        "    if torch.cuda.is_available():\n",
        "        # Aggressive GPU optimizations\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "        # Maximum performance settings\n",
        "        if hasattr(torch.backends.cuda.matmul, 'allow_tf32'):\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        # Fixed: Proper Flash Attention setup\n",
        "        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):\n",
        "            torch.backends.cuda.enable_flash_sdp(True)\n",
        "\n",
        "        # OPTIMIZATION: Use more aggressive GPU memory (increased to 95%)\n",
        "        torch.cuda.set_per_process_memory_fraction(0.95)\n",
        "\n",
        "        # Get GPU specifications\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            gpu_props = torch.cuda.get_device_properties(i)\n",
        "            gpu_memory_gb = max(gpu_memory_gb, gpu_props.total_memory / 1e9)\n",
        "\n",
        "            cprint(f\"üéÆ GPU {i}: {gpu_props.name}\", 'green', attrs=['bold'])\n",
        "            print(f\"   Memory: {gpu_memory_gb:.1f}GB\")\n",
        "            print(f\"   Compute: {gpu_props.major}.{gpu_props.minor}\")\n",
        "            print(f\"   Cores: {gpu_props.multi_processor_count}\")\n",
        "\n",
        "        # Set multi-GPU if available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            cprint(f\"üî• Using {torch.cuda.device_count()} GPUs!\", 'red', attrs=['bold'])\n",
        "    else:\n",
        "        cprint(\"‚ö†Ô∏è  No GPU available - using CPU only\", 'yellow', attrs=['bold'])\n",
        "\n",
        "    # OPTIMIZATION: Conservative CPU optimizations to save RAM\n",
        "    cpu_count = os.cpu_count()\n",
        "    optimal_threads = min(cpu_count // 2, 8)  # More conservative to save CPU RAM\n",
        "\n",
        "    torch.set_num_threads(optimal_threads)\n",
        "    os.environ['OMP_NUM_THREADS'] = str(optimal_threads)\n",
        "    os.environ['MKL_NUM_THREADS'] = str(optimal_threads)\n",
        "    os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)\n",
        "\n",
        "    # Memory information\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    available_ram = memory_info.available / (1024**3)\n",
        "    total_ram = memory_info.total / (1024**3)\n",
        "\n",
        "    cprint(f\"üíª CPU: {cpu_count} cores (using {optimal_threads})\", 'blue', attrs=['bold'])\n",
        "    cprint(f\"üß† RAM: {total_ram:.1f}GB total, {available_ram:.1f}GB available\", 'blue', attrs=['bold'])\n",
        "\n",
        "    return optimal_threads, available_ram, gpu_memory_gb\n",
        "\n",
        "\n",
        "def get_maximum_batch_sizes(model_name: str, available_ram_gb: float, gpu_memory_gb: float) -> Tuple[int, int]:\n",
        "    \"\"\"Calculate maximum batch sizes for full GPU utilization - ENHANCED for better GPU usage\"\"\"\n",
        "\n",
        "    # OPTIMIZATION: More aggressive base batch sizes for better GPU utilization\n",
        "    base_batch_sizes = {\n",
        "        'resnet50': {'train': 96, 'val': 192},           # Increased from 64/128\n",
        "        'efficientnet_b0': {'train': 128, 'val': 256},   # Increased from 96/192\n",
        "        'mobilenet_v3_large': {'train': 160, 'val': 320}, # Increased from 128/256\n",
        "        'vgg16': {'train': 48, 'val': 96},               # Increased from 32/64\n",
        "        'densenet121': {'train': 64, 'val': 128},        # Increased from 48/96\n",
        "        'inception_v3': {'train': 56, 'val': 112},       # Increased from 40/80\n",
        "        'vit_b_16': {'train': 48, 'val': 96},            # Increased from 32/64\n",
        "        'convnext_base': {'train': 48, 'val': 96},       # Increased from 36/72\n",
        "        'regnet_y_32gf': {'train': 32, 'val': 64}       # Increased from 24/48\n",
        "    }\n",
        "\n",
        "    # OPTIMIZATION: More aggressive GPU memory scaling\n",
        "    if gpu_memory_gb >= 24:  # High-end GPU (RTX 4090, A100)\n",
        "        gpu_multiplier = 2.5  # Increased from 1.8\n",
        "    elif gpu_memory_gb >= 16:  # Mid-range GPU (RTX 4080, 3090)\n",
        "        gpu_multiplier = 2.0  # Increased from 1.5\n",
        "    elif gpu_memory_gb >= 12:  # RTX 4070Ti, 3080Ti\n",
        "        gpu_multiplier = 1.7  # New tier\n",
        "    elif gpu_memory_gb >= 8:   # Entry-level GPU (RTX 3070, 4060Ti)\n",
        "        gpu_multiplier = 1.3  # Increased from 1.2\n",
        "    else:\n",
        "        gpu_multiplier = 1.0\n",
        "\n",
        "    # RAM scaling - less conservative since we're prioritizing GPU\n",
        "    ram_multiplier = min(1.5, available_ram_gb / 16)  # Reduced impact\n",
        "    total_multiplier = gpu_multiplier * 0.8 + ram_multiplier * 0.2  # 80% GPU focus, 20% RAM\n",
        "\n",
        "    model_key = model_name.lower()\n",
        "    if model_key not in base_batch_sizes:\n",
        "        model_key = 'resnet50'\n",
        "\n",
        "    base_train = base_batch_sizes[model_key]['train']\n",
        "    base_val = base_batch_sizes[model_key]['val']\n",
        "\n",
        "    train_batch = int(base_train * total_multiplier)\n",
        "    val_batch = int(base_val * total_multiplier)\n",
        "\n",
        "    # OPTIMIZATION: Higher minimum viable sizes for better GPU utilization\n",
        "    train_batch = max(32, train_batch)  # Increased from 16\n",
        "    val_batch = max(64, val_batch)      # Increased from 32\n",
        "\n",
        "    return train_batch, val_batch\n",
        "\n",
        "\n",
        "class HyperparameterOptimizer:\n",
        "    \"\"\"Enhanced hyperparameter optimizer with proper error handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, train_loader, val_loader, n_trials: int = 100,\n",
        "                  train_batch_size: int = 64, val_batch_size: int = 128, X = None , Y = None ):\n",
        "        self.model_name = model_name\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.n_trials = n_trials\n",
        "        self.train_batch_size = train_batch_size  # ADD THIS\n",
        "        self.val_batch_size = val_batch_size      # ADD THIS\n",
        "        self.X = X  # ADD THIS\n",
        "        self.Y = Y  # ADD THIS\n",
        "\n",
        "        # Set Google Drive path\n",
        "        self.drive_path = '/content/drive/MyDrive/Hilsha'\n",
        "        os.makedirs(self.drive_path, exist_ok=True)\n",
        "\n",
        "        # Use all available GPUs\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda:0')\n",
        "            self.use_multi_gpu = torch.cuda.device_count() > 1\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            self.use_multi_gpu = False\n",
        "\n",
        "        self.lock = Lock()\n",
        "        self.best_accuracy = 0.0\n",
        "        self.current_trial = 0\n",
        "\n",
        "        # Track best trial information\n",
        "        self.best_trial_info = {\n",
        "            'trial_number': 0,\n",
        "            'accuracy': 0.0,\n",
        "            'train_loss': 0.0,\n",
        "            'val_loss': 0.0,\n",
        "            'train_acc': 0.0,\n",
        "            'val_acc': 0.0,\n",
        "            'train_f1': 0.0,\n",
        "            'val_f1': 0.0,\n",
        "            'hyperparameters': {}\n",
        "        }\n",
        "\n",
        "    def suggest_hyperparameters(self, trial) -> Dict[str, Any]:\n",
        "        return {\n",
        "            # More conservative learning rate range for better convergence\n",
        "            'lr': trial.suggest_float('lr', 5e-6, 5e-3, log=True),\n",
        "\n",
        "            # Wider weight decay range for better regularization\n",
        "            'weight_decay': trial.suggest_float('weight_decay', 1e-7, 5e-2, log=True),\n",
        "\n",
        "            # Add RMSprop which works well for many vision models\n",
        "            'optimizer': trial.suggest_categorical('optimizer', ['adamw', 'adam', 'sgd', 'rmsprop']),\n",
        "\n",
        "            # Add more scheduler options including warmup restart\n",
        "            'scheduler': trial.suggest_categorical('scheduler', ['cosine', 'cosine_warm', 'step', 'plateau']),\n",
        "\n",
        "            # Reduce label smoothing max for better accuracy\n",
        "            'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.15),\n",
        "\n",
        "            # More conservative gradient clipping\n",
        "            'gradient_clip': trial.suggest_float('gradient_clip', 0.5, 1.5),\n",
        "\n",
        "            # Extended warmup range\n",
        "            'warmup_epochs': trial.suggest_int('warmup_epochs', 0, 5),\n",
        "\n",
        "            # Model-specific dropout based on architecture\n",
        "            'dropout': self._get_model_specific_dropout(trial),\n",
        "\n",
        "            # Flexible batch size multipliers\n",
        "            # Get batch multipliers first\n",
        "            'train_batch_multiplier': trial.suggest_categorical('train_batch_multiplier', [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]),\n",
        "            'val_batch_multiplier': trial.suggest_categorical('val_batch_multiplier', [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]),\n",
        "\n",
        "            # Calculate and round to nearest power of 2 for GPU efficiency\n",
        "            'train_batch_size': max(8, 2 ** round(__import__('math').log2(max(8, int(self.train_batch_size * trial.params['train_batch_multiplier']))))),\n",
        "            'val_batch_size': max(16, 2 ** round(__import__('math').log2(max(16, int(self.val_batch_size * trial.params['val_batch_multiplier'])))))\n",
        "        }\n",
        "\n",
        "    def _get_model_specific_dropout(self, trial):\n",
        "        \"\"\"Get model-specific dropout ranges\"\"\"\n",
        "        if 'vgg' in self.model_name.lower():\n",
        "            # VGG needs higher dropout\n",
        "            return trial.suggest_float('dropout', 0.3, 0.7)\n",
        "        elif 'mobilenet' in self.model_name.lower():\n",
        "            # MobileNet is already regularized\n",
        "            return trial.suggest_float('dropout', 0.1, 0.4)\n",
        "        elif 'efficientnet' in self.model_name.lower():\n",
        "            # EfficientNet has built-in regularization\n",
        "            return trial.suggest_float('dropout', 0.1, 0.4)\n",
        "        elif 'inception' in self.model_name.lower():\n",
        "            # Inception needs moderate dropout\n",
        "            return trial.suggest_float('dropout', 0.2, 0.5)\n",
        "        else:\n",
        "            # ResNet, DenseNet - standard range\n",
        "            return trial.suggest_float('dropout', 0.1, 0.5)\n",
        "\n",
        "    def create_model_with_params(self, params: Dict[str, Any]):\n",
        "        \"\"\"Create model with parameters - you need to implement this\"\"\"\n",
        "        # This is a placeholder - implement your model creation logic\n",
        "        try:\n",
        "            # from your_model_module import ModelFactory  # Replace with actual import\n",
        "            model = ModelFactory.create_model(self.model_name, params)\n",
        "            if self.use_multi_gpu:\n",
        "                model = nn.DataParallel(model)\n",
        "            return model.to(self.device)\n",
        "        except ImportError:\n",
        "            raise ImportError(\"ModelFactory not found. Please ensure it is imported.\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error creating model: {e}\")\n",
        "\n",
        "    def display_hyperparameters(self, trial_num: int, params: Dict[str, Any]):\n",
        "        \"\"\"Display hyperparameters in a formatted way\"\"\"\n",
        "        print(\"\\n\" + \"üîß\" * 40)\n",
        "        cprint(f\"üìã TRIAL {trial_num} HYPERPARAMETERS - {self.model_name.upper()}\", 'cyan', attrs=['bold'])\n",
        "        print(\"üîß\" * 40)\n",
        "\n",
        "        # Display batch configuration with multipliers\n",
        "        cprint(\"  üéØ BATCH CONFIGURATION:\", 'yellow', attrs=['bold'])\n",
        "        print(f\"    üîπ {'base_train_batch':<20}: {self.train_batch_size}\")\n",
        "        print(f\"    üîπ {'train_multiplier':<20}: {params.get('train_batch_multiplier', 1.0)}\")\n",
        "        print(f\"    üîπ {'final_train_batch':<20}: {params['train_batch_size']}\")\n",
        "        print(f\"    üîπ {'base_val_batch':<20}: {self.val_batch_size}\")\n",
        "        print(f\"    üîπ {'val_multiplier':<20}: {params.get('val_batch_multiplier', 1.0)}\")\n",
        "        print(f\"    üîπ {'final_val_batch':<20}: {params['val_batch_size']}\")\n",
        "\n",
        "        # Display other hyperparameters\n",
        "        cprint(\"  üéØ HYPERPARAMETERS:\", 'yellow', attrs=['bold'])\n",
        "        skip_keys = ['train_batch_size', 'val_batch_size', 'train_batch_multiplier', 'val_batch_multiplier']\n",
        "        for key, value in params.items():\n",
        "            if key not in skip_keys:\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"    üîπ {key:<20}: {value:.8f}\")\n",
        "                else:\n",
        "                    print(f\"    üîπ {key:<20}: {value}\")\n",
        "        print(\"üîß\" * 40)\n",
        "\n",
        "    def display_best_trial_status(self):\n",
        "        \"\"\"Display current best trial information\"\"\"\n",
        "        # print(\"\\n\" + \"üèÜ\" * 40)\n",
        "        cprint(f\"üëë CURRENT BEST TRIAL STATUS - {self.model_name.upper()}\", 'red', attrs=['bold'])\n",
        "        print(\"üèÜ\" * 40)\n",
        "\n",
        "        if self.best_trial_info['trial_number'] > 0:\n",
        "            cprint(f\"  ü•á Best Trial : #{self.best_trial_info['trial_number']}\", 'yellow', attrs=['bold'])\n",
        "            cprint(f\"  üéØ Best Accuracy: {self.best_trial_info['accuracy']:.4f}%\", 'green', attrs=['bold'])\n",
        "\n",
        "            # Display metrics\n",
        "            print(f\"  üìä METRICS:\")\n",
        "            print(f\"    üî∏ Train Loss:     {self.best_trial_info['train_loss']:.6f}\")\n",
        "            print(f\"    üî∏ Val Loss:       {self.best_trial_info['val_loss']:.6f}\")\n",
        "            print(f\"    üî∏ Train Accuracy: {self.best_trial_info['train_acc']:.4f}%\")\n",
        "            print(f\"    üî∏ Val Accuracy:   {self.best_trial_info['val_acc']:.4f}%\")\n",
        "            print(f\"    üî∏ Train F1:       {self.best_trial_info['train_f1']:.4f}%\")\n",
        "            print(f\"    üî∏ Val F1:         {self.best_trial_info['val_f1']:.4f}%\")\n",
        "        else:\n",
        "            cprint(\"  üîÑ No trials completed yet\", 'yellow')\n",
        "\n",
        "        # print(\"üèÜ\" * 80)\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, model, params: Dict[str, Any], steps_per_epoch: int):\n",
        "        \"\"\"Create optimizer and scheduler with GPU optimizations\"\"\"\n",
        "\n",
        "        # Create optimizer\n",
        "        if params['optimizer'] == 'adamw':\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'],\n",
        "                betas=(0.9, 0.999), eps=1e-8\n",
        "            )\n",
        "        elif params['optimizer'] == 'adam':\n",
        "            optimizer = torch.optim.Adam(\n",
        "                model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'],\n",
        "                betas=(0.9, 0.999), eps=1e-8\n",
        "            )\n",
        "        elif params['optimizer'] == 'sgd':\n",
        "            optimizer = torch.optim.SGD(\n",
        "                model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'],\n",
        "                momentum=0.9, nesterov=True\n",
        "            )\n",
        "        else:  # rmsprop\n",
        "            optimizer = torch.optim.RMSprop(\n",
        "                model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'],\n",
        "                momentum=0.9, alpha=0.99\n",
        "            )\n",
        "\n",
        "        # Create scheduler\n",
        "        if params['scheduler'] == 'cosine':\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "        elif params['scheduler'] == 'cosine_warm':\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=2)\n",
        "        elif params['scheduler'] == 'step':\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
        "        else:  # plateau\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='max', patience=2, factor=0.5\n",
        "            )\n",
        "\n",
        "        return optimizer, scheduler\n",
        "\n",
        "    def train_and_validate(self, model, params: Dict[str, Any], train_loader, val_loader, epochs: int=None, trial=None) -> Tuple[float, Dict]:\n",
        "        #Enhanced training with comprehensive metrics and GPU utilization\n",
        "        if epochs is None:\n",
        "            epochs = Config.OPTUNA_EPOCHS\n",
        "\n",
        "        steps_per_epoch = len(train_loader)\n",
        "        optimizer, scheduler = self.create_optimizer_and_scheduler(model, params, steps_per_epoch)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=params.get('label_smoothing', 0.0))\n",
        "        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "        # OPTIMIZATION: Enable aggressive GPU optimizations\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cudnn.deterministic = False\n",
        "            # Reset peak memory stats for accurate monitoring\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # OPTIMIZATION: Use larger gradient accumulation for better GPU utilization\n",
        "        gradient_accumulation_steps = max(1, 128 // params.get('train_batch_size', train_loader.batch_size))\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        metrics_history = []\n",
        "        patience = 0  # ‡¶≤‡ßÅ‡¶™‡ßá‡¶∞ ‡¶¨‡¶æ‡¶á‡¶∞‡ßá initialize\n",
        "        epoch_best_f1 = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # OPTIMIZATION: Aggressive memory cleanup each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "            train_preds = []\n",
        "            train_targets = []\n",
        "\n",
        "            accumulated_loss = 0\n",
        "\n",
        "            train_pbar = tqdm(\n",
        "                train_loader,\n",
        "                desc=f\"  üèÉ Epoch {epoch+1:2d} Train\",\n",
        "                leave=False,\n",
        "                ncols=100\n",
        "            )\n",
        "\n",
        "            for batch_idx, (data, targets) in enumerate(train_pbar):\n",
        "                data, targets = data.to(self.device, non_blocking=True), targets.to(self.device, non_blocking=True)\n",
        "\n",
        "                if scaler and torch.cuda.is_available():\n",
        "                    # OPTIMIZATION: More aggressive mixed precision usage\n",
        "                    with torch.cuda.amp.autocast(enabled=True):\n",
        "                        outputs = model(data)\n",
        "                        loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    accumulated_loss += loss.item()\n",
        "\n",
        "                    # OPTIMIZATION: Better gradient accumulation for larger effective batch size\n",
        "                    if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), params.get('gradient_clip', 1.0))\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "                else:\n",
        "                    outputs = model(data)\n",
        "                    loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
        "                    loss.backward()\n",
        "\n",
        "                    if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), params.get('gradient_clip', 1.0))\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                # Metrics calculation\n",
        "                train_loss += loss.item() * gradient_accumulation_steps * data.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                train_total += targets.size(0)\n",
        "                train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "                train_preds.extend(predicted.cpu().numpy())\n",
        "                train_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                current_acc = 100 * train_correct / train_total\n",
        "                train_pbar.set_postfix({\n",
        "                    'Loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n",
        "                    'Acc': f'{current_acc:.2f}%'\n",
        "                })\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            val_preds = []\n",
        "            val_targets = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for data, targets in val_loader:\n",
        "                    data, targets = data.to(self.device, non_blocking=True), targets.to(self.device, non_blocking=True)\n",
        "\n",
        "                    if scaler and torch.cuda.is_available():\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            outputs = model(data)\n",
        "                            loss = criterion(outputs, targets)\n",
        "                    else:\n",
        "                        outputs = model(data)\n",
        "                        loss = criterion(outputs, targets)\n",
        "\n",
        "                    val_loss += loss.item() * data.size(0)\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    val_total += targets.size(0)\n",
        "                    val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "                    val_preds.extend(predicted.cpu().numpy())\n",
        "                    val_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_accuracy = 100 * train_correct / train_total\n",
        "            val_accuracy = 100 * val_correct / val_total\n",
        "            avg_train_loss = train_loss / train_total\n",
        "            avg_val_loss = val_loss / val_total\n",
        "\n",
        "            train_f1 = f1_score(train_targets, train_preds, average='weighted') * 100\n",
        "            val_f1 = f1_score(val_targets, val_preds, average='weighted') * 100\n",
        "\n",
        "            val_wrong = val_total - val_correct\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"    üìä TL:{avg_train_loss:.4f} VL:{avg_val_loss:.4f} | \" +\n",
        "                  f\"TA:{train_accuracy:.2f}% VA:{val_accuracy:.2f}% | \" +\n",
        "                  f\"TF1:{train_f1:.2f}% VF1:{val_f1:.2f}% | \" +\n",
        "                  f\"WP:{val_wrong}\")\n",
        "\n",
        "            # Store metrics\n",
        "            epoch_metrics = {\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'train_acc': train_accuracy,\n",
        "                'val_acc': val_accuracy,\n",
        "                'train_f1': train_f1,\n",
        "                'val_f1': val_f1,\n",
        "                'wrong_predictions': val_wrong\n",
        "            }\n",
        "            metrics_history.append(epoch_metrics)\n",
        "\n",
        "            # Update best accuracy\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "\n",
        "            # Update scheduler\n",
        "            if params['scheduler'] == 'plateau':\n",
        "                scheduler.step(val_accuracy)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "            # Early stopping for optimization speed\n",
        "            if epoch+1 >= 5 and val_accuracy < 50.0:\n",
        "                print(f\"‚ö†Ô∏è Early stopping cause epoch {epoch+1} but still not satisfactory accuracy obtain.\")\n",
        "                break\n",
        "\n",
        "            # Early stopping for not improvement\n",
        "            if val_f1 > epoch_best_f1 * 1.001:  # improvement condition (0.1% increment)\n",
        "                epoch_best_f1 = val_f1\n",
        "                patience = 0  # reset patience, ‡¶ï‡¶æ‡¶∞‡¶£ improvement ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá\n",
        "            else:\n",
        "                patience += 1  # no improvement, patience ‡¶¨‡¶æ‡¶°‡¶º‡¶æ‡¶ì\n",
        "            if patience > Config.PATIENCE:\n",
        "                print(f\"‚ö†Ô∏è Early stopping: No improvement for {Config.PATIENCE} consecutive epochs\")\n",
        "                break\n",
        "\n",
        "            #Early stopping for trial level pruning\n",
        "            # Report intermediate value for pruning\n",
        "            if trial is not None:\n",
        "                trial.report(val_accuracy, epoch)\n",
        "                # Check if trial should be pruned\n",
        "                if trial.should_prune():\n",
        "                    print(f\"    ‚ö†Ô∏è Early stopping at epoch {epoch+1}: Low accuracy probability detected\")\n",
        "                    print(f\"    üîÑ Pruning trial - proceeding to next hyperparameter combination\")\n",
        "                    raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        return best_val_acc, {'history': metrics_history, 'best_epoch_metrics': max(metrics_history, key=lambda x: x['val_acc'])}\n",
        "\n",
        "    def objective(self, trial) -> float:\n",
        "      \"\"\"Enhanced objective function with detailed progress tracking\"\"\"\n",
        "      self.current_trial += 1\n",
        "\n",
        "      print(\"\\n\" + \"‚ñà\" * 100)\n",
        "      cprint(f\"üî• TRIAL {self.current_trial:3d}/{self.n_trials} STARTING - {self.model_name.upper()}\", 'red', attrs=['bold'])\n",
        "      print(\"‚ñà\" * 100)\n",
        "\n",
        "      try:\n",
        "          with self.lock:\n",
        "              if torch.cuda.is_available():\n",
        "                  torch.cuda.empty_cache()\n",
        "                  for i in range(torch.cuda.device_count()):\n",
        "                      memory_used = torch.cuda.memory_allocated(i) / 1e9\n",
        "                      memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "                      print(f\"  üéÆ GPU {i}: {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)\")\n",
        "              gc.collect()\n",
        "\n",
        "          # Get hyperparameters\n",
        "          params = self.suggest_hyperparameters(trial)\n",
        "          self.display_hyperparameters(self.current_trial, params)\n",
        "\n",
        "          # Recreate data loaders with new batch sizes if they differ significantly\n",
        "          current_train_batch = params['train_batch_size']\n",
        "          current_val_batch = params['val_batch_size']\n",
        "\n",
        "          # Only recreate loaders if batch size changed significantly\n",
        "          if (abs(current_train_batch - self.train_loader.batch_size) > 8 or\n",
        "              abs(current_val_batch - self.val_loader.batch_size) > 16):\n",
        "\n",
        "              train_loader, val_loader, _, _, _ = Optuna_DataManager.create_data_loaders(\n",
        "                  self.X, self.Y,\n",
        "                  train_batch_size=current_train_batch,\n",
        "                  val_batch_size=current_val_batch,\n",
        "                  num_workers=4,  # OPTIMIZATION: Reduced workers to save CPU RAM\n",
        "                  pin_memory=True,\n",
        "                  persistent_workers=True\n",
        "              )\n",
        "              using_new_loaders = True  # ADD THIS FLAG\n",
        "          else:\n",
        "              train_loader = self.train_loader\n",
        "              val_loader = self.val_loader\n",
        "              using_new_loaders = False  # ADD THIS FLAG\n",
        "\n",
        "          # Create and train model\n",
        "          model = self.create_model_with_params(params)\n",
        "          best_acc, detailed_metrics = self.train_and_validate(\n",
        "              model, params, train_loader, val_loader, trial=trial  # PASS THE LOADERS\n",
        "          )\n",
        "\n",
        "          # SUCCESS HANDLING - Move this BEFORE the except blocks\n",
        "          best_epoch_metrics = detailed_metrics['best_epoch_metrics']\n",
        "\n",
        "          # Trial completion summary\n",
        "          print(\"  \" + \"‚îÄ\" * 80)\n",
        "          cprint(f\"  ‚úÖ TRIAL {self.current_trial} COMPLETED\", 'green', attrs=['bold'])\n",
        "          cprint(f\"  üéØ Highest Validation Accuracy for this Trial: {best_acc:.4f}%\", 'yellow', attrs=['bold'])\n",
        "\n",
        "          # Update best trial info if this is better\n",
        "          if best_acc > self.best_accuracy:\n",
        "              self.best_accuracy = best_acc\n",
        "              self.best_trial_info = {\n",
        "                  'trial_number': self.current_trial,\n",
        "                  'accuracy': best_acc,\n",
        "                  'train_loss': best_epoch_metrics['train_loss'],\n",
        "                  'val_loss': best_epoch_metrics['val_loss'],\n",
        "                  'train_acc': best_epoch_metrics['train_acc'],\n",
        "                  'val_acc': best_epoch_metrics['val_acc'],\n",
        "                  'train_f1': best_epoch_metrics['train_f1'],\n",
        "                  'val_f1': best_epoch_metrics['val_f1'],\n",
        "                  'hyperparameters': params.copy()\n",
        "              }\n",
        "              cprint(f\"  üèÜ NEW BEST ACCURACY: {best_acc:.4f}%\", 'red', attrs=['bold'])\n",
        "              # Save immediately to Google Drive only\n",
        "              self.save_best_params_immediately()\n",
        "\n",
        "          self.display_best_trial_status()\n",
        "\n",
        "          # OPTIMIZATION: Cleanup - ADD LOADER CLEANUP\n",
        "          del model\n",
        "          if using_new_loaders:  # Clean up new loaders if created\n",
        "              del train_loader, val_loader\n",
        "          if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "          gc.collect()\n",
        "\n",
        "          return best_acc\n",
        "\n",
        "      except optuna.exceptions.TrialPruned:\n",
        "          cprint(f\"  ‚úÇÔ∏è TRIAL {self.current_trial} PRUNED: Low accuracy probability detected\", 'yellow', attrs=['bold'])\n",
        "          cprint(f\"  üîÑ Skipping to next hyperparameter combination for efficiency\", 'cyan')\n",
        "          # Cleanup\n",
        "          if 'model' in locals():\n",
        "              del model\n",
        "          if 'using_new_loaders' in locals() and using_new_loaders:\n",
        "              if 'train_loader' in locals():\n",
        "                  del train_loader\n",
        "              if 'val_loader' in locals():\n",
        "                  del val_loader\n",
        "          if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "          gc.collect()\n",
        "          raise  # Re-raise the TrialPruned exception\n",
        "\n",
        "      except Exception as e:\n",
        "          cprint(f\"  ‚ùå TRIAL {self.current_trial} FAILED: {e}\", 'red', attrs=['bold'])\n",
        "          cprint(f\"  üìã Error Details: {traceback.format_exc()}\", 'yellow')\n",
        "          # Cleanup\n",
        "          if 'model' in locals():\n",
        "              del model\n",
        "          if 'using_new_loaders' in locals() and using_new_loaders:\n",
        "              if 'train_loader' in locals():\n",
        "                  del train_loader\n",
        "              if 'val_loader' in locals():\n",
        "                  del val_loader\n",
        "          if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "          gc.collect()\n",
        "          return 0.0\n",
        "\n",
        "    def optimize(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run optimization with enhanced progress tracking\"\"\"\n",
        "\n",
        "        # print(\"\\n\" + \"üöÄ\" *20)\n",
        "        cprint(f\"STARTING HYPERPARAMETER OPTIMIZATION FOR {self.model_name.upper()}\", 'red', attrs=['bold'])\n",
        "        print(\"üöÄ\" * 20)\n",
        "\n",
        "        # Create study\n",
        "        study = optuna.create_study(\n",
        "            direction='maximize',\n",
        "            sampler=optuna.samplers.TPESampler(\n",
        "                # OPTIMIZATION: Reduced startup trials to save time and CPU\n",
        "                n_startup_trials = max(10, self.n_trials // 8),  # Reduced from n_trials // 6\n",
        "                # Example: If you set n_startup_trials=10, the first 10 trials will be random, then trial 11 onwards will use TPE-guided sampling.\n",
        "                n_ei_candidates=24,  # Reduced from 32 to save CPU\n",
        "                constant_liar=True,\n",
        "                multivariate=True\n",
        "            ),\n",
        "            # Sampler's n_startup_trials ‚Üí when TPE optimization begins.\n",
        "            # Pruner's n_startup_trials ‚Üí how many full trials to finish before pruning starts.\n",
        "            # Pruner's n_warmup_steps ‚Üí how many epochs per trial to protect before pruning checks.\n",
        "            pruner=optuna.pruners.MedianPruner(\n",
        "                # OPTIMIZATION: More aggressive pruning to save resources\n",
        "                n_startup_trials = max(6, self.n_trials //12),  # More aggressive\n",
        "                n_warmup_steps=2,    # Reduced from 3\n",
        "                interval_steps=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Run optimization with early stopping check\n",
        "        cprint(f\"üéØ Target: {self.n_trials} trials\", 'cyan', attrs=['bold'])\n",
        "        for trial_num in range(self.n_trials):\n",
        "            try:\n",
        "                study.optimize(self.objective, timeout=None, n_jobs=1, n_trials=1)\n",
        "\n",
        "                # Check for early stopping after each trial\n",
        "                if self.best_accuracy >= 99.5:\n",
        "                    cprint(f\"\\nüéØ TARGET ACCURACY ACHIEVED!\", 'red', attrs=['bold'])\n",
        "                    cprint(f\"üèÜ Best Accuracy: {self.best_accuracy:.4f}% >= 99.5%\", 'green', attrs=['bold'])\n",
        "                    cprint(f\"‚ö° Stopping optimization early after {self.current_trial} trials\", 'yellow', attrs=['bold'])\n",
        "                    cprint(f\"üöÄ Moving to next model for maximum efficiency!\", 'cyan', attrs=['bold'])\n",
        "                    break\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                cprint(f\"\\n‚ö†Ô∏è Optimization interrupted by user\", 'yellow', attrs=['bold'])\n",
        "                break\n",
        "            except Exception as e:\n",
        "                cprint(f\"‚ö†Ô∏è Trial failed: {e}\", 'red')\n",
        "                continue\n",
        "\n",
        "        # Final results\n",
        "        print(\"\\n\" + \"üèÅ\" * 40)\n",
        "        if self.best_accuracy >= 99.5:\n",
        "            cprint(f\"üéØ OPTIMIZATION COMPLETED - TARGET ACHIEVED!\", 'green', attrs=['bold'])\n",
        "            cprint(f\"‚ö° Completed in {self.current_trial} trials (saved {self.n_trials - self.current_trial} trials)\", 'yellow', attrs=['bold'])\n",
        "        else:\n",
        "            cprint(f\"OPTIMIZATION COMPLETED FOR {self.model_name.upper()}\", 'green', attrs=['bold'])\n",
        "\n",
        "        cprint(f\"üèÜ OPTIMIZATION BEST ACCURACY: {self.best_accuracy:.4f}%\", 'red', attrs=['bold'])\n",
        "        print(\"üèÅ\" * 40)\n",
        "\n",
        "        return study.best_params if study.best_params else {}\n",
        "\n",
        "    def save_best_params_immediately(self) -> None:\n",
        "        \"\"\"Save best parameters immediately to Google Drive only\"\"\"\n",
        "        if self.best_trial_info['trial_number'] == 0:\n",
        "            return\n",
        "\n",
        "        # Google Drive save only\n",
        "        drive_file = f\"{self.drive_path}/{self.model_name}_best_params_trial_{self.best_trial_info['trial_number']}.json\"\n",
        "\n",
        "        results_with_meta = {\n",
        "            'model_name': self.model_name,\n",
        "            'trial_number': self.best_trial_info['trial_number'],\n",
        "            'accuracy': self.best_trial_info['accuracy'],\n",
        "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'metrics': {\n",
        "                'train_loss': self.best_trial_info['train_loss'],\n",
        "                'val_loss': self.best_trial_info['val_loss'],\n",
        "                'train_acc': self.best_trial_info['train_acc'],\n",
        "                'val_acc': self.best_trial_info['val_acc'],\n",
        "                'train_f1': self.best_trial_info['train_f1'],\n",
        "                'val_f1': self.best_trial_info['val_f1']\n",
        "            },\n",
        "            'hyperparameters': self.best_trial_info['hyperparameters']\n",
        "        }\n",
        "\n",
        "        # Save to Google Drive only\n",
        "        with open(drive_file, 'w') as f:\n",
        "            json.dump(results_with_meta, f, indent=4, sort_keys=True)\n",
        "\n",
        "        cprint(f\"  ‚òÅÔ∏è Best params saved to Drive: {drive_file}\", 'green')\n",
        "\n",
        "def optimize_single_model(model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Optimize single model with maximum GPU utilization\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"‚ö°\" * 50)\n",
        "    cprint(f\"OPTIMIZING {model_name.upper()}\", 'red', attrs=['bold'])\n",
        "    print(\"‚ö°\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Setup environment\n",
        "        optimal_threads, available_ram, gpu_memory_gb = setup_maximum_gpu_utilization()\n",
        "\n",
        "        # Get maximum batch sizes\n",
        "        train_batch_size, val_batch_size = get_maximum_batch_sizes(\n",
        "            model_name, available_ram, gpu_memory_gb\n",
        "        )\n",
        "\n",
        "        cprint(f\"üéØ Maximum Batch Sizes - Train: {train_batch_size}, Val: {val_batch_size}\", 'green', attrs=['bold'])\n",
        "\n",
        "        # Create data loaders with reduced workers to save CPU RAM\n",
        "        train_loader, val_loader, test_loader, val_data, test_data = Optuna_DataManager.create_data_loaders(\n",
        "            config['X'], config['Y'],\n",
        "            train_batch_size=train_batch_size,\n",
        "            val_batch_size=val_batch_size,\n",
        "            num_workers=optimal_threads//2,  # OPTIMIZATION: Reduced workers\n",
        "            pin_memory=True,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "        # Run optimization\n",
        "        optimizer = HyperparameterOptimizer(\n",
        "            model_name, train_loader, val_loader,\n",
        "            n_trials=Config.OPTUNA_TRIALS,\n",
        "            train_batch_size=train_batch_size,\n",
        "            val_batch_size=val_batch_size,\n",
        "            X=config['X'],\n",
        "            Y=config['Y']\n",
        "        )\n",
        "\n",
        "        best_params = optimizer.optimize()\n",
        "\n",
        "        # OPTIMIZATION: Cleanup\n",
        "        del optimizer, train_loader, val_loader, test_loader\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return best_params\n",
        "\n",
        "    except Exception as e:\n",
        "        cprint(f\"‚ùå OPTIMIZATION FAILED FOR {model_name}: {e}\", 'red', attrs=['bold'])\n",
        "        cprint(f\"üìã Error: {traceback.format_exc()}\", 'yellow')\n",
        "        return {}\n",
        "\n",
        "def parallel_hyperparameter_optimization(model_configs: Dict[str, Any], max_workers: int = 1) -> Dict[str, Any]:\n",
        "    # Run optimization with sequential processing for maximum GPU utilization\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # print(\"\\n\" + \"üé™\" * 40)\n",
        "    cprint(\"STARTING PARALLEL HYPERPARAMETER OPTIMIZATION\", 'red', attrs=['bold'])\n",
        "    # print(\"üé™\" * 50)\n",
        "\n",
        "    # Sequential processing for maximum GPU utilization per model\n",
        "    for i, (model_name, config) in enumerate(model_configs.items(), 1):\n",
        "        cprint(f\"\\nüìç MODEL {i}/{len(model_configs)}: {model_name.upper()}\", 'cyan', attrs=['bold'])\n",
        "\n",
        "        try:\n",
        "            best_params = optimize_single_model(model_name, config)\n",
        "            results[model_name] = best_params\n",
        "\n",
        "            if best_params:\n",
        "                cprint(f\"‚úÖ {model_name.upper()} OPTIMIZATION COMPLETED!\", 'green', attrs=['bold'])\n",
        "            else:\n",
        "                cprint(f\"‚ùå {model_name.upper()} OPTIMIZATION FAILED!\", 'red', attrs=['bold'])\n",
        "\n",
        "        except Exception as e:\n",
        "            cprint(f\"‚ùå {model_name.upper()} CRASHED: {e}\", 'red', attrs=['bold'])\n",
        "            results[model_name] = {}\n",
        "\n",
        "        # OPTIMIZATION: Aggressive cleanup between models\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_optimization_results(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"Save optimization results to Google Drive only\"\"\"\n",
        "\n",
        "    # Set Google Drive path\n",
        "    drive_path = '/content/drive/MyDrive/Hilsha/hyper-parameters'\n",
        "    os.makedirs(f\"{drive_path}/hyperparameters\", exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"üíæ\" * 50)\n",
        "    cprint(\"SAVING OPTIMIZATION RESULTS TO GOOGLE DRIVE\", 'cyan', attrs=['bold'])\n",
        "    print(\"üíæ\" * 50)\n",
        "\n",
        "    # Save individual model results\n",
        "    successful_models = 0\n",
        "    for model_name, best_params in results.items():\n",
        "        if best_params:\n",
        "            # Google Drive save only\n",
        "            drive_file = f\"{drive_path}/hyperparameters/{model_name}_best_params.json\"\n",
        "\n",
        "            # Enhanced metadata\n",
        "            results_with_meta = {\n",
        "                'model_name': model_name,\n",
        "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'gpu_optimized': True,\n",
        "                'hyperparameters': best_params,\n",
        "                'optimization_config': {\n",
        "                    'framework': 'optuna',\n",
        "                    'sampler': 'TPE_Multivariate',\n",
        "                    'pruner': 'Median',\n",
        "                    'trials': 40,\n",
        "                    'gpu_acceleration': torch.cuda.is_available(),\n",
        "                    'multi_gpu': torch.cuda.device_count() > 1 if torch.cuda.is_available() else False\n",
        "                }\n",
        "            }\n",
        "\n",
        "            with open(drive_file, 'w') as f:\n",
        "                json.dump(results_with_meta, f, indent=4, sort_keys=True)\n",
        "\n",
        "            cprint(f\"‚úÖ {model_name.upper()} parameters saved to Google Drive!\", 'green')\n",
        "\n",
        "            # Display best parameters\n",
        "            print(f\"  üìã {model_name.upper()} BEST PARAMETERS:\")\n",
        "            for key, value in best_params.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"    üîπ {key:<20}: {value:.6f}\")\n",
        "                else:\n",
        "                    print(f\"    üîπ {key:<20}: {value}\")\n",
        "            print()\n",
        "\n",
        "            successful_models += 1\n",
        "\n",
        "    # Save master results file to Google Drive\n",
        "    master_file = f\"{drive_path}/hyperparameters/all_best_params.json\"\n",
        "\n",
        "    # GPU information\n",
        "    gpu_info = {}\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_info = {\n",
        "            'gpu_count': torch.cuda.device_count(),\n",
        "            'gpu_names': [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())],\n",
        "            'total_gpu_memory_gb': sum(torch.cuda.get_device_properties(i).total_memory / 1e9 for i in range(torch.cuda.device_count()))\n",
        "        }\n",
        "\n",
        "    master_results = {\n",
        "        'optimization_summary': {\n",
        "            'total_models': len(results),\n",
        "            'successful_optimizations': successful_models,\n",
        "            'failed_optimizations': len(results) - successful_models,\n",
        "            'success_rate_percent': (successful_models / len(results)) * 100 if results else 0,\n",
        "            'gpu_accelerated': torch.cuda.is_available(),\n",
        "            'system_info': {\n",
        "                'cpu_cores': os.cpu_count(),\n",
        "                'ram_gb': psutil.virtual_memory().total / (1024**3),\n",
        "                **gpu_info\n",
        "            }\n",
        "        },\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    with open(master_file, 'w') as f:\n",
        "        json.dump(master_results, f, indent=4, sort_keys=True)\n",
        "\n",
        "    cprint(f\"üíæ Master results saved to Google Drive: {master_file}\", 'cyan', attrs=['bold'])\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"üìä\" * 50)\n",
        "    cprint(\"OPTIMIZATION SUMMARY\", 'yellow', attrs=['bold'])\n",
        "    print(\"üìä\" * 50)\n",
        "    print(f\"  üéØ Total Models: {len(results)}\")\n",
        "    print(f\"  ‚úÖ Successful: {successful_models}\")\n",
        "    print(f\"  ‚ùå Failed: {len(results) - successful_models}\")\n",
        "    print(f\"  üìà Success Rate: {(successful_models / len(results)) * 100:.1f}%\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  üéÆ GPU Acceleration: Enabled ({torch.cuda.device_count()} GPUs)\")\n",
        "    else:\n",
        "        print(f\"  üíª GPU Acceleration: Disabled (CPU only)\")\n",
        "\n",
        "def display_startup_banner():\n",
        "    #Display an impressive startup banner\n",
        "    banner = \"\"\"\n",
        "‚îå‚îÄ THE FISH OPTIMIZER ‚îÄ‚îê\n",
        "‚îÇ   üêü Optimizing üêü   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"\\n\" + \"=\"*120)\n",
        "    cprint(banner, 'red', attrs=['bold'])\n",
        "    # print(\"=\"*120)\n",
        "    # cprint(\"üöÄ MAXIMUM GPU-ACCELERATED HYPERPARAMETER OPTIMIZATION üöÄ\", 'yellow', attrs=['bold'])\n",
        "    # cprint(\"üî• DESIGNED FOR MAXIMUM PERFORMANCE AND USER EXPERIENCE üî•\", 'cyan', attrs=['bold'])\n",
        "    # print(\"=\"*120)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with spectacular UI and maximum GPU utilization\"\"\"\n",
        "\n",
        "    # Display startup banner\n",
        "    display_startup_banner()\n",
        "\n",
        "    # Environment setup with detailed reporting\n",
        "    print(\"\\nüîß SYSTEM INITIALIZATION\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "\n",
        "    optimal_threads, available_ram, gpu_memory_gb = setup_maximum_gpu_utilization()\n",
        "\n",
        "    # Data loading with progress\n",
        "    print(\"\\nüìä DATA LOADING AND PREPROCESSING\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "\n",
        "    try:\n",
        "        # You need to implement or import these classes\n",
        "        # from your_data_module import DataManager  # Replace with actual import\n",
        "\n",
        "        cprint(\"üîÑ Loading and balancing dataset...\", 'cyan', attrs=['bold'])\n",
        "        X, Y = DataManager.load_and_balance_data()\n",
        "\n",
        "        cprint(f\"‚úÖ Dataset loaded successfully!\", 'green', attrs=['bold'])\n",
        "        print(f\"   üìà Total samples: {len(X):,}\")\n",
        "        print(f\"   üè∑Ô∏è  Total labels: {len(Y):,}\")\n",
        "        print(f\"   üìä Classes: {len(np.unique(Y))}\")\n",
        "\n",
        "        if len(X) != len(Y) or len(X) == 0:\n",
        "            raise ValueError(\"Invalid dataset: inconsistent or empty data\")\n",
        "\n",
        "    except ImportError:\n",
        "        cprint(\"‚ùå DataManager not found. Please ensure it is imported.\", 'red', attrs=['bold'])\n",
        "        return\n",
        "    except Exception as e:\n",
        "        cprint(f\"‚ùå Data loading failed: {e}\", 'red', attrs=['bold'])\n",
        "        return\n",
        "\n",
        "    # Prepare model configurations\n",
        "    print(\"\\nü§ñ MODEL CONFIGURATION\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "\n",
        "    # Default models if Config.MODELS is not available\n",
        "    try:\n",
        "        # from your_config_module import Config  # Replace with actual import\n",
        "        models = Config.MODELS\n",
        "    except ImportError:\n",
        "        cprint(\"‚ö†Ô∏è  Config not found. Using default models.\", 'yellow', attrs=['bold'])\n",
        "        models = ['resnet50', 'efficientnet_b0', 'mobilenet_v3_large']\n",
        "\n",
        "    model_configs = {}\n",
        "    for i, model_name in enumerate(models, 1):\n",
        "        model_configs[model_name] = {'X': X, 'Y': Y}\n",
        "        print(f\"  {i:2d}. {model_name}\")\n",
        "\n",
        "    cprint(f\"üéØ Configured {len(models)} models for optimization\", 'green', attrs=['bold'])\n",
        "\n",
        "    # Run optimization\n",
        "    print(\"\\nüöÄ STARTING HYPERPARAMETER OPTIMIZATION\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    all_best_params = parallel_hyperparameter_optimization(\n",
        "        model_configs,\n",
        "        max_workers=1\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Save results to Google Drive only\n",
        "    save_optimization_results(all_best_params)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"üéâ\" * 45)\n",
        "    cprint(\"üèÜ HYPERPARAMETER OPTIMIZATION COMPLETED! üèÜ\", 'red', attrs=['bold'])\n",
        "    print(\"üéâ\" * 45)\n",
        "\n",
        "    successful = sum(1 for params in all_best_params.values() if params)\n",
        "    total = len(all_best_params)\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Total Time: {total_time//3600:.0f}h {(total_time%3600)//60:.0f}m {total_time%60:.0f}s\")\n",
        "    print(f\"üìä Models Processed: {total}\")\n",
        "    print(f\"‚úÖ Successful Optimizations: {successful}\")\n",
        "    print(f\"‚ùå Failed Optimizations: {total - successful}\")\n",
        "    print(f\"üìà Success Rate: {100*successful/total:.1f}%\")\n",
        "    print(f\"üíæ Results Location: /content/drive/MyDrive/Hilsha/hyperparameters/\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üéÆ GPU Utilization: Maximum\")\n",
        "        print(f\"üî• Multi-GPU: {'Yes' if torch.cuda.device_count() > 1 else 'No'}\")\n",
        "\n",
        "    print(\"\\n\" + \"üéâ\" * 45)\n",
        "    cprint(\"üöÄ READY FOR TRAINING WITH OPTIMIZED HYPERPARAMETERS! üöÄ\", 'green', attrs=['bold'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CmSTaip_n5bV"
      },
      "id": "CmSTaip_n5bV",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 9: Training - Pipeline"
      ],
      "metadata": {
        "id": "UaFBF8Jucyz3"
      },
      "id": "UaFBF8Jucyz3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Resource Management\n",
        "# ============================================================================\n",
        "class ResourceManager:\n",
        "    \"\"\"Smart resource management for optimal GPU/CPU utilization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gpu_memory_gb = 20\n",
        "        self.cpu_memory_gb = 50\n",
        "        self.max_gpu_usage = 0.85\n",
        "        self.max_cpu_usage = 0.90\n",
        "\n",
        "    def get_memory_stats(self):\n",
        "        \"\"\"Get current memory usage statistics\"\"\"\n",
        "        stats = {'cpu_percent': psutil.virtual_memory().percent}\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            stats['gpu_allocated_gb'] = torch.cuda.memory_allocated() / (1024**3)\n",
        "            stats['gpu_reserved_gb'] = torch.cuda.memory_reserved() / (1024**3)\n",
        "            stats['gpu_percent'] = (stats['gpu_reserved_gb'] / self.gpu_memory_gb) * 100\n",
        "        else:\n",
        "            stats.update({'gpu_allocated_gb': 0, 'gpu_reserved_gb': 0, 'gpu_percent': 0})\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def should_cleanup_aggressive(self):\n",
        "        \"\"\"Check if aggressive cleanup is needed\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "        return (stats['gpu_percent'] > 90 or stats['cpu_percent'] > 90)\n",
        "\n",
        "    def aggressive_cleanup(self):\n",
        "        \"\"\"Perform comprehensive memory cleanup\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "        gc.collect()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    def optimize_batch_size(self, base_size, model_complexity=1.0):\n",
        "        \"\"\"Calculate optimal batch size based on current memory state\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "        memory_factor = max(0.4, 1.0 - (stats['gpu_percent'] / 100))\n",
        "        optimal_size = int(base_size * memory_factor / model_complexity)\n",
        "        return max(32, min(256, optimal_size))\n",
        "\n",
        "# ============================================================================\n",
        "# Training Progress Tracker (Console Only)\n",
        "# ============================================================================\n",
        "class TrainingProgressTracker:\n",
        "    \"\"\"Track training progress without plotting dependencies\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, total_epochs, batches_per_epoch):\n",
        "        self.model_name = model_name\n",
        "        self.total_epochs = total_epochs\n",
        "        self.batches_per_epoch = batches_per_epoch\n",
        "        self.current_epoch = 0\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def start_epoch(self, epoch):\n",
        "        \"\"\"Start tracking an epoch\"\"\"\n",
        "        self.current_epoch = epoch\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def update_batch(self, batch_idx, loss, acc, is_training=True, total_batches=None):\n",
        "        \"\"\"Update batch progress - simplified for console only\"\"\"\n",
        "        if batch_idx % 50 == 0 and batch_idx > 0:\n",
        "            phase = \"Train\" if is_training else \"Val\"\n",
        "            elapsed = time.time() - self.epoch_start_time\n",
        "            tqdm.write(f\"  [{phase}] Batch {batch_idx:4d} - Loss: {loss:.4f}, Acc: {acc:.4f}, Time: {elapsed:.1f}s\")\n",
        "\n",
        "    def finish_epoch(self, train_loss, train_acc, val_loss, val_acc, val_f1, is_best=False, lr=None):\n",
        "        \"\"\"Finish epoch tracking\"\"\"\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        total_time = time.time() - self.start_time\n",
        "\n",
        "        status = \"üåü NEW BEST!\" if is_best else \"\"\n",
        "\n",
        "        tqdm.write(f\"\\nEpoch {self.current_epoch + 1}/{self.total_epochs} Complete {status}\")\n",
        "        tqdm.write(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "        tqdm.write(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "        if lr:\n",
        "            tqdm.write(f\"  LR: {lr:.6f}\")\n",
        "        tqdm.write(f\"  Epoch Time: {epoch_time:.1f}s, Total: {total_time:.1f}s\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Model Evaluator (Training-focused)\n",
        "# ============================================================================\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Model evaluation for training purposes (no plotting)\"\"\"\n",
        "\n",
        "    def evaluate_model(self, model, data_loader, model_name):\n",
        "        \"\"\"Evaluate model and return metrics for saving\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "        misclassified_samples = []\n",
        "        total_samples = 0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels) in enumerate(data_loader):\n",
        "                images = images.to(Config.DEVICE, memory_format=torch.channels_last)\n",
        "                labels = labels.to(Config.DEVICE)\n",
        "\n",
        "                outputs = model(images)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Store results\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "                # Track misclassified samples\n",
        "                mask = predicted != labels\n",
        "                if mask.any():\n",
        "                    misclassified_indices = torch.where(mask)[0]\n",
        "                    for idx in misclassified_indices:\n",
        "                        misclassified_samples.append({\n",
        "                            'batch_idx': batch_idx,\n",
        "                            'sample_idx': idx.item(),\n",
        "                            'true_label': labels[idx].item(),\n",
        "                            'predicted_label': predicted[idx].item(),\n",
        "                            'confidence': probabilities[idx].max().item(),\n",
        "                            'image_tensor': images[idx].cpu()  # Store for later visualization\n",
        "                        })\n",
        "\n",
        "                # Update counters\n",
        "                batch_size = labels.size(0)\n",
        "                total_samples += batch_size\n",
        "                correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        # Classification report\n",
        "        class_report = classification_report(\n",
        "            all_labels, all_predictions,\n",
        "            target_names=Config.CLASS_NAMES,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f\"Evaluation Results for {model_name}:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  F1-Macro: {f1_macro:.4f}\")\n",
        "        print(f\"  F1-Weighted: {f1_weighted:.4f}\")\n",
        "        print(f\"  Misclassified: {len(misclassified_samples)}/{total_samples}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1_macro,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'confusion_matrix': conf_matrix.tolist(),  # Convert to list for JSON serialization\n",
        "            'classification_report': class_report,\n",
        "            'predictions': all_predictions,\n",
        "            'true_labels': all_labels,\n",
        "            'probabilities': np.array(all_probabilities).tolist(),  # Convert for JSON\n",
        "            'misclassified_count': len(misclassified_samples),\n",
        "            'total_samples': total_samples,\n",
        "            'misclassified_details': misclassified_samples[:50]  # Limit to first 50 for storage\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# Enhanced Model Trainer\n",
        "# ============================================================================\n",
        "class EnhancedModelTrainer:\n",
        "    def __init__(self, model, model_name, hyperparameters):\n",
        "        self.model = model.to(Config.DEVICE)\n",
        "        self.model_name = model_name\n",
        "        self.hyperparameters = hyperparameters\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_val_f1 = 0.0\n",
        "        self.patience_counter = 0\n",
        "\n",
        "        # Resource management\n",
        "        self.resource_manager = ResourceManager()\n",
        "        self.memory_check_interval = 15\n",
        "\n",
        "        # Setup training components\n",
        "        self._setup_training_components()\n",
        "\n",
        "        # Initialize history for saving\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'val_f1': [],\n",
        "            'learning_rates': [],\n",
        "            'epoch_times': [],\n",
        "            'memory_usage': []\n",
        "        }\n",
        "\n",
        "    def _setup_training_components(self):\n",
        "        \"\"\"Setup optimizer, criterion, and scheduler\"\"\"\n",
        "        # Filter hyperparameters\n",
        "        allowed_keys = ['lr', 'weight_decay', 'dropout', 'hidden_dim_multiplier',\n",
        "                       'augmentation_strength', 'batch_size', 'optimizer_type',\n",
        "                       'scheduler_type', 'label_smoothing']\n",
        "        self.hyperparameters = {k: v for k, v in self.hyperparameters.items() if k in allowed_keys}\n",
        "\n",
        "        # Optimizer setup\n",
        "        lr = self.hyperparameters.get('lr', Config.LEARNING_RATE)\n",
        "        weight_decay = self.hyperparameters.get('weight_decay', Config.WEIGHT_DECAY)\n",
        "        optimizer_type = self.hyperparameters.get('optimizer_type', 'adamw')\n",
        "\n",
        "        if optimizer_type == 'adamw':\n",
        "            self.optimizer = optim.AdamW(\n",
        "                self.model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                fused=torch.cuda.is_available()\n",
        "            )\n",
        "        elif optimizer_type == 'adam':\n",
        "            self.optimizer = optim.Adam(\n",
        "                self.model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                fused=torch.cuda.is_available()\n",
        "            )\n",
        "        else:\n",
        "            self.optimizer = optim.SGD(\n",
        "                self.model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                momentum=0.9, nesterov=True\n",
        "            )\n",
        "\n",
        "        # Criterion\n",
        "        label_smoothing = self.hyperparameters.get('label_smoothing', 0.1)\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler_type = self.hyperparameters.get('scheduler_type', 'cosine')\n",
        "        if scheduler_type == 'cosine':\n",
        "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "                self.optimizer, T_max=Config.EPOCHS, eta_min=1e-6\n",
        "            )\n",
        "        elif scheduler_type == 'plateau':\n",
        "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                self.optimizer, mode='min', factor=0.5, patience=5\n",
        "            )\n",
        "        else:\n",
        "            self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.95)\n",
        "\n",
        "        # Mixed precision scaler\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=Config.USE_MIXED_PRECISION)\n",
        "\n",
        "    def train_epoch(self, train_loader, progress_tracker):\n",
        "        \"\"\"Enhanced training epoch with smart memory management\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        batch_count = len(train_loader)\n",
        "\n",
        "        tqdm.write(f\"Training: {len(train_loader.dataset):,} samples, \"\n",
        "                   f\"{batch_count:,} batches, batch_size: {train_loader.batch_size}\")\n",
        "\n",
        "        try:\n",
        "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "                try:\n",
        "                    # Smart memory management\n",
        "                    if batch_idx % self.memory_check_interval == 0:\n",
        "                        if self.resource_manager.should_cleanup_aggressive():\n",
        "                            self.resource_manager.aggressive_cleanup()\n",
        "\n",
        "                    # Move data to device\n",
        "                    images = images.to(Config.DEVICE, non_blocking=True, memory_format=torch.channels_last)\n",
        "                    labels = labels.to(Config.DEVICE, non_blocking=True)\n",
        "\n",
        "                    # Forward pass\n",
        "                    self.optimizer.zero_grad(set_to_none=True)\n",
        "                    with torch.cuda.amp.autocast(enabled=Config.USE_MIXED_PRECISION):\n",
        "                        outputs = self.model(images)\n",
        "                        loss = self.criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.scaler.scale(loss).backward()\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    batch_acc = (predicted == labels).float().mean().item()\n",
        "                    batch_loss = loss.item()\n",
        "\n",
        "                    # Update totals\n",
        "                    total_loss += batch_loss * images.size(0)\n",
        "                    total += images.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Update progress\n",
        "                    progress_tracker.update_batch(batch_idx, batch_loss, batch_acc, is_training=True, total_batches=batch_count)\n",
        "\n",
        "                    # Memory cleanup\n",
        "                    del outputs, loss, predicted, images, labels\n",
        "\n",
        "                except Exception as e:\n",
        "                    tqdm.write(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                    self.resource_manager.aggressive_cleanup()\n",
        "                    continue\n",
        "\n",
        "            # Final cleanup\n",
        "            self.resource_manager.aggressive_cleanup()\n",
        "\n",
        "            return total_loss / max(1, total), correct / max(1, total)\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Training epoch failed: {str(e)}\")\n",
        "            self.resource_manager.aggressive_cleanup()\n",
        "            return float('inf'), 0.0\n",
        "\n",
        "    def validate_epoch(self, val_loader, progress_tracker):\n",
        "        \"\"\"Enhanced validation epoch with memory optimization\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        batch_count = len(val_loader)\n",
        "\n",
        "        tqdm.write(f\"Validation: {len(val_loader.dataset):,} samples, \"\n",
        "                   f\"{batch_count:,} batches, batch_size: {val_loader.batch_size}\")\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (images, labels) in enumerate(val_loader):\n",
        "                    try:\n",
        "                        images = images.to(Config.DEVICE, non_blocking=True, memory_format=torch.channels_last)\n",
        "                        labels = labels.to(Config.DEVICE, non_blocking=True)\n",
        "\n",
        "                        with torch.cuda.amp.autocast(enabled=Config.USE_MIXED_PRECISION):\n",
        "                            outputs = self.model(images)\n",
        "                            loss = self.criterion(outputs, labels)\n",
        "\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        batch_acc = (predicted == labels).float().mean().item()\n",
        "                        batch_loss = loss.item()\n",
        "\n",
        "                        # Store results\n",
        "                        total_loss += batch_loss * images.size(0)\n",
        "                        total_samples += images.size(0)\n",
        "                        all_predictions.extend(predicted.cpu().numpy())\n",
        "                        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                        # Update progress\n",
        "                        progress_tracker.update_batch(batch_idx, batch_loss, batch_acc,\n",
        "                                                    is_training=False, total_batches=batch_count)\n",
        "\n",
        "                        # Memory cleanup\n",
        "                        del outputs, loss, predicted, images, labels\n",
        "\n",
        "                    except Exception as e:\n",
        "                        tqdm.write(f\"Error in validation batch {batch_idx}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate final metrics\n",
        "            val_acc = accuracy_score(all_labels, all_predictions)\n",
        "            val_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "\n",
        "            return total_loss / max(1, total_samples), val_acc, val_f1\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Validation epoch failed: {str(e)}\")\n",
        "            self.resource_manager.aggressive_cleanup()\n",
        "            return float('inf'), 0.0, 0.0\n",
        "\n",
        "    def train_main_model(self, train_loader, val_loader, test_loader=None):\n",
        "        \"\"\"Main model training with comprehensive data saving\"\"\"\n",
        "        if not train_loader or len(train_loader.dataset) == 0:\n",
        "            tqdm.write(f\"Skipping {self.model_name}: No training data\")\n",
        "            return False\n",
        "\n",
        "        if not val_loader or len(val_loader.dataset) == 0:\n",
        "            tqdm.write(f\"Skipping {self.model_name}: No validation data\")\n",
        "            return False\n",
        "\n",
        "        tqdm.write(f\"\\nTraining {self.model_name}\")\n",
        "        tqdm.write(f\"Training samples: {len(train_loader.dataset):,}\")\n",
        "        tqdm.write(f\"Validation samples: {len(val_loader.dataset):,}\")\n",
        "        tqdm.write(f\"Total epochs: {Config.EPOCHS}\")\n",
        "        tqdm.write(f\"Batch size: {train_loader.batch_size}\")\n",
        "\n",
        "        # Setup model for training\n",
        "        self.model = self.model.to(Config.DEVICE, memory_format=torch.channels_last)\n",
        "\n",
        "        # Progress tracker\n",
        "        progress_tracker = TrainingProgressTracker(self.model_name, Config.EPOCHS, len(train_loader))\n",
        "\n",
        "        # Training loop\n",
        "        training_start_time = time.time()\n",
        "\n",
        "        for epoch in range(Config.EPOCHS):\n",
        "            epoch_start_time = time.time()\n",
        "            tqdm.write(f\"\\nEpoch {epoch + 1}/{Config.EPOCHS}\")\n",
        "\n",
        "            progress_tracker.start_epoch(epoch)\n",
        "\n",
        "            # Training phase\n",
        "            train_loss, train_acc = self.train_epoch(train_loader, progress_tracker)\n",
        "\n",
        "            # Validation phase\n",
        "            val_loss, val_acc, val_f1 = self.validate_epoch(val_loader, progress_tracker)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                self.scheduler.step(val_loss)\n",
        "            else:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            # Track metrics\n",
        "            is_best = val_f1 > self.best_val_f1 * 1.001\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            memory_stats = self.resource_manager.get_memory_stats()\n",
        "\n",
        "            # Update progress tracker\n",
        "            progress_tracker.finish_epoch(train_loss, train_acc, val_loss, val_acc, val_f1, is_best=is_best, lr=current_lr)\n",
        "\n",
        "            # Store history\n",
        "            self.history['train_loss'].append(float(train_loss))\n",
        "            self.history['train_acc'].append(float(train_acc))\n",
        "            self.history['val_loss'].append(float(val_loss))\n",
        "            self.history['val_acc'].append(float(val_acc))\n",
        "            self.history['val_f1'].append(float(val_f1))\n",
        "            self.history['learning_rates'].append(float(current_lr))\n",
        "            self.history['epoch_times'].append(float(epoch_time))\n",
        "            self.history['memory_usage'].append(memory_stats)\n",
        "\n",
        "            # Save best model\n",
        "            if is_best:\n",
        "                self.best_val_f1 = val_f1\n",
        "                self.best_val_acc = val_acc\n",
        "                self.patience_counter = 0\n",
        "                self._save_best_model(epoch + 1, val_f1, val_acc)\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Early stopping check\n",
        "            if self.patience_counter >= Config.PATIENCE:\n",
        "                total_time = time.time() - training_start_time\n",
        "                tqdm.write(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                tqdm.write(f\"Total training time: {total_time:.1f}s\")\n",
        "                break\n",
        "\n",
        "        # Final evaluation and save results\n",
        "        eval_loader = test_loader if test_loader else val_loader\n",
        "        self._final_evaluation_and_save(eval_loader, training_start_time)\n",
        "\n",
        "        # Cleanup\n",
        "        self.resource_manager.aggressive_cleanup()\n",
        "        return True\n",
        "\n",
        "    def _save_best_model(self, epoch, val_f1, val_acc):\n",
        "        \"\"\"Save best model checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_name': self.model_name,\n",
        "            'hyperparameters': self.hyperparameters,\n",
        "            'epoch': epoch,\n",
        "            'best_val_f1': val_f1,\n",
        "            'best_val_acc': val_acc,\n",
        "            'optimizer_state': self.optimizer.state_dict(),\n",
        "            'scheduler_state': self.scheduler.state_dict() if self.scheduler else None,\n",
        "            'num_classes': Config.NUM_CLASSES,\n",
        "            'class_names': Config.CLASS_NAMES,\n",
        "            'save_format_version': '1.0'\n",
        "        }\n",
        "\n",
        "        # Save paths\n",
        "        best_model_dir = f\"{Config.OUTPUT_DIR}/best_model\"\n",
        "        os.makedirs(best_model_dir, exist_ok=True)\n",
        "\n",
        "        save_path = f\"{best_model_dir}/{self.model_name}_best.pt\"\n",
        "\n",
        "        try:\n",
        "            torch.save(checkpoint, save_path)\n",
        "            tqdm.write(f\"‚úÖ Best model saved: F1={val_f1:.4f}, Acc={val_acc:.4f}\")\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error saving model {self.model_name}: {e}\")\n",
        "\n",
        "    def _final_evaluation_and_save(self, eval_loader, training_start_time):\n",
        "        \"\"\"Final evaluation and comprehensive data saving\"\"\"\n",
        "        # Load best model for evaluation\n",
        "        best_model_path = f\"{Config.OUTPUT_DIR}/best_model/{self.model_name}_best.pt\"\n",
        "        if os.path.exists(best_model_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(best_model_path, map_location=Config.DEVICE, weights_only=False)\n",
        "                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                    tqdm.write(f\"‚úÖ Loaded best model for evaluation\")\n",
        "                else:\n",
        "                    self.model.load_state_dict(checkpoint)\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"‚ö†Ô∏è Could not load best model: {e}\")\n",
        "\n",
        "        # Evaluate model\n",
        "        evaluator = ModelEvaluator()\n",
        "        evaluation_results = evaluator.evaluate_model(self.model, eval_loader, self.model_name)\n",
        "\n",
        "        # Create comprehensive training data package\n",
        "        training_data = {\n",
        "            'model_info': {\n",
        "                'model_name': self.model_name,\n",
        "                'num_classes': Config.NUM_CLASSES,\n",
        "                'class_names': Config.CLASS_NAMES,\n",
        "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
        "                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            },\n",
        "            'hyperparameters': self.hyperparameters,\n",
        "            'training_history': self.history,\n",
        "            'final_results': evaluation_results,\n",
        "            'training_metadata': {\n",
        "                'total_training_time': time.time() - training_start_time,\n",
        "                'best_epoch': len(self.history['val_f1']) - self.patience_counter if self.patience_counter < Config.PATIENCE else len(self.history['val_f1']),\n",
        "                'best_val_f1': self.best_val_f1,\n",
        "                'best_val_acc': self.best_val_acc,\n",
        "                'early_stopped': self.patience_counter >= Config.PATIENCE,\n",
        "                'final_epoch': len(self.history['train_loss']),\n",
        "                'device_used': str(Config.DEVICE),\n",
        "                'mixed_precision': Config.USE_MIXED_PRECISION\n",
        "            },\n",
        "            'save_timestamp': time.time(),\n",
        "            'config': {\n",
        "                'batch_size': Config.BATCH_SIZE,\n",
        "                'epochs': Config.EPOCHS,\n",
        "                'patience': Config.PATIENCE,\n",
        "                'learning_rate': Config.LEARNING_RATE,\n",
        "                'weight_decay': Config.WEIGHT_DECAY\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save training data\n",
        "        results_dir = f\"{Config.OUTPUT_DIR}/training_results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        # Save as JSON (for easy reading by visualization script)\n",
        "        json_path = f\"{results_dir}/{self.model_name}_training_data.json\"\n",
        "        try:\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(training_data, f, indent=2, default=str)\n",
        "            tqdm.write(f\"‚úÖ Training data saved to {json_path}\")\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error saving training data: {e}\")\n",
        "\n",
        "        # Also save as pickle for complex objects\n",
        "        import pickle\n",
        "        pickle_path = f\"{results_dir}/{self.model_name}_training_data.pkl\"\n",
        "        try:\n",
        "            with open(pickle_path, 'wb') as f:\n",
        "                pickle.dump(training_data, f)\n",
        "            tqdm.write(f\"‚úÖ Training data saved to {pickle_path}\")\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error saving pickle data: {e}\")\n",
        "\n",
        "        # Training summary\n",
        "        total_training_time = time.time() - training_start_time\n",
        "        tqdm.write(f\"\\nTraining Summary for {self.model_name}:\")\n",
        "        tqdm.write(f\"  Final Accuracy: {evaluation_results.get('accuracy', 0.0):.4f}\")\n",
        "        tqdm.write(f\"  Final F1 Score: {evaluation_results.get('f1_macro', 0.0):.4f}\")\n",
        "        tqdm.write(f\"  Best Validation F1: {self.best_val_f1:.4f}\")\n",
        "        tqdm.write(f\"  Total Training Time: {total_training_time:.1f}s\")\n",
        "        tqdm.write(f\"  Final Epoch: {len(self.history['train_loss'])}/{Config.EPOCHS}\")\n",
        "\n",
        "    def train_kfold(self, train_loader, val_loader, test_loader, n_folds=3):\n",
        "        \"\"\"K-fold cross-validation with data saving\"\"\"\n",
        "        if n_folds <= 0:\n",
        "            tqdm.write(f\"Skipping k-fold for {self.model_name}: n_folds <= 0\")\n",
        "            return False\n",
        "\n",
        "        from torch.utils.data import ConcatDataset\n",
        "        combined_dataset = ConcatDataset([train_loader.dataset, val_loader.dataset])\n",
        "        total_samples = len(combined_dataset)\n",
        "        min_samples_per_fold = 500\n",
        "\n",
        "        # Adjust folds based on data availability\n",
        "        if total_samples < n_folds * min_samples_per_fold:\n",
        "            n_folds = max(1, total_samples // min_samples_per_fold)\n",
        "\n",
        "        if n_folds < 2:\n",
        "            tqdm.write(f\"Skipping k-fold: need at least {min_samples_per_fold*2} samples\")\n",
        "            return False\n",
        "\n",
        "        tqdm.write(f\"\\nK-fold Cross-Validation for {self.model_name} ({n_folds} folds)\")\n",
        "\n",
        "        # Calculate fold indices\n",
        "        samples_per_fold = total_samples // n_folds\n",
        "        fold_results = []\n",
        "\n",
        "        # Create model complexity map for batch size optimization\n",
        "        model_complexity_map = {\n",
        "            'efficientnet': 1.5, 'resnet': 1.0, 'vgg': 0.8,\n",
        "            'mobilenet': 0.6, 'densenet': 1.3, 'convnext': 1.4\n",
        "        }\n",
        "        model_complexity = model_complexity_map.get(self.model_name.split('_')[0].lower(), 1.0)\n",
        "        base_batch_size = self.hyperparameters.get('batch_size', Config.BATCH_SIZE)\n",
        "        fold_batch_size = self.resource_manager.optimize_batch_size(base_batch_size, model_complexity)\n",
        "\n",
        "        total_kfold_start = time.time()\n",
        "\n",
        "        for fold in range(n_folds):\n",
        "            fold_start_time = time.time()\n",
        "            tqdm.write(f\"\\nTraining Fold {fold + 1}/{n_folds}\")\n",
        "\n",
        "            try:\n",
        "                # Create fold indices\n",
        "                val_start = fold * samples_per_fold\n",
        "                val_end = min(val_start + samples_per_fold, total_samples)\n",
        "                val_idx = list(range(val_start, val_end))\n",
        "                train_idx = list(range(0, val_start)) + list(range(val_end, total_samples))\n",
        "\n",
        "                # Create fold data loaders\n",
        "                train_subsampler = SubsetRandomSampler(train_idx)\n",
        "                val_subsampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "                train_loader_fold = DataLoader(\n",
        "                    combined_dataset, batch_size=fold_batch_size,\n",
        "                    sampler=train_subsampler, num_workers=min(8, mp.cpu_count() // 2),\n",
        "                    pin_memory=torch.cuda.is_available(), prefetch_factor=2\n",
        "                )\n",
        "\n",
        "                val_loader_fold = DataLoader(\n",
        "                    combined_dataset, batch_size=fold_batch_size,\n",
        "                    sampler=val_subsampler, num_workers=min(8, mp.cpu_count() // 2),\n",
        "                    pin_memory=torch.cuda.is_available(), prefetch_factor=2\n",
        "                )\n",
        "\n",
        "                # Create fold model\n",
        "                fold_model = ModelFactory.create_model(\n",
        "                    self.model_name, num_classes=Config.NUM_CLASSES,\n",
        "                    dropout_rate=self.hyperparameters.get('dropout', 0.5),\n",
        "                    hidden_dim_multiplier=self.hyperparameters.get('hidden_dim_multiplier', 0.5)\n",
        "                ).to(Config.DEVICE, memory_format=torch.channels_last)\n",
        "\n",
        "                # Create fold trainer\n",
        "                fold_trainer = EnhancedModelTrainer(\n",
        "                    fold_model, f\"{self.model_name}_fold_{fold + 1}\", self.hyperparameters\n",
        "                )\n",
        "\n",
        "                # Train fold\n",
        "                fold_success = fold_trainer.train_main_model(train_loader_fold, val_loader_fold)\n",
        "                fold_time = time.time() - fold_start_time\n",
        "\n",
        "                if fold_success:\n",
        "                    # Evaluate fold on test data\n",
        "                    evaluator = ModelEvaluator()\n",
        "                    eval_loader = test_loader if test_loader else val_loader_fold\n",
        "                    fold_results_data = evaluator.evaluate_model(fold_model, eval_loader, f\"{self.model_name}_fold_{fold + 1}\")\n",
        "\n",
        "                    # Store fold results\n",
        "                    fold_result = {\n",
        "                        'fold_number': fold + 1,\n",
        "                        'training_time': fold_time,\n",
        "                        'training_history': fold_trainer.history,\n",
        "                        'evaluation_results': fold_results_data,\n",
        "                        'hyperparameters': self.hyperparameters,\n",
        "                        'fold_indices': {'train': train_idx, 'val': val_idx},\n",
        "                        'model_info': {\n",
        "                            'model_name': f\"{self.model_name}_fold_{fold + 1}\",\n",
        "                            'total_parameters': sum(p.numel() for p in fold_model.parameters()),\n",
        "                            'trainable_parameters': sum(p.numel() for p in fold_model.parameters() if p.requires_grad)\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    fold_results.append(fold_result)\n",
        "\n",
        "                    # Save fold model\n",
        "                    fold_model_dir = f\"{Config.OUTPUT_DIR}/kfold_models\"\n",
        "                    os.makedirs(fold_model_dir, exist_ok=True)\n",
        "                    torch.save(fold_model.state_dict(), f\"{fold_model_dir}/{self.model_name}_fold_{fold + 1}.pt\")\n",
        "\n",
        "                    tqdm.write(f\"Fold {fold + 1} completed - Acc: {fold_results_data['accuracy']:.4f}, \"\n",
        "                              f\"F1: {fold_results_data['f1_macro']:.4f}, Time: {fold_time:.1f}s\")\n",
        "                else:\n",
        "                    tqdm.write(f\"Fold {fold + 1} training failed\")\n",
        "\n",
        "                # Cleanup\n",
        "                del fold_trainer, fold_model, train_loader_fold, val_loader_fold\n",
        "                self.resource_manager.aggressive_cleanup()\n",
        "\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"Error in fold {fold + 1}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Save comprehensive k-fold results\n",
        "        if fold_results:\n",
        "            total_kfold_time = time.time() - total_kfold_start\n",
        "\n",
        "            # Calculate k-fold statistics\n",
        "            fold_accuracies = [fr['evaluation_results']['accuracy'] for fr in fold_results]\n",
        "            fold_f1_scores = [fr['evaluation_results']['f1_macro'] for fr in fold_results]\n",
        "\n",
        "            kfold_summary = {\n",
        "                'model_name': self.model_name,\n",
        "                'n_folds': n_folds,\n",
        "                'successful_folds': len(fold_results),\n",
        "                'total_kfold_time': total_kfold_time,\n",
        "                'fold_results': fold_results,\n",
        "                'summary_statistics': {\n",
        "                    'mean_accuracy': np.mean(fold_accuracies),\n",
        "                    'std_accuracy': np.std(fold_accuracies),\n",
        "                    'mean_f1_macro': np.mean(fold_f1_scores),\n",
        "                    'std_f1_macro': np.std(fold_f1_scores),\n",
        "                    'best_fold': {\n",
        "                        'fold_number': fold_results[np.argmax(fold_f1_scores)]['fold_number'],\n",
        "                        'accuracy': max(fold_accuracies),\n",
        "                        'f1_macro': max(fold_f1_scores)\n",
        "                    },\n",
        "                    'worst_fold': {\n",
        "                        'fold_number': fold_results[np.argmin(fold_f1_scores)]['fold_number'],\n",
        "                        'accuracy': min(fold_accuracies),\n",
        "                        'f1_macro': min(fold_f1_scores)\n",
        "                    }\n",
        "                },\n",
        "                'hyperparameters': self.hyperparameters,\n",
        "                'save_timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            # Save k-fold results\n",
        "            kfold_dir = f\"{Config.OUTPUT_DIR}/kfold_results\"\n",
        "            os.makedirs(kfold_dir, exist_ok=True)\n",
        "\n",
        "            # Save as JSON\n",
        "            json_path = f\"{kfold_dir}/{self.model_name}_kfold_results.json\"\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(kfold_summary, f, indent=2, default=str)\n",
        "\n",
        "            # Save as pickle\n",
        "            import pickle\n",
        "            pickle_path = f\"{kfold_dir}/{self.model_name}_kfold_results.pkl\"\n",
        "            with open(pickle_path, 'wb') as f:\n",
        "                pickle.dump(kfold_summary, f)\n",
        "\n",
        "            tqdm.write(f\"\\nK-fold Summary for {self.model_name}:\")\n",
        "            tqdm.write(f\"  Successful folds: {len(fold_results)}/{n_folds}\")\n",
        "            tqdm.write(f\"  Mean Accuracy: {np.mean(fold_accuracies):.4f} ¬± {np.std(fold_accuracies):.4f}\")\n",
        "            tqdm.write(f\"  Mean F1-Score: {np.mean(fold_f1_scores):.4f} ¬± {np.std(fold_f1_scores):.4f}\")\n",
        "            tqdm.write(f\"  Total time: {total_kfold_time:.1f}s\")\n",
        "            tqdm.write(f\"  Results saved to {json_path}\")\n",
        "\n",
        "        return len(fold_results) > 0\n",
        "\n",
        "    def cleanup_trainer(self):\n",
        "        \"\"\"Complete cleanup of trainer resources\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'model'):\n",
        "                del self.model\n",
        "            if hasattr(self, 'optimizer'):\n",
        "                del self.optimizer\n",
        "            if hasattr(self, 'scheduler'):\n",
        "                del self.scheduler\n",
        "            if hasattr(self, 'criterion'):\n",
        "                del self.criterion\n",
        "            if hasattr(self, 'scaler'):\n",
        "                del self.scaler\n",
        "\n",
        "            self.history.clear()\n",
        "            self.resource_manager.aggressive_cleanup()\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Cleanup error: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Environment Setup\n",
        "# ============================================================================\n",
        "def setup_environment():\n",
        "    \"\"\"Setup training environment for optimal performance\"\"\"\n",
        "    # Set optimal thread count for CPU utilization\n",
        "    torch.set_num_threads(min(16, os.cpu_count()))\n",
        "    os.environ['OMP_NUM_THREADS'] = str(min(16, os.cpu_count()))\n",
        "\n",
        "    # GPU optimizations\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.backends.cuda.enable_flash_sdp(True)\n",
        "\n",
        "        gpu_props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU: {gpu_props.name}, Memory: {gpu_props.total_memory / 1024**3:.1f}GB\")\n",
        "\n",
        "    print(f\"CPU Cores: {os.cpu_count()}, Using threads: {torch.get_num_threads()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Main Training Function\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"\\nStarting Fish Species Model Training...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Environment setup\n",
        "    setup_environment()\n",
        "\n",
        "    # Create output directories\n",
        "    directories = [\n",
        "        f\"{Config.OUTPUT_DIR}/models\",\n",
        "        f\"{Config.OUTPUT_DIR}/best_model\",\n",
        "        f\"{Config.OUTPUT_DIR}/training_results\",\n",
        "        f\"{Config.OUTPUT_DIR}/kfold_results\",\n",
        "        f\"{Config.OUTPUT_DIR}/kfold_models\"\n",
        "    ]\n",
        "\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Initialize resource manager\n",
        "    resource_manager = ResourceManager()\n",
        "\n",
        "    # Load and balance data (REPLACE WITH YOUR IMPLEMENTATION)\n",
        "    try:\n",
        "        print(\"\\nLoading and balancing data...\")\n",
        "        X, Y = DataManager.load_and_balance_data()\n",
        "        print(f\"Total samples after balancing: {len(X):,}, Labels: {len(Y):,}\")\n",
        "\n",
        "        # Validate data consistency\n",
        "        if len(X) != len(Y):\n",
        "            raise ValueError(f\"Inconsistent data: X has {len(X)} samples, Y has {len(Y)} labels\")\n",
        "        if len(X) == 0:\n",
        "            raise ValueError(\"No data available after loading and balancing\")\n",
        "\n",
        "    except NotImplementedError:\n",
        "        print(\"ERROR: Please implement DataManager.load_and_balance_data() with your actual data loading logic\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load hyperparameters\n",
        "    hyperparams_file = f\"{Config.OUTPUT_DIR}/hyperparameters/all_best_params.json\"\n",
        "    if os.path.exists(hyperparams_file):\n",
        "        with open(hyperparams_file, 'r') as f:\n",
        "            all_best_params = json.load(f)\n",
        "        print(f\"Loaded best parameters for {len(all_best_params)} models\")\n",
        "    else:\n",
        "        print(\"No hyperparameters found, using default parameters\")\n",
        "        all_best_params = {}\n",
        "\n",
        "    # Process each model individually\n",
        "    for model_name in Config.MODELS:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"TRAINING MODEL: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            # Get parameters for this model\n",
        "            if model_name in all_best_params:\n",
        "                best_params = all_best_params[model_name]\n",
        "                print(f\"Using optimized parameters for {model_name}\")\n",
        "            else:\n",
        "                # Default parameters\n",
        "                best_params = {\n",
        "                    \"dropout\": 0.10289132195027265,\n",
        "                    \"label_smoothing\": 0.03714841610239749,\n",
        "                    \"lr\": 0.004155652374869997,\n",
        "                    \"optimizer_type\": \"adamw\",\n",
        "                    \"scheduler_type\": \"cosine\",\n",
        "                    \"batch_size\": 64,\n",
        "                    \"weight_decay\": 2.156989662164921e-06\n",
        "                }\n",
        "                print(f\"Using default parameters for {model_name}\")\n",
        "\n",
        "            # Display parameters\n",
        "            print(f\"\\n{model_name.upper()} TRAINING PARAMETERS:\")\n",
        "            for key, value in best_params.items():\n",
        "                if key in ['lr', 'weight_decay', 'dropout', 'hidden_dim_multiplier', 'label_smoothing']:\n",
        "                    print(f\"  {key}: {value:.6f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "\n",
        "            # Create data loaders\n",
        "            print(f\"\\nCreating data loaders for {model_name}...\")\n",
        "            try:\n",
        "                train_loader, val_loader, test_loader, val_data, test_data = DataManager.create_data_loaders(\n",
        "                    X, Y, test_size=0.2,\n",
        "                    batch_size=best_params.get('batch_size', Config.BATCH_SIZE),\n",
        "                    augmentation_strength=best_params.get('augmentation_strength', 'medium')\n",
        "                )\n",
        "\n",
        "                print(f\"Data loaders created successfully\")\n",
        "                print(f\"Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
        "\n",
        "            except NotImplementedError:\n",
        "                print(\"ERROR: Please implement DataManager.create_data_loaders() with your actual data loader creation\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR creating data loaders: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Validate data loaders\n",
        "            if not train_loader or len(train_loader.dataset) == 0:\n",
        "                print(f\"Skipping {model_name}: No training data available\")\n",
        "                continue\n",
        "            if not val_loader or len(val_loader.dataset) == 0:\n",
        "                print(f\"Skipping {model_name}: No validation data available\")\n",
        "                continue\n",
        "\n",
        "            # Create model\n",
        "            print(f\"\\nCreating model: {model_name}\")\n",
        "            try:\n",
        "                model = ModelFactory.create_model(\n",
        "                    model_name, num_classes=Config.NUM_CLASSES,\n",
        "                    dropout_rate=best_params.get('dropout', 0.5),\n",
        "                    hidden_dim_multiplier=best_params.get('hidden_dim_multiplier', 0.5)\n",
        "                ).to(Config.DEVICE, memory_format=torch.channels_last)\n",
        "\n",
        "                total_params = sum(p.numel() for p in model.parameters())\n",
        "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "                print(f\"Model created: {total_params:,} total params, {trainable_params:,} trainable\")\n",
        "\n",
        "            except NotImplementedError:\n",
        "                print(\"ERROR: Please implement ModelFactory.create_model() with your actual model creation\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR creating model: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Train main model\n",
        "            print(f\"\\nStarting main model training for {model_name}\")\n",
        "            trainer = EnhancedModelTrainer(model, model_name, best_params)\n",
        "            training_success = trainer.train_main_model(train_loader, val_loader, test_loader)\n",
        "\n",
        "            if not training_success:\n",
        "                print(f\"Main training failed for {model_name}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Main model training completed for {model_name}\")\n",
        "\n",
        "            # K-fold cross-validation\n",
        "            print(f\"\\nStarting K-fold cross-validation for {model_name}\")\n",
        "            total_samples = len(train_loader.dataset)\n",
        "            min_samples_per_fold = 500\n",
        "            max_folds = total_samples // min_samples_per_fold\n",
        "            n_folds = min(3, max_folds) if max_folds > 1 else 0\n",
        "\n",
        "            if n_folds > 1:\n",
        "                kfold_success = trainer.train_kfold(train_loader, val_loader, test_loader, n_folds=n_folds)\n",
        "                if kfold_success:\n",
        "                    print(f\"K-fold validation completed for {model_name}\")\n",
        "                else:\n",
        "                    print(f\"K-fold validation failed for {model_name}\")\n",
        "            else:\n",
        "                print(f\"Skipping k-fold validation for {model_name}: insufficient data\")\n",
        "\n",
        "            # Save model for ensemble\n",
        "            model_ensemble_path = f\"{Config.OUTPUT_DIR}/models/{model_name}_for_ensemble.pt\"\n",
        "            torch.save(model.state_dict(), model_ensemble_path)\n",
        "            print(f\"Model saved for ensemble: {model_name}\")\n",
        "\n",
        "            # Cleanup\n",
        "            trainer.cleanup_trainer()\n",
        "            del trainer, model, train_loader, val_loader, test_loader\n",
        "            resource_manager.aggressive_cleanup()\n",
        "\n",
        "            print(f\"‚úÖ {model_name} TRAINING COMPLETED!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {model_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Emergency cleanup\n",
        "            try:\n",
        "                if 'trainer' in locals():\n",
        "                    trainer.cleanup_trainer()\n",
        "                    del trainer\n",
        "                if 'model' in locals():\n",
        "                    del model\n",
        "                resource_manager.aggressive_cleanup()\n",
        "            except:\n",
        "                pass\n",
        "            continue\n",
        "\n",
        "    # Final cleanup and summary\n",
        "    print(\"\\nFinal cleanup and summary...\")\n",
        "    try:\n",
        "        if 'X' in locals():\n",
        "            del X\n",
        "        if 'Y' in locals():\n",
        "            del Y\n",
        "        if 'all_best_params' in locals():\n",
        "            del all_best_params\n",
        "\n",
        "        resource_manager.aggressive_cleanup()\n",
        "        final_stats = resource_manager.get_memory_stats()\n",
        "        print(f\"Final GPU memory: {final_stats['gpu_allocated_gb']:.2f}GB ({final_stats['gpu_percent']:.1f}%)\")\n",
        "        print(f\"Final CPU usage: {final_stats['cpu_percent']:.1f}%\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in final cleanup: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MODEL TRAINING COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(f\"- Model checkpoints: {Config.OUTPUT_DIR}/models/\")\n",
        "    print(f\"- Best models: {Config.OUTPUT_DIR}/best_model/\")\n",
        "    print(f\"- Training results: {Config.OUTPUT_DIR}/training_results/\")\n",
        "    print(f\"- K-fold results: {Config.OUTPUT_DIR}/kfold_results/\")\n",
        "    print(f\"- K-fold models: {Config.OUTPUT_DIR}/kfold_models/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "V57jiUT2c3BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f9e31d-0ba3-4a93-ae41-a0dfb6d2d7b8"
      },
      "id": "V57jiUT2c3BV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Fish Species Model Training...\n",
            "======================================================================\n",
            "GPU: NVIDIA L4, Memory: 22.2GB\n",
            "CPU Cores: 12, Using threads: 12\n",
            "\n",
            "Loading and balancing data...\n",
            "Loading and preprocessing data...\n",
            "Applying SMOTE for class balancing...\n",
            "Balanced data shape: (15000, 3, 224, 224)\n",
            "Balanced class distribution: [3000 3000 3000 3000 3000]\n",
            "Total samples after balancing: 15,000, Labels: 15,000\n",
            "No hyperparameters found, using default parameters\n",
            "\n",
            "======================================================================\n",
            "TRAINING MODEL: resnet50\n",
            "======================================================================\n",
            "Using default parameters for resnet50\n",
            "\n",
            "RESNET50 TRAINING PARAMETERS:\n",
            "  dropout: 0.102891\n",
            "  label_smoothing: 0.037148\n",
            "  lr: 0.004156\n",
            "  optimizer_type: adamw\n",
            "  scheduler_type: cosine\n",
            "  batch_size: 64\n",
            "  weight_decay: 0.000002\n",
            "\n",
            "Creating data loaders for resnet50...\n",
            "Train: 9000, Val: 3000, Test: 3000\n",
            "Using optimized batch size: 64\n",
            "Data loaders created successfully\n",
            "Train: 9000, Val: 3000, Test: 3000\n",
            "\n",
            "Creating model: resnet50\n",
            "Model created: 25,613,381 total params, 24,168,453 trainable\n",
            "\n",
            "Starting main model training for resnet50\n",
            "\n",
            "Training resnet50\n",
            "Training samples: 9,000\n",
            "Validation samples: 3,000\n",
            "Total epochs: 40\n",
            "Batch size: 64\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.2714, Acc: 0.5312, Time: 41.4s\n",
            "  [Train] Batch  100 - Loss: 1.5981, Acc: 0.3438, Time: 53.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.6450, Acc: 0.3848\n",
            "  Val   - Loss: 7.5133, Acc: 0.4347, F1: 0.3918\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 110.0s, Total: 110.0s\n",
            "‚úÖ Best model saved: F1=0.3918, Acc=0.4347\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.6873, Acc: 0.2969, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 1.3748, Acc: 0.4531, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.4363, Acc: 0.4444\n",
            "  Val   - Loss: 1.2510, Acc: 0.6050, F1: 0.5643\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 41.7s, Total: 152.3s\n",
            "‚úÖ Best model saved: F1=0.5643, Acc=0.6050\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.5294, Acc: 0.3750, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 1.1605, Acc: 0.5625, Time: 24.7s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.3695, Acc: 0.4919\n",
            "  Val   - Loss: 0.9905, Acc: 0.6350, F1: 0.6283\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 41.6s, Total: 194.6s\n",
            "‚úÖ Best model saved: F1=0.6283, Acc=0.6350\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.4714, Acc: 0.3750, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 1.0952, Acc: 0.5625, Time: 24.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.2886, Acc: 0.5202\n",
            "  Val   - Loss: 1.3226, Acc: 0.6553, F1: 0.6483\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 41.4s, Total: 236.8s\n",
            "‚úÖ Best model saved: F1=0.6483, Acc=0.6553\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.2264, Acc: 0.5781, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 1.0920, Acc: 0.7031, Time: 24.8s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 5/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.1694, Acc: 0.5863\n",
            "  Val   - Loss: 4.4459, Acc: 0.7607, F1: 0.7501\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 42.1s, Total: 279.6s\n",
            "‚úÖ Best model saved: F1=0.7501, Acc=0.7607\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.1039, Acc: 0.6094, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.8865, Acc: 0.6406, Time: 24.5s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0349, Acc: 0.6416\n",
            "  Val   - Loss: 0.7140, Acc: 0.7670, F1: 0.7726\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 41.5s, Total: 321.8s\n",
            "‚úÖ Best model saved: F1=0.7726, Acc=0.7670\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.9464, Acc: 0.6562, Time: 12.3s\n",
            "  [Train] Batch  100 - Loss: 1.0037, Acc: 0.6406, Time: 24.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 7/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0195, Acc: 0.6508\n",
            "  Val   - Loss: 0.6168, Acc: 0.8397, F1: 0.8392\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 41.4s, Total: 363.9s\n",
            "‚úÖ Best model saved: F1=0.8392, Acc=0.8397\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.0272, Acc: 0.6562, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.8128, Acc: 0.7188, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 8/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8552, Acc: 0.7057\n",
            "  Val   - Loss: 0.4640, Acc: 0.8873, F1: 0.8875\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 41.6s, Total: 406.3s\n",
            "‚úÖ Best model saved: F1=0.8875, Acc=0.8873\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7612, Acc: 0.7500, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.8567, Acc: 0.7031, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 9/40 Complete \n",
            "  Train - Loss: 0.8005, Acc: 0.7317\n",
            "  Val   - Loss: 0.6116, Acc: 0.8083, F1: 0.8118\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 41.5s, Total: 448.5s\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7104, Acc: 0.7812, Time: 13.1s\n",
            "  [Train] Batch  100 - Loss: 0.6076, Acc: 0.7812, Time: 26.0s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7651, Acc: 0.7451\n",
            "  Val   - Loss: 0.4441, Acc: 0.9183, F1: 0.9186\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 43.6s, Total: 492.1s\n",
            "‚úÖ Best model saved: F1=0.9186, Acc=0.9183\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.6406, Acc: 0.7500, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.7111, Acc: 0.8438, Time: 25.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7184, Acc: 0.7727\n",
            "  Val   - Loss: 0.3648, Acc: 0.9353, F1: 0.9354\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 42.1s, Total: 534.9s\n",
            "‚úÖ Best model saved: F1=0.9354, Acc=0.9353\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.9354, Acc: 0.7188, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.6521, Acc: 0.8125, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.6791, Acc: 0.7901\n",
            "  Val   - Loss: 0.4012, Acc: 0.9320, F1: 0.9320\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 41.6s, Total: 577.3s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7842, Acc: 0.7344, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.5386, Acc: 0.8750, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 13/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6536, Acc: 0.8047\n",
            "  Val   - Loss: 0.3618, Acc: 0.9487, F1: 0.9485\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 44.0s, Total: 621.3s\n",
            "‚úÖ Best model saved: F1=0.9485, Acc=0.9487\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7447, Acc: 0.7812, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.6751, Acc: 0.7969, Time: 24.5s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.6126, Acc: 0.8220\n",
            "  Val   - Loss: 0.3926, Acc: 0.9423, F1: 0.9419\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 41.4s, Total: 663.4s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5772, Acc: 0.8125, Time: 13.2s\n",
            "  [Train] Batch  100 - Loss: 0.5096, Acc: 0.8906, Time: 26.0s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 15/40 Complete \n",
            "  Train - Loss: 0.5875, Acc: 0.8331\n",
            "  Val   - Loss: 0.3629, Acc: 0.9437, F1: 0.9422\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 43.6s, Total: 707.0s\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5929, Acc: 0.8125, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.5885, Acc: 0.9219, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 16/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5626, Acc: 0.8443\n",
            "  Val   - Loss: 0.2969, Acc: 0.9613, F1: 0.9614\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 43.8s, Total: 750.8s\n",
            "‚úÖ Best model saved: F1=0.9614, Acc=0.9613\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7261, Acc: 0.7656, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.5924, Acc: 0.8125, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 17/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5419, Acc: 0.8524\n",
            "  Val   - Loss: 0.2930, Acc: 0.9730, F1: 0.9729\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 41.6s, Total: 793.2s\n",
            "‚úÖ Best model saved: F1=0.9729, Acc=0.9730\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5505, Acc: 0.8438, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.5456, Acc: 0.8594, Time: 25.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 18/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5228, Acc: 0.8609\n",
            "  Val   - Loss: 0.2743, Acc: 0.9800, F1: 0.9800\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 42.1s, Total: 836.0s\n",
            "‚úÖ Best model saved: F1=0.9800, Acc=0.9800\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4587, Acc: 0.8906, Time: 12.6s\n",
            "  [Train] Batch  100 - Loss: 0.5378, Acc: 0.8594, Time: 25.0s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 19/40 Complete \n",
            "  Train - Loss: 0.4941, Acc: 0.8742\n",
            "  Val   - Loss: 0.2961, Acc: 0.9650, F1: 0.9650\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 42.1s, Total: 878.8s\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5680, Acc: 0.8125, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.5748, Acc: 0.7969, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 20/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4918, Acc: 0.8717\n",
            "  Val   - Loss: 0.2517, Acc: 0.9830, F1: 0.9830\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 43.8s, Total: 922.7s\n",
            "‚úÖ Best model saved: F1=0.9830, Acc=0.9830\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5452, Acc: 0.8281, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.5412, Acc: 0.8750, Time: 25.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 21/40 Complete \n",
            "  Train - Loss: 0.4681, Acc: 0.8867\n",
            "  Val   - Loss: 0.2693, Acc: 0.9783, F1: 0.9784\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 42.1s, Total: 965.5s\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.5778, Acc: 0.7969, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.3218, Acc: 0.9688, Time: 26.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 22/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4479, Acc: 0.8930\n",
            "  Val   - Loss: 0.2464, Acc: 0.9863, F1: 0.9863\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 44.1s, Total: 1009.7s\n",
            "‚úÖ Best model saved: F1=0.9863, Acc=0.9863\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3453, Acc: 0.9531, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.5909, Acc: 0.8125, Time: 24.8s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 23/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4403, Acc: 0.8978\n",
            "  Val   - Loss: 0.2333, Acc: 0.9873, F1: 0.9873\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 41.9s, Total: 1052.3s\n",
            "‚úÖ Best model saved: F1=0.9873, Acc=0.9873\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3502, Acc: 0.9375, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.5480, Acc: 0.8438, Time: 24.8s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 24/40 Complete \n",
            "  Train - Loss: 0.4345, Acc: 0.8996\n",
            "  Val   - Loss: 0.2418, Acc: 0.9867, F1: 0.9866\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 41.8s, Total: 1094.8s\n",
            "\n",
            "Epoch 25/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3708, Acc: 0.9219, Time: 13.2s\n",
            "  [Train] Batch  100 - Loss: 0.3333, Acc: 0.9375, Time: 26.0s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 25/40 Complete \n",
            "  Train - Loss: 0.4228, Acc: 0.9073\n",
            "  Val   - Loss: 0.2373, Acc: 0.9820, F1: 0.9821\n",
            "  LR: 0.001283\n",
            "  Epoch Time: 43.6s, Total: 1138.4s\n",
            "\n",
            "Epoch 26/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4450, Acc: 0.8906, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.5372, Acc: 0.8125, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 26/40 Complete \n",
            "  Train - Loss: 0.4194, Acc: 0.9064\n",
            "  Val   - Loss: 0.2393, Acc: 0.9830, F1: 0.9830\n",
            "  LR: 0.001135\n",
            "  Epoch Time: 43.9s, Total: 1182.3s\n",
            "\n",
            "Epoch 27/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3705, Acc: 0.9062, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.4787, Acc: 0.8750, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 27/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4030, Acc: 0.9122\n",
            "  Val   - Loss: 0.2240, Acc: 0.9883, F1: 0.9883\n",
            "  LR: 0.000993\n",
            "  Epoch Time: 43.7s, Total: 1226.0s\n",
            "‚úÖ Best model saved: F1=0.9883, Acc=0.9883\n",
            "\n",
            "Epoch 28/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3769, Acc: 0.9531, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.4177, Acc: 0.9062, Time: 24.7s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 28/40 Complete \n",
            "  Train - Loss: 0.4097, Acc: 0.9092\n",
            "  Val   - Loss: 0.2177, Acc: 0.9890, F1: 0.9890\n",
            "  LR: 0.000857\n",
            "  Epoch Time: 41.8s, Total: 1268.6s\n",
            "\n",
            "Epoch 29/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4178, Acc: 0.8750, Time: 13.2s\n",
            "  [Train] Batch  100 - Loss: 0.4107, Acc: 0.8906, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 29/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3859, Acc: 0.9211\n",
            "  Val   - Loss: 0.2167, Acc: 0.9897, F1: 0.9897\n",
            "  LR: 0.000729\n",
            "  Epoch Time: 43.9s, Total: 1312.5s\n",
            "‚úÖ Best model saved: F1=0.9897, Acc=0.9897\n",
            "\n",
            "Epoch 30/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3965, Acc: 0.9375, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.4139, Acc: 0.9062, Time: 24.7s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 30/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3837, Acc: 0.9194\n",
            "  Val   - Loss: 0.2124, Acc: 0.9910, F1: 0.9910\n",
            "  LR: 0.000609\n",
            "  Epoch Time: 41.8s, Total: 1355.0s\n",
            "‚úÖ Best model saved: F1=0.9910, Acc=0.9910\n",
            "\n",
            "Epoch 31/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.2965, Acc: 0.9531, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.3071, Acc: 0.9375, Time: 24.6s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 31/40 Complete \n",
            "  Train - Loss: 0.3659, Acc: 0.9263\n",
            "  Val   - Loss: 0.2124, Acc: 0.9917, F1: 0.9917\n",
            "  LR: 0.000499\n",
            "  Epoch Time: 41.7s, Total: 1397.4s\n",
            "\n",
            "Epoch 32/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3677, Acc: 0.9219, Time: 13.7s\n",
            "  [Train] Batch  100 - Loss: 0.2746, Acc: 0.9688, Time: 26.7s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 32/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3650, Acc: 0.9293\n",
            "  Val   - Loss: 0.2098, Acc: 0.9923, F1: 0.9923\n",
            "  LR: 0.000398\n",
            "  Epoch Time: 44.4s, Total: 1441.9s\n",
            "‚úÖ Best model saved: F1=0.9923, Acc=0.9923\n",
            "\n",
            "Epoch 33/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4248, Acc: 0.8906, Time: 12.4s\n",
            "  [Train] Batch  100 - Loss: 0.3924, Acc: 0.9062, Time: 24.5s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 33/40 Complete \n",
            "  Train - Loss: 0.3673, Acc: 0.9263\n",
            "  Val   - Loss: 0.2103, Acc: 0.9920, F1: 0.9920\n",
            "  LR: 0.000307\n",
            "  Epoch Time: 41.8s, Total: 1484.4s\n",
            "\n",
            "Epoch 34/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4516, Acc: 0.8906, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.3412, Acc: 0.9219, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 34/40 Complete \n",
            "  Train - Loss: 0.3629, Acc: 0.9302\n",
            "  Val   - Loss: 0.2092, Acc: 0.9930, F1: 0.9930\n",
            "  LR: 0.000227\n",
            "  Epoch Time: 44.0s, Total: 1528.4s\n",
            "\n",
            "Epoch 35/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3330, Acc: 0.9219, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.2688, Acc: 0.9844, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 35/40 Complete \n",
            "  Train - Loss: 0.3574, Acc: 0.9297\n",
            "  Val   - Loss: 0.2082, Acc: 0.9920, F1: 0.9920\n",
            "  LR: 0.000159\n",
            "  Epoch Time: 43.9s, Total: 1572.3s\n",
            "\n",
            "Epoch 36/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3666, Acc: 0.9219, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.3441, Acc: 0.9062, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 36/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3545, Acc: 0.9307\n",
            "  Val   - Loss: 0.2076, Acc: 0.9933, F1: 0.9933\n",
            "  LR: 0.000103\n",
            "  Epoch Time: 43.8s, Total: 1616.2s\n",
            "‚úÖ Best model saved: F1=0.9933, Acc=0.9933\n",
            "\n",
            "Epoch 37/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3194, Acc: 0.9688, Time: 12.5s\n",
            "  [Train] Batch  100 - Loss: 0.4149, Acc: 0.8906, Time: 24.8s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 37/40 Complete \n",
            "  Train - Loss: 0.3495, Acc: 0.9349\n",
            "  Val   - Loss: 0.2068, Acc: 0.9923, F1: 0.9923\n",
            "  LR: 0.000058\n",
            "  Epoch Time: 41.8s, Total: 1658.8s\n",
            "\n",
            "Epoch 38/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.4188, Acc: 0.9062, Time: 13.2s\n",
            "  [Train] Batch  100 - Loss: 0.3930, Acc: 0.9375, Time: 26.0s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 38/40 Complete \n",
            "  Train - Loss: 0.3509, Acc: 0.9343\n",
            "  Val   - Loss: 0.2058, Acc: 0.9920, F1: 0.9920\n",
            "  LR: 0.000027\n",
            "  Epoch Time: 43.6s, Total: 1702.4s\n",
            "\n",
            "Epoch 39/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.3487, Acc: 0.9375, Time: 13.2s\n",
            "  [Train] Batch  100 - Loss: 0.2561, Acc: 0.9688, Time: 26.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 39/40 Complete \n",
            "  Train - Loss: 0.3434, Acc: 0.9354\n",
            "  Val   - Loss: 0.2068, Acc: 0.9923, F1: 0.9923\n",
            "  LR: 0.000007\n",
            "  Epoch Time: 43.7s, Total: 1746.1s\n",
            "\n",
            "Epoch 40/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.2959, Acc: 0.9688, Time: 13.1s\n",
            "  [Train] Batch  100 - Loss: 0.2581, Acc: 0.9688, Time: 26.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 40/40 Complete \n",
            "  Train - Loss: 0.3390, Acc: 0.9387\n",
            "  Val   - Loss: 0.2056, Acc: 0.9920, F1: 0.9920\n",
            "  LR: 0.000001\n",
            "  Epoch Time: 43.7s, Total: 1789.8s\n",
            "Early stopping at epoch 40\n",
            "Total training time: 1789.8s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating resnet50...\n",
            "Evaluation Results for resnet50:\n",
            "  Accuracy: 0.9903\n",
            "  F1-Macro: 0.9903\n",
            "  F1-Weighted: 0.9903\n",
            "  Misclassified: 29/3000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_training_data.pkl\n",
            "\n",
            "Training Summary for resnet50:\n",
            "  Final Accuracy: 0.9903\n",
            "  Final F1 Score: 0.9903\n",
            "  Best Validation F1: 0.9933\n",
            "  Total Training Time: 1827.0s\n",
            "  Final Epoch: 40/40\n",
            "Main model training completed for resnet50\n",
            "\n",
            "Starting K-fold cross-validation for resnet50\n",
            "\n",
            "K-fold Cross-Validation for resnet50 (3 folds)\n",
            "\n",
            "Training Fold 1/3\n",
            "\n",
            "Training resnet50_fold_1\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 61\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.3942, Acc: 0.4262, Time: 32.5s\n",
            "  [Train] Batch  100 - Loss: 1.0622, Acc: 0.6557, Time: 35.8s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.0858, Acc: 0.5574, Time: 69.6s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.4322, Acc: 0.4945\n",
            "  Val   - Loss: 1.4093, Acc: 0.5022, F1: 0.4846\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 81.2s, Total: 81.2s\n",
            "‚úÖ Best model saved: F1=0.4846, Acc=0.5022\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.1928, Acc: 0.5082, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.8937, Acc: 0.6721, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.1372, Acc: 0.5246, Time: 13.3s\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0508, Acc: 0.6116\n",
            "  Val   - Loss: 1.0966, Acc: 0.5765, F1: 0.5941\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 14.1s, Total: 96.0s\n",
            "‚úÖ Best model saved: F1=0.5941, Acc=0.5765\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8405, Acc: 0.6557, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.8574, Acc: 0.7869, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.9941, Acc: 0.6393, Time: 12.9s\n",
            "\n",
            "Epoch 3/40 Complete \n",
            "  Train - Loss: 0.9261, Acc: 0.6767\n",
            "  Val   - Loss: 1.1276, Acc: 0.5760, F1: 0.5706\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 13.6s, Total: 110.3s\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8675, Acc: 0.7213, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.8899, Acc: 0.6393, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.2514, Acc: 0.5574, Time: 13.0s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8186, Acc: 0.7282\n",
            "  Val   - Loss: 1.1790, Acc: 0.6340, F1: 0.6256\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 13.7s, Total: 124.0s\n",
            "‚úÖ Best model saved: F1=0.6256, Acc=0.6340\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8076, Acc: 0.7049, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.7930, Acc: 0.6885, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.2468, Acc: 0.5902, Time: 12.7s\n",
            "\n",
            "Epoch 5/40 Complete \n",
            "  Train - Loss: 0.7442, Acc: 0.7609\n",
            "  Val   - Loss: 1.3405, Acc: 0.5435, F1: 0.5280\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 13.5s, Total: 138.2s\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6415, Acc: 0.8033, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5859, Acc: 0.8525, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.8569, Acc: 0.7377, Time: 12.7s\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6788, Acc: 0.7877\n",
            "  Val   - Loss: 0.8149, Acc: 0.7265, F1: 0.7264\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 13.4s, Total: 151.6s\n",
            "‚úÖ Best model saved: F1=0.7264, Acc=0.7265\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7498, Acc: 0.7705, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5856, Acc: 0.8525, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6294, Acc: 0.8197, Time: 12.4s\n",
            "\n",
            "Epoch 7/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6328, Acc: 0.8093\n",
            "  Val   - Loss: 0.8694, Acc: 0.7380, F1: 0.7374\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 13.2s, Total: 165.5s\n",
            "‚úÖ Best model saved: F1=0.7374, Acc=0.7380\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6481, Acc: 0.8033, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5151, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5960, Acc: 0.8525, Time: 12.9s\n",
            "\n",
            "Epoch 8/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6145, Acc: 0.8194\n",
            "  Val   - Loss: 0.6795, Acc: 0.7758, F1: 0.7769\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 13.7s, Total: 181.6s\n",
            "‚úÖ Best model saved: F1=0.7769, Acc=0.7758\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8948, Acc: 0.7049, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5581, Acc: 0.8033, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5822, Acc: 0.8689, Time: 12.8s\n",
            "\n",
            "Epoch 9/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5772, Acc: 0.8330\n",
            "  Val   - Loss: 0.6872, Acc: 0.7873, F1: 0.7859\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 13.5s, Total: 198.6s\n",
            "‚úÖ Best model saved: F1=0.7859, Acc=0.7873\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5887, Acc: 0.8197, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4110, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5580, Acc: 0.8197, Time: 12.8s\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5615, Acc: 0.8450\n",
            "  Val   - Loss: 0.6509, Acc: 0.7950, F1: 0.7948\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 13.5s, Total: 215.9s\n",
            "‚úÖ Best model saved: F1=0.7948, Acc=0.7950\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4909, Acc: 0.8852, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.7350, Acc: 0.7705, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6627, Acc: 0.7705, Time: 13.2s\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5388, Acc: 0.8524\n",
            "  Val   - Loss: 0.6536, Acc: 0.8027, F1: 0.8023\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 13.9s, Total: 233.8s\n",
            "‚úÖ Best model saved: F1=0.8023, Acc=0.8027\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5974, Acc: 0.8361, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5880, Acc: 0.8525, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6223, Acc: 0.7705, Time: 12.9s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.5357, Acc: 0.8559\n",
            "  Val   - Loss: 0.6357, Acc: 0.7987, F1: 0.8008\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 13.7s, Total: 250.2s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6499, Acc: 0.7705, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5565, Acc: 0.8033, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5157, Acc: 0.8852, Time: 13.0s\n",
            "\n",
            "Epoch 13/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5125, Acc: 0.8644\n",
            "  Val   - Loss: 0.5973, Acc: 0.8243, F1: 0.8243\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 13.7s, Total: 263.9s\n",
            "‚úÖ Best model saved: F1=0.8243, Acc=0.8243\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4702, Acc: 0.8525, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.7522, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5400, Acc: 0.8689, Time: 13.0s\n",
            "\n",
            "Epoch 14/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5127, Acc: 0.8714\n",
            "  Val   - Loss: 0.6058, Acc: 0.8290, F1: 0.8295\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 13.7s, Total: 278.2s\n",
            "‚úÖ Best model saved: F1=0.8295, Acc=0.8290\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5476, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5234, Acc: 0.8197, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4681, Acc: 0.9180, Time: 13.3s\n",
            "\n",
            "Epoch 15/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4880, Acc: 0.8755\n",
            "  Val   - Loss: 0.5680, Acc: 0.8377, F1: 0.8378\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 14.0s, Total: 294.4s\n",
            "‚úÖ Best model saved: F1=0.8378, Acc=0.8377\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4105, Acc: 0.9016, Time: 3.8s\n",
            "  [Train] Batch  100 - Loss: 0.5522, Acc: 0.8689, Time: 7.1s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6144, Acc: 0.7705, Time: 12.9s\n",
            "\n",
            "Epoch 16/40 Complete \n",
            "  Train - Loss: 0.4695, Acc: 0.8834\n",
            "  Val   - Loss: 0.5927, Acc: 0.8215, F1: 0.8222\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 13.7s, Total: 311.2s\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4769, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4268, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4408, Acc: 0.9180, Time: 12.7s\n",
            "\n",
            "Epoch 17/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4560, Acc: 0.8875\n",
            "  Val   - Loss: 0.5176, Acc: 0.8535, F1: 0.8535\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 13.3s, Total: 324.5s\n",
            "‚úÖ Best model saved: F1=0.8535, Acc=0.8535\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5519, Acc: 0.8361, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.6539, Acc: 0.8197, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4419, Acc: 0.8852, Time: 12.8s\n",
            "\n",
            "Epoch 18/40 Complete \n",
            "  Train - Loss: 0.4623, Acc: 0.8860\n",
            "  Val   - Loss: 0.5279, Acc: 0.8448, F1: 0.8450\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 13.6s, Total: 338.9s\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3951, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3375, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3959, Acc: 0.9344, Time: 12.7s\n",
            "\n",
            "Epoch 19/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4378, Acc: 0.8984\n",
            "  Val   - Loss: 0.4916, Acc: 0.8745, F1: 0.8741\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 13.4s, Total: 352.2s\n",
            "‚úÖ Best model saved: F1=0.8741, Acc=0.8745\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3795, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3817, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4436, Acc: 0.9016, Time: 12.6s\n",
            "\n",
            "Epoch 20/40 Complete \n",
            "  Train - Loss: 0.4319, Acc: 0.8975\n",
            "  Val   - Loss: 0.5000, Acc: 0.8632, F1: 0.8642\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 13.5s, Total: 366.5s\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4738, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3946, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5230, Acc: 0.8852, Time: 12.8s\n",
            "\n",
            "Epoch 21/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4254, Acc: 0.9018\n",
            "  Val   - Loss: 0.4800, Acc: 0.8752, F1: 0.8751\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 13.6s, Total: 380.1s\n",
            "‚úÖ Best model saved: F1=0.8751, Acc=0.8752\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4799, Acc: 0.8525, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4093, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3983, Acc: 0.9344, Time: 13.1s\n",
            "\n",
            "Epoch 22/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4152, Acc: 0.9056\n",
            "  Val   - Loss: 0.4726, Acc: 0.8825, F1: 0.8822\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 13.9s, Total: 394.8s\n",
            "‚úÖ Best model saved: F1=0.8822, Acc=0.8825\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4800, Acc: 0.8525, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4968, Acc: 0.8525, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4063, Acc: 0.9180, Time: 12.7s\n",
            "\n",
            "Epoch 23/40 Complete \n",
            "  Train - Loss: 0.3998, Acc: 0.9129\n",
            "  Val   - Loss: 0.4774, Acc: 0.8815, F1: 0.8806\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 13.5s, Total: 411.4s\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4723, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.6104, Acc: 0.8033, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4183, Acc: 0.9344, Time: 12.6s\n",
            "\n",
            "Epoch 24/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3982, Acc: 0.9141\n",
            "  Val   - Loss: 0.4532, Acc: 0.8842, F1: 0.8856\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 13.4s, Total: 424.7s\n",
            "‚úÖ Best model saved: F1=0.8856, Acc=0.8842\n",
            "\n",
            "Epoch 25/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4376, Acc: 0.8852, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4271, Acc: 0.9180, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4806, Acc: 0.8361, Time: 12.8s\n",
            "\n",
            "Epoch 25/40 Complete \n",
            "  Train - Loss: 0.3951, Acc: 0.9121\n",
            "  Val   - Loss: 0.4527, Acc: 0.8860, F1: 0.8851\n",
            "  LR: 0.001283\n",
            "  Epoch Time: 13.5s, Total: 438.9s\n",
            "\n",
            "Epoch 26/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3178, Acc: 0.9672, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4512, Acc: 0.9016, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3554, Acc: 0.9344, Time: 12.7s\n",
            "\n",
            "Epoch 26/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3813, Acc: 0.9219\n",
            "  Val   - Loss: 0.4445, Acc: 0.8918, F1: 0.8918\n",
            "  LR: 0.001135\n",
            "  Epoch Time: 13.4s, Total: 452.4s\n",
            "‚úÖ Best model saved: F1=0.8918, Acc=0.8918\n",
            "\n",
            "Epoch 27/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5114, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3603, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5053, Acc: 0.8852, Time: 13.1s\n",
            "\n",
            "Epoch 27/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3707, Acc: 0.9266\n",
            "  Val   - Loss: 0.4223, Acc: 0.9055, F1: 0.9055\n",
            "  LR: 0.000993\n",
            "  Epoch Time: 13.9s, Total: 467.0s\n",
            "‚úÖ Best model saved: F1=0.9055, Acc=0.9055\n",
            "\n",
            "Epoch 28/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3139, Acc: 0.9508, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3291, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3886, Acc: 0.9016, Time: 13.0s\n",
            "\n",
            "Epoch 28/40 Complete \n",
            "  Train - Loss: 0.3794, Acc: 0.9225\n",
            "  Val   - Loss: 0.4206, Acc: 0.9030, F1: 0.9032\n",
            "  LR: 0.000857\n",
            "  Epoch Time: 13.8s, Total: 483.9s\n",
            "\n",
            "Epoch 29/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3353, Acc: 0.9836, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3080, Acc: 0.9508, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3836, Acc: 0.9016, Time: 13.3s\n",
            "\n",
            "Epoch 29/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3572, Acc: 0.9339\n",
            "  Val   - Loss: 0.4039, Acc: 0.9070, F1: 0.9073\n",
            "  LR: 0.000729\n",
            "  Epoch Time: 14.1s, Total: 497.9s\n",
            "‚úÖ Best model saved: F1=0.9073, Acc=0.9070\n",
            "\n",
            "Epoch 30/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3413, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.3589, Acc: 0.9344, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4199, Acc: 0.9344, Time: 13.3s\n",
            "\n",
            "Epoch 30/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3472, Acc: 0.9347\n",
            "  Val   - Loss: 0.3866, Acc: 0.9163, F1: 0.9166\n",
            "  LR: 0.000609\n",
            "  Epoch Time: 14.1s, Total: 513.8s\n",
            "‚úÖ Best model saved: F1=0.9166, Acc=0.9163\n",
            "\n",
            "Epoch 31/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3432, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2542, Acc: 1.0000, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4374, Acc: 0.9180, Time: 12.9s\n",
            "\n",
            "Epoch 31/40 Complete \n",
            "  Train - Loss: 0.3483, Acc: 0.9339\n",
            "  Val   - Loss: 0.3917, Acc: 0.9135, F1: 0.9134\n",
            "  LR: 0.000499\n",
            "  Epoch Time: 13.6s, Total: 530.8s\n",
            "\n",
            "Epoch 32/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2837, Acc: 0.9672, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 0.2403, Acc: 0.9836, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3381, Acc: 0.9344, Time: 12.8s\n",
            "\n",
            "Epoch 32/40 Complete \n",
            "  Train - Loss: 0.3417, Acc: 0.9361\n",
            "  Val   - Loss: 0.3944, Acc: 0.9103, F1: 0.9103\n",
            "  LR: 0.000398\n",
            "  Epoch Time: 13.5s, Total: 544.3s\n",
            "\n",
            "Epoch 33/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2847, Acc: 0.9508, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5293, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4726, Acc: 0.8361, Time: 12.7s\n",
            "\n",
            "Epoch 33/40 Complete \n",
            "  Train - Loss: 0.3285, Acc: 0.9443\n",
            "  Val   - Loss: 0.3847, Acc: 0.9167, F1: 0.9167\n",
            "  LR: 0.000307\n",
            "  Epoch Time: 13.6s, Total: 557.9s\n",
            "\n",
            "Epoch 34/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4019, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2743, Acc: 0.9672, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3297, Acc: 0.9672, Time: 12.7s\n",
            "\n",
            "Epoch 34/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3354, Acc: 0.9401\n",
            "  Val   - Loss: 0.3764, Acc: 0.9200, F1: 0.9200\n",
            "  LR: 0.000227\n",
            "  Epoch Time: 13.4s, Total: 571.3s\n",
            "‚úÖ Best model saved: F1=0.9200, Acc=0.9200\n",
            "\n",
            "Epoch 35/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3479, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.2815, Acc: 0.9672, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3879, Acc: 0.9180, Time: 13.1s\n",
            "\n",
            "Epoch 35/40 Complete \n",
            "  Train - Loss: 0.3362, Acc: 0.9405\n",
            "  Val   - Loss: 0.3800, Acc: 0.9187, F1: 0.9187\n",
            "  LR: 0.000159\n",
            "  Epoch Time: 13.8s, Total: 585.8s\n",
            "\n",
            "Epoch 36/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3968, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.2631, Acc: 0.9508, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3707, Acc: 0.9016, Time: 12.8s\n",
            "\n",
            "Epoch 36/40 Complete \n",
            "  Train - Loss: 0.3259, Acc: 0.9443\n",
            "  Val   - Loss: 0.3709, Acc: 0.9183, F1: 0.9184\n",
            "  LR: 0.000103\n",
            "  Epoch Time: 13.6s, Total: 599.4s\n",
            "\n",
            "Epoch 37/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3256, Acc: 0.9180, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2892, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2898, Acc: 0.9672, Time: 12.5s\n",
            "\n",
            "Epoch 37/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3216, Acc: 0.9436\n",
            "  Val   - Loss: 0.3610, Acc: 0.9265, F1: 0.9267\n",
            "  LR: 0.000058\n",
            "  Epoch Time: 13.2s, Total: 612.6s\n",
            "‚úÖ Best model saved: F1=0.9267, Acc=0.9265\n",
            "\n",
            "Epoch 38/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3184, Acc: 0.9508, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2455, Acc: 0.9836, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3345, Acc: 0.9508, Time: 12.8s\n",
            "\n",
            "Epoch 38/40 Complete \n",
            "  Train - Loss: 0.3268, Acc: 0.9434\n",
            "  Val   - Loss: 0.3569, Acc: 0.9255, F1: 0.9257\n",
            "  LR: 0.000027\n",
            "  Epoch Time: 13.6s, Total: 626.9s\n",
            "\n",
            "Epoch 39/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3606, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.3792, Acc: 0.9344, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4006, Acc: 0.9016, Time: 12.7s\n",
            "\n",
            "Epoch 39/40 Complete \n",
            "  Train - Loss: 0.3209, Acc: 0.9457\n",
            "  Val   - Loss: 0.3808, Acc: 0.9195, F1: 0.9195\n",
            "  LR: 0.000007\n",
            "  Epoch Time: 13.5s, Total: 640.4s\n",
            "\n",
            "Epoch 40/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3307, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2927, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4707, Acc: 0.8852, Time: 12.6s\n",
            "\n",
            "Epoch 40/40 Complete \n",
            "  Train - Loss: 0.3144, Acc: 0.9481\n",
            "  Val   - Loss: 0.3696, Acc: 0.9213, F1: 0.9213\n",
            "  LR: 0.000001\n",
            "  Epoch Time: 13.3s, Total: 653.7s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating resnet50_fold_1...\n",
            "Evaluation Results for resnet50_fold_1:\n",
            "  Accuracy: 0.9215\n",
            "  F1-Macro: 0.9215\n",
            "  F1-Weighted: 0.9213\n",
            "  Misclassified: 314/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_1_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_1_training_data.pkl\n",
            "\n",
            "Training Summary for resnet50_fold_1:\n",
            "  Final Accuracy: 0.9215\n",
            "  Final F1 Score: 0.9215\n",
            "  Best Validation F1: 0.9267\n",
            "  Total Training Time: 689.7s\n",
            "  Final Epoch: 40/40\n",
            "\n",
            "Evaluating resnet50_fold_1...\n",
            "Evaluation Results for resnet50_fold_1:\n",
            "  Accuracy: 0.9903\n",
            "  F1-Macro: 0.9903\n",
            "  F1-Weighted: 0.9903\n",
            "  Misclassified: 29/3000\n",
            "Fold 1 completed - Acc: 0.9903, F1: 0.9903, Time: 690.5s\n",
            "\n",
            "Training Fold 2/3\n",
            "\n",
            "Training resnet50_fold_2\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 61\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.4636, Acc: 0.4590, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 1.6402, Acc: 0.4426, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 13.4704, Acc: 0.3443, Time: 12.6s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.5477, Acc: 0.4509\n",
            "  Val   - Loss: 16.4604, Acc: 0.4007, F1: 0.3825\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 13.3s, Total: 13.3s\n",
            "‚úÖ Best model saved: F1=0.3825, Acc=0.4007\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.3734, Acc: 0.4754, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.9820, Acc: 0.7049, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.0405, Acc: 0.6066, Time: 12.7s\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.1007, Acc: 0.6076\n",
            "  Val   - Loss: 1.1287, Acc: 0.5703, F1: 0.5549\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 13.4s, Total: 27.4s\n",
            "‚úÖ Best model saved: F1=0.5549, Acc=0.5703\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.0498, Acc: 0.6230, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.8240, Acc: 0.7869, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.0787, Acc: 0.5738, Time: 12.6s\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9410, Acc: 0.6759\n",
            "  Val   - Loss: 1.0011, Acc: 0.6522, F1: 0.6400\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 13.4s, Total: 41.4s\n",
            "‚úÖ Best model saved: F1=0.6400, Acc=0.6522\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7271, Acc: 0.7705, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.9148, Acc: 0.7705, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.7933, Acc: 0.7049, Time: 12.6s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8385, Acc: 0.7230\n",
            "  Val   - Loss: 0.9108, Acc: 0.6683, F1: 0.6671\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 13.3s, Total: 55.5s\n",
            "‚úÖ Best model saved: F1=0.6671, Acc=0.6683\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7177, Acc: 0.7705, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.7318, Acc: 0.7541, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.9044, Acc: 0.6557, Time: 12.5s\n",
            "\n",
            "Epoch 5/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7322, Acc: 0.7652\n",
            "  Val   - Loss: 0.8143, Acc: 0.7210, F1: 0.7249\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 13.3s, Total: 69.5s\n",
            "‚úÖ Best model saved: F1=0.7249, Acc=0.7210\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8820, Acc: 0.7049, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.6798, Acc: 0.7705, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5822, Acc: 0.8525, Time: 12.9s\n",
            "\n",
            "Epoch 6/40 Complete \n",
            "  Train - Loss: 0.6748, Acc: 0.7939\n",
            "  Val   - Loss: 0.8307, Acc: 0.7015, F1: 0.7026\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 13.7s, Total: 83.9s\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5871, Acc: 0.8525, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.6835, Acc: 0.7377, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.7768, Acc: 0.8361, Time: 13.0s\n",
            "\n",
            "Epoch 7/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6254, Acc: 0.8164\n",
            "  Val   - Loss: 0.7182, Acc: 0.7650, F1: 0.7640\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 13.7s, Total: 97.5s\n",
            "‚úÖ Best model saved: F1=0.7640, Acc=0.7650\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6153, Acc: 0.8197, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5838, Acc: 0.8689, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6028, Acc: 0.7705, Time: 12.9s\n",
            "\n",
            "Epoch 8/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5765, Acc: 0.8396\n",
            "  Val   - Loss: 0.7228, Acc: 0.7680, F1: 0.7707\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 13.7s, Total: 111.9s\n",
            "‚úÖ Best model saved: F1=0.7707, Acc=0.7680\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6090, Acc: 0.8033, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5367, Acc: 0.8361, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5960, Acc: 0.8197, Time: 12.5s\n",
            "\n",
            "Epoch 9/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5615, Acc: 0.8485\n",
            "  Val   - Loss: 0.6548, Acc: 0.7950, F1: 0.7938\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 13.2s, Total: 128.5s\n",
            "‚úÖ Best model saved: F1=0.7938, Acc=0.7950\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6016, Acc: 0.8197, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5584, Acc: 0.8525, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.8164, Acc: 0.7705, Time: 12.9s\n",
            "\n",
            "Epoch 10/40 Complete \n",
            "  Train - Loss: 0.5423, Acc: 0.8539\n",
            "  Val   - Loss: 1.0778, Acc: 0.6720, F1: 0.6652\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 13.7s, Total: 142.9s\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5267, Acc: 0.8361, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5277, Acc: 0.8852, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5660, Acc: 0.8033, Time: 12.7s\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5234, Acc: 0.8601\n",
            "  Val   - Loss: 0.6005, Acc: 0.8237, F1: 0.8236\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 13.3s, Total: 156.3s\n",
            "‚úÖ Best model saved: F1=0.8236, Acc=0.8237\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4499, Acc: 0.8852, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4274, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5213, Acc: 0.8361, Time: 12.7s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.4987, Acc: 0.8736\n",
            "  Val   - Loss: 0.6329, Acc: 0.8197, F1: 0.8182\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 13.5s, Total: 170.5s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4729, Acc: 0.8852, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3759, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4160, Acc: 0.9180, Time: 12.7s\n",
            "\n",
            "Epoch 13/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4954, Acc: 0.8734\n",
            "  Val   - Loss: 0.5768, Acc: 0.8410, F1: 0.8396\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 13.5s, Total: 184.0s\n",
            "‚úÖ Best model saved: F1=0.8396, Acc=0.8410\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5666, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4341, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5769, Acc: 0.7869, Time: 13.2s\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.4767, Acc: 0.8808\n",
            "  Val   - Loss: 0.6392, Acc: 0.8117, F1: 0.8100\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 14.0s, Total: 198.8s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4781, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5028, Acc: 0.9016, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5480, Acc: 0.8361, Time: 12.9s\n",
            "\n",
            "Epoch 15/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4583, Acc: 0.8899\n",
            "  Val   - Loss: 0.5354, Acc: 0.8552, F1: 0.8553\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 13.7s, Total: 212.4s\n",
            "‚úÖ Best model saved: F1=0.8553, Acc=0.8552\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5766, Acc: 0.8361, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4116, Acc: 0.9180, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4642, Acc: 0.9016, Time: 12.8s\n",
            "\n",
            "Epoch 16/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4511, Acc: 0.8949\n",
            "  Val   - Loss: 0.5189, Acc: 0.8598, F1: 0.8592\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 13.6s, Total: 226.8s\n",
            "‚úÖ Best model saved: F1=0.8592, Acc=0.8598\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4334, Acc: 0.8852, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5260, Acc: 0.8689, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3682, Acc: 0.9180, Time: 12.5s\n",
            "\n",
            "Epoch 17/40 Complete \n",
            "  Train - Loss: 0.4314, Acc: 0.9015\n",
            "  Val   - Loss: 0.5380, Acc: 0.8592, F1: 0.8575\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 13.2s, Total: 243.2s\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4868, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4679, Acc: 0.8852, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4002, Acc: 0.9344, Time: 12.6s\n",
            "\n",
            "Epoch 18/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4216, Acc: 0.9065\n",
            "  Val   - Loss: 0.5111, Acc: 0.8628, F1: 0.8626\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 13.3s, Total: 256.5s\n",
            "‚úÖ Best model saved: F1=0.8626, Acc=0.8628\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3852, Acc: 0.9180, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5247, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5280, Acc: 0.8525, Time: 12.6s\n",
            "\n",
            "Epoch 19/40 Complete \n",
            "  Train - Loss: 0.4023, Acc: 0.9136\n",
            "  Val   - Loss: 0.5208, Acc: 0.8625, F1: 0.8630\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 13.4s, Total: 270.6s\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4228, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3827, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4633, Acc: 0.8689, Time: 12.7s\n",
            "\n",
            "Epoch 20/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3974, Acc: 0.9125\n",
            "  Val   - Loss: 0.4716, Acc: 0.8745, F1: 0.8750\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 13.5s, Total: 284.1s\n",
            "‚úÖ Best model saved: F1=0.8750, Acc=0.8745\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3366, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3515, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3620, Acc: 0.9180, Time: 12.9s\n",
            "\n",
            "Epoch 21/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3686, Acc: 0.9284\n",
            "  Val   - Loss: 0.4692, Acc: 0.8802, F1: 0.8803\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 13.6s, Total: 298.4s\n",
            "‚úÖ Best model saved: F1=0.8803, Acc=0.8802\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4407, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4185, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5227, Acc: 0.8852, Time: 13.0s\n",
            "\n",
            "Epoch 22/40 Complete \n",
            "  Train - Loss: 0.3788, Acc: 0.9213\n",
            "  Val   - Loss: 0.4835, Acc: 0.8718, F1: 0.8723\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 13.7s, Total: 312.8s\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5171, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4760, Acc: 0.8852, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3604, Acc: 0.9180, Time: 12.7s\n",
            "\n",
            "Epoch 23/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3750, Acc: 0.9260\n",
            "  Val   - Loss: 0.4366, Acc: 0.9038, F1: 0.9036\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 13.3s, Total: 326.2s\n",
            "‚úÖ Best model saved: F1=0.9036, Acc=0.9038\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3614, Acc: 0.8852, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3712, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3770, Acc: 0.9344, Time: 12.5s\n",
            "\n",
            "Epoch 24/40 Complete \n",
            "  Train - Loss: 0.3524, Acc: 0.9320\n",
            "  Val   - Loss: 0.4217, Acc: 0.9012, F1: 0.9013\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 13.3s, Total: 340.2s\n",
            "\n",
            "Epoch 25/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3980, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3919, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3375, Acc: 0.9344, Time: 12.6s\n",
            "\n",
            "Epoch 25/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3593, Acc: 0.9305\n",
            "  Val   - Loss: 0.4090, Acc: 0.9062, F1: 0.9060\n",
            "  LR: 0.001283\n",
            "  Epoch Time: 13.2s, Total: 353.4s\n",
            "‚úÖ Best model saved: F1=0.9060, Acc=0.9062\n",
            "\n",
            "Epoch 26/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4608, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2675, Acc: 1.0000, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3895, Acc: 0.9344, Time: 12.9s\n",
            "\n",
            "Epoch 26/40 Complete \n",
            "  Train - Loss: 0.3448, Acc: 0.9384\n",
            "  Val   - Loss: 0.4247, Acc: 0.9020, F1: 0.9020\n",
            "  LR: 0.001135\n",
            "  Epoch Time: 13.7s, Total: 367.8s\n",
            "\n",
            "Epoch 27/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4192, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3042, Acc: 0.9180, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2923, Acc: 0.9508, Time: 12.6s\n",
            "\n",
            "Epoch 27/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3472, Acc: 0.9336\n",
            "  Val   - Loss: 0.3868, Acc: 0.9217, F1: 0.9212\n",
            "  LR: 0.000993\n",
            "  Epoch Time: 13.3s, Total: 381.2s\n",
            "‚úÖ Best model saved: F1=0.9212, Acc=0.9217\n",
            "\n",
            "Epoch 28/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2999, Acc: 0.9672, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3433, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3607, Acc: 0.9180, Time: 12.8s\n",
            "\n",
            "Epoch 28/40 Complete \n",
            "  Train - Loss: 0.3368, Acc: 0.9401\n",
            "  Val   - Loss: 0.3759, Acc: 0.9217, F1: 0.9215\n",
            "  LR: 0.000857\n",
            "  Epoch Time: 13.6s, Total: 395.6s\n",
            "\n",
            "Epoch 29/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4239, Acc: 0.9508, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 0.3337, Acc: 0.9508, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3902, Acc: 0.9016, Time: 13.1s\n",
            "\n",
            "Epoch 29/40 Complete \n",
            "  Train - Loss: 0.3346, Acc: 0.9413\n",
            "  Val   - Loss: 0.3705, Acc: 0.9207, F1: 0.9206\n",
            "  LR: 0.000729\n",
            "  Epoch Time: 14.0s, Total: 409.6s\n",
            "\n",
            "Epoch 30/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3404, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3984, Acc: 0.9180, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3832, Acc: 0.9344, Time: 12.7s\n",
            "\n",
            "Epoch 30/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3245, Acc: 0.9456\n",
            "  Val   - Loss: 0.3611, Acc: 0.9263, F1: 0.9260\n",
            "  LR: 0.000609\n",
            "  Epoch Time: 13.4s, Total: 423.0s\n",
            "‚úÖ Best model saved: F1=0.9260, Acc=0.9263\n",
            "\n",
            "Epoch 31/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2461, Acc: 1.0000, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3249, Acc: 0.9344, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3153, Acc: 0.9508, Time: 12.8s\n",
            "\n",
            "Epoch 31/40 Complete \n",
            "  Train - Loss: 0.3141, Acc: 0.9484\n",
            "  Val   - Loss: 0.3659, Acc: 0.9240, F1: 0.9239\n",
            "  LR: 0.000499\n",
            "  Epoch Time: 13.6s, Total: 437.4s\n",
            "\n",
            "Epoch 32/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3042, Acc: 0.9672, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.3682, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2887, Acc: 0.9508, Time: 12.7s\n",
            "\n",
            "Epoch 32/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3120, Acc: 0.9485\n",
            "  Val   - Loss: 0.3529, Acc: 0.9297, F1: 0.9297\n",
            "  LR: 0.000398\n",
            "  Epoch Time: 13.4s, Total: 450.7s\n",
            "‚úÖ Best model saved: F1=0.9297, Acc=0.9297\n",
            "\n",
            "Epoch 33/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2472, Acc: 0.9672, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2471, Acc: 0.9836, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3128, Acc: 0.9344, Time: 12.8s\n",
            "\n",
            "Epoch 33/40 Complete \n",
            "  Train - Loss: 0.3109, Acc: 0.9489\n",
            "  Val   - Loss: 0.3504, Acc: 0.9290, F1: 0.9288\n",
            "  LR: 0.000307\n",
            "  Epoch Time: 13.5s, Total: 465.0s\n",
            "\n",
            "Epoch 34/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.2826, Acc: 0.9836, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2819, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2379, Acc: 0.9836, Time: 12.6s\n",
            "\n",
            "Epoch 34/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.3068, Acc: 0.9524\n",
            "  Val   - Loss: 0.3333, Acc: 0.9373, F1: 0.9370\n",
            "  LR: 0.000227\n",
            "  Epoch Time: 13.3s, Total: 478.3s\n",
            "‚úÖ Best model saved: F1=0.9370, Acc=0.9373\n",
            "\n",
            "Epoch 35/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4217, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4987, Acc: 0.8689, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3493, Acc: 0.9016, Time: 12.7s\n",
            "\n",
            "Epoch 35/40 Complete \n",
            "  Train - Loss: 0.3016, Acc: 0.9555\n",
            "  Val   - Loss: 0.3378, Acc: 0.9363, F1: 0.9359\n",
            "  LR: 0.000159\n",
            "  Epoch Time: 13.5s, Total: 492.4s\n",
            "\n",
            "Epoch 36/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3417, Acc: 0.9672, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3303, Acc: 0.9016, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2921, Acc: 0.9508, Time: 12.7s\n",
            "\n",
            "Epoch 36/40 Complete \n",
            "  Train - Loss: 0.2970, Acc: 0.9557\n",
            "  Val   - Loss: 0.3347, Acc: 0.9380, F1: 0.9378\n",
            "  LR: 0.000103\n",
            "  Epoch Time: 13.4s, Total: 505.9s\n",
            "\n",
            "Epoch 37/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3572, Acc: 0.9180, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.2833, Acc: 0.9508, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3636, Acc: 0.9016, Time: 12.7s\n",
            "\n",
            "Epoch 37/40 Complete \n",
            "  Train - Loss: 0.2993, Acc: 0.9527\n",
            "  Val   - Loss: 0.3482, Acc: 0.9330, F1: 0.9327\n",
            "  LR: 0.000058\n",
            "  Epoch Time: 13.4s, Total: 519.3s\n",
            "\n",
            "Epoch 38/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3170, Acc: 0.9180, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3917, Acc: 0.9180, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3409, Acc: 0.9016, Time: 12.6s\n",
            "\n",
            "Epoch 38/40 Complete \n",
            "  Train - Loss: 0.3013, Acc: 0.9529\n",
            "  Val   - Loss: 0.3325, Acc: 0.9373, F1: 0.9372\n",
            "  LR: 0.000027\n",
            "  Epoch Time: 13.3s, Total: 532.6s\n",
            "Early stopping at epoch 38\n",
            "Total training time: 532.6s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating resnet50_fold_2...\n",
            "Evaluation Results for resnet50_fold_2:\n",
            "  Accuracy: 0.9373\n",
            "  F1-Macro: 0.9371\n",
            "  F1-Weighted: 0.9371\n",
            "  Misclassified: 251/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_2_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_2_training_data.pkl\n",
            "\n",
            "Training Summary for resnet50_fold_2:\n",
            "  Final Accuracy: 0.9373\n",
            "  Final F1 Score: 0.9371\n",
            "  Best Validation F1: 0.9370\n",
            "  Total Training Time: 540.0s\n",
            "  Final Epoch: 38/40\n",
            "\n",
            "Evaluating resnet50_fold_2...\n",
            "Evaluation Results for resnet50_fold_2:\n",
            "  Accuracy: 0.9903\n",
            "  F1-Macro: 0.9903\n",
            "  F1-Weighted: 0.9903\n",
            "  Misclassified: 29/3000\n",
            "Fold 2 completed - Acc: 0.9903, F1: 0.9903, Time: 540.8s\n",
            "\n",
            "Training Fold 3/3\n",
            "\n",
            "Training resnet50_fold_3\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 61\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.4784, Acc: 0.3607, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 1.4701, Acc: 0.3934, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 2.4951, Acc: 0.5246, Time: 12.3s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.5854, Acc: 0.3860\n",
            "  Val   - Loss: 2.0726, Acc: 0.4650, F1: 0.4467\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 13.0s, Total: 13.0s\n",
            "‚úÖ Best model saved: F1=0.4467, Acc=0.4650\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.2733, Acc: 0.5082, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 1.5472, Acc: 0.4590, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.3028, Acc: 0.4098, Time: 12.2s\n",
            "\n",
            "Epoch 2/40 Complete \n",
            "  Train - Loss: 1.5065, Acc: 0.4331\n",
            "  Val   - Loss: 1.3576, Acc: 0.3725, F1: 0.3016\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 12.9s, Total: 26.5s\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.4515, Acc: 0.4262, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 1.3671, Acc: 0.4262, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.5069, Acc: 0.4098, Time: 12.3s\n",
            "\n",
            "Epoch 3/40 Complete \n",
            "  Train - Loss: 1.4752, Acc: 0.4430\n",
            "  Val   - Loss: 1.5938, Acc: 0.4440, F1: 0.3696\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 12.9s, Total: 39.4s\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.5096, Acc: 0.4590, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 1.4317, Acc: 0.3934, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.0260, Acc: 0.6557, Time: 12.3s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.4269, Acc: 0.4680\n",
            "  Val   - Loss: 1.0609, Acc: 0.5857, F1: 0.5365\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 12.9s, Total: 52.3s\n",
            "‚úÖ Best model saved: F1=0.5365, Acc=0.5857\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.3649, Acc: 0.4426, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 1.2172, Acc: 0.4590, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.9528, Acc: 0.6557, Time: 12.3s\n",
            "\n",
            "Epoch 5/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.3420, Acc: 0.4910\n",
            "  Val   - Loss: 1.2030, Acc: 0.6062, F1: 0.5941\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 12.9s, Total: 66.0s\n",
            "‚úÖ Best model saved: F1=0.5941, Acc=0.6062\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.4810, Acc: 0.4590, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 1.0884, Acc: 0.5410, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.1923, Acc: 0.5082, Time: 12.3s\n",
            "\n",
            "Epoch 6/40 Complete \n",
            "  Train - Loss: 1.2976, Acc: 0.5132\n",
            "  Val   - Loss: 1.1712, Acc: 0.5545, F1: 0.5260\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 13.0s, Total: 83.5s\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.1170, Acc: 0.5738, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 1.3124, Acc: 0.5410, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.0197, Acc: 0.6393, Time: 12.3s\n",
            "\n",
            "Epoch 7/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.2382, Acc: 0.5381\n",
            "  Val   - Loss: 0.9842, Acc: 0.6800, F1: 0.6761\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 12.9s, Total: 96.4s\n",
            "‚úÖ Best model saved: F1=0.6761, Acc=0.6800\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.4375, Acc: 0.5410, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 1.3404, Acc: 0.6393, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.8251, Acc: 0.6885, Time: 12.4s\n",
            "\n",
            "Epoch 8/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.2001, Acc: 0.5786\n",
            "  Val   - Loss: 1.0983, Acc: 0.7060, F1: 0.6964\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 13.0s, Total: 110.2s\n",
            "‚úÖ Best model saved: F1=0.6964, Acc=0.7060\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.9072, Acc: 0.6230, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.9682, Acc: 0.6066, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6326, Acc: 0.8197, Time: 12.3s\n",
            "\n",
            "Epoch 9/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0701, Acc: 0.6278\n",
            "  Val   - Loss: 1.0043, Acc: 0.7705, F1: 0.7665\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 12.9s, Total: 127.6s\n",
            "‚úÖ Best model saved: F1=0.7665, Acc=0.7705\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.1943, Acc: 0.6393, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 1.1485, Acc: 0.6230, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5501, Acc: 0.8197, Time: 12.3s\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0039, Acc: 0.6519\n",
            "  Val   - Loss: 0.9996, Acc: 0.8070, F1: 0.8020\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 13.0s, Total: 141.3s\n",
            "‚úÖ Best model saved: F1=0.8020, Acc=0.8070\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.8613, Acc: 0.7213, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 1.0297, Acc: 0.6557, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.6122, Acc: 0.8361, Time: 12.3s\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9605, Acc: 0.6754\n",
            "  Val   - Loss: 0.7097, Acc: 0.8287, F1: 0.8284\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 12.9s, Total: 157.7s\n",
            "‚úÖ Best model saved: F1=0.8284, Acc=0.8287\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.0222, Acc: 0.7049, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.8625, Acc: 0.7213, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5897, Acc: 0.8197, Time: 12.3s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.9000, Acc: 0.6984\n",
            "  Val   - Loss: 0.6553, Acc: 0.8123, F1: 0.8044\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 13.0s, Total: 171.5s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7422, Acc: 0.7869, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.7315, Acc: 0.7541, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.8259, Acc: 0.8033, Time: 12.4s\n",
            "\n",
            "Epoch 13/40 Complete \n",
            "  Train - Loss: 0.9017, Acc: 0.7015\n",
            "  Val   - Loss: 0.7596, Acc: 0.7802, F1: 0.7783\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 13.0s, Total: 184.5s\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 1.0932, Acc: 0.6066, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.9414, Acc: 0.6721, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4674, Acc: 0.9016, Time: 12.3s\n",
            "\n",
            "Epoch 14/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8427, Acc: 0.7131\n",
            "  Val   - Loss: 0.5229, Acc: 0.8590, F1: 0.8569\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 13.0s, Total: 197.4s\n",
            "‚úÖ Best model saved: F1=0.8569, Acc=0.8590\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6208, Acc: 0.8197, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.6370, Acc: 0.7869, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 1.1401, Acc: 0.7869, Time: 12.3s\n",
            "\n",
            "Epoch 15/40 Complete \n",
            "  Train - Loss: 0.7996, Acc: 0.7331\n",
            "  Val   - Loss: 1.0770, Acc: 0.8205, F1: 0.8151\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 13.0s, Total: 211.4s\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7355, Acc: 0.7705, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.6304, Acc: 0.7705, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3862, Acc: 0.9344, Time: 12.3s\n",
            "\n",
            "Epoch 16/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7569, Acc: 0.7560\n",
            "  Val   - Loss: 0.4588, Acc: 0.8875, F1: 0.8855\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 13.0s, Total: 224.3s\n",
            "‚úÖ Best model saved: F1=0.8855, Acc=0.8875\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5180, Acc: 0.8689, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.7087, Acc: 0.8361, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3670, Acc: 0.9016, Time: 12.2s\n",
            "\n",
            "Epoch 17/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7263, Acc: 0.7682\n",
            "  Val   - Loss: 0.4398, Acc: 0.8958, F1: 0.8954\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 12.8s, Total: 237.9s\n",
            "‚úÖ Best model saved: F1=0.8954, Acc=0.8958\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7520, Acc: 0.7541, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.6296, Acc: 0.7869, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4754, Acc: 0.9180, Time: 12.2s\n",
            "\n",
            "Epoch 18/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6873, Acc: 0.7877\n",
            "  Val   - Loss: 0.4227, Acc: 0.9120, F1: 0.9109\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 12.9s, Total: 255.3s\n",
            "‚úÖ Best model saved: F1=0.9109, Acc=0.9120\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4572, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.6243, Acc: 0.8197, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.4949, Acc: 0.9344, Time: 12.3s\n",
            "\n",
            "Epoch 19/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6614, Acc: 0.8021\n",
            "  Val   - Loss: 0.4089, Acc: 0.9173, F1: 0.9168\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 13.0s, Total: 268.9s\n",
            "‚úÖ Best model saved: F1=0.9168, Acc=0.9173\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6790, Acc: 0.7541, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.6664, Acc: 0.8197, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.5376, Acc: 0.9016, Time: 12.3s\n",
            "\n",
            "Epoch 20/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6450, Acc: 0.8093\n",
            "  Val   - Loss: 0.4519, Acc: 0.9307, F1: 0.9303\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 12.9s, Total: 282.8s\n",
            "‚úÖ Best model saved: F1=0.9303, Acc=0.9307\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5543, Acc: 0.8852, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4679, Acc: 0.8689, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3356, Acc: 0.9672, Time: 12.4s\n",
            "\n",
            "Epoch 21/40 Complete \n",
            "  Train - Loss: 0.6160, Acc: 0.8161\n",
            "  Val   - Loss: 0.3736, Acc: 0.9260, F1: 0.9256\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 13.0s, Total: 296.6s\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.7089, Acc: 0.8197, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4694, Acc: 0.9016, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3218, Acc: 0.9508, Time: 12.3s\n",
            "\n",
            "Epoch 22/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5786, Acc: 0.8341\n",
            "  Val   - Loss: 0.3591, Acc: 0.9427, F1: 0.9425\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 12.9s, Total: 309.5s\n",
            "‚úÖ Best model saved: F1=0.9425, Acc=0.9427\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6197, Acc: 0.7869, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5378, Acc: 0.8361, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2916, Acc: 0.9672, Time: 12.3s\n",
            "\n",
            "Epoch 23/40 Complete \n",
            "  Train - Loss: 0.5586, Acc: 0.8400\n",
            "  Val   - Loss: 0.3405, Acc: 0.9425, F1: 0.9425\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 13.0s, Total: 323.2s\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.5400, Acc: 0.8361, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 0.4804, Acc: 0.8852, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2844, Acc: 0.9836, Time: 12.3s\n",
            "\n",
            "Epoch 24/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5474, Acc: 0.8505\n",
            "  Val   - Loss: 0.3238, Acc: 0.9513, F1: 0.9512\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 13.0s, Total: 336.2s\n",
            "‚úÖ Best model saved: F1=0.9512, Acc=0.9513\n",
            "\n",
            "Epoch 25/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4880, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5275, Acc: 0.8689, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3180, Acc: 0.9672, Time: 12.2s\n",
            "\n",
            "Epoch 25/40 Complete \n",
            "  Train - Loss: 0.5147, Acc: 0.8650\n",
            "  Val   - Loss: 0.3231, Acc: 0.9477, F1: 0.9477\n",
            "  LR: 0.001283\n",
            "  Epoch Time: 12.9s, Total: 349.8s\n",
            "\n",
            "Epoch 26/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3719, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.6267, Acc: 0.8033, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2996, Acc: 0.9508, Time: 12.2s\n",
            "\n",
            "Epoch 26/40 Complete \n",
            "  Train - Loss: 0.5220, Acc: 0.8665\n",
            "  Val   - Loss: 0.3141, Acc: 0.9495, F1: 0.9494\n",
            "  LR: 0.001135\n",
            "  Epoch Time: 12.9s, Total: 362.8s\n",
            "\n",
            "Epoch 27/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4375, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.5926, Acc: 0.8033, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3424, Acc: 0.9836, Time: 12.2s\n",
            "\n",
            "Epoch 27/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.5102, Acc: 0.8646\n",
            "  Val   - Loss: 0.3270, Acc: 0.9590, F1: 0.9589\n",
            "  LR: 0.000993\n",
            "  Epoch Time: 12.9s, Total: 375.6s\n",
            "‚úÖ Best model saved: F1=0.9589, Acc=0.9590\n",
            "\n",
            "Epoch 28/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4125, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4746, Acc: 0.8689, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2865, Acc: 0.9672, Time: 12.4s\n",
            "\n",
            "Epoch 28/40 Complete \n",
            "  Train - Loss: 0.4868, Acc: 0.8768\n",
            "  Val   - Loss: 0.3117, Acc: 0.9585, F1: 0.9587\n",
            "  LR: 0.000857\n",
            "  Epoch Time: 13.0s, Total: 391.0s\n",
            "\n",
            "Epoch 29/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4938, Acc: 0.9016, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5630, Acc: 0.8197, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2953, Acc: 0.9508, Time: 12.3s\n",
            "\n",
            "Epoch 29/40 Complete \n",
            "  Train - Loss: 0.4630, Acc: 0.8889\n",
            "  Val   - Loss: 0.3053, Acc: 0.9595, F1: 0.9595\n",
            "  LR: 0.000729\n",
            "  Epoch Time: 13.0s, Total: 403.9s\n",
            "\n",
            "Epoch 30/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4716, Acc: 0.8689, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5366, Acc: 0.8525, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2672, Acc: 0.9672, Time: 12.2s\n",
            "\n",
            "Epoch 30/40 Complete \n",
            "  Train - Loss: 0.4678, Acc: 0.8836\n",
            "  Val   - Loss: 0.2987, Acc: 0.9583, F1: 0.9581\n",
            "  LR: 0.000609\n",
            "  Epoch Time: 12.9s, Total: 416.8s\n",
            "\n",
            "Epoch 31/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4537, Acc: 0.8361, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5003, Acc: 0.8852, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3447, Acc: 0.9344, Time: 12.3s\n",
            "\n",
            "Epoch 31/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4555, Acc: 0.8874\n",
            "  Val   - Loss: 0.2994, Acc: 0.9627, F1: 0.9626\n",
            "  LR: 0.000499\n",
            "  Epoch Time: 13.0s, Total: 429.8s\n",
            "‚úÖ Best model saved: F1=0.9626, Acc=0.9627\n",
            "\n",
            "Epoch 32/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4428, Acc: 0.9016, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.3197, Acc: 0.9508, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.3292, Acc: 0.9672, Time: 12.3s\n",
            "\n",
            "Epoch 32/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4417, Acc: 0.8975\n",
            "  Val   - Loss: 0.2881, Acc: 0.9670, F1: 0.9669\n",
            "  LR: 0.000398\n",
            "  Epoch Time: 13.0s, Total: 443.5s\n",
            "‚úÖ Best model saved: F1=0.9669, Acc=0.9670\n",
            "\n",
            "Epoch 33/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3601, Acc: 0.9344, Time: 3.9s\n",
            "  [Train] Batch  100 - Loss: 0.4923, Acc: 0.8852, Time: 7.2s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2204, Acc: 1.0000, Time: 12.2s\n",
            "\n",
            "Epoch 33/40 Complete \n",
            "  Train - Loss: 0.4343, Acc: 0.8961\n",
            "  Val   - Loss: 0.2800, Acc: 0.9613, F1: 0.9612\n",
            "  LR: 0.000307\n",
            "  Epoch Time: 12.9s, Total: 458.2s\n",
            "\n",
            "Epoch 34/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4232, Acc: 0.9016, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5749, Acc: 0.8525, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2491, Acc: 0.9836, Time: 12.3s\n",
            "\n",
            "Epoch 34/40 Complete \n",
            "  Train - Loss: 0.4316, Acc: 0.9008\n",
            "  Val   - Loss: 0.2794, Acc: 0.9625, F1: 0.9625\n",
            "  LR: 0.000227\n",
            "  Epoch Time: 12.9s, Total: 471.2s\n",
            "\n",
            "Epoch 35/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3613, Acc: 0.9180, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4119, Acc: 0.9180, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2481, Acc: 0.9836, Time: 12.2s\n",
            "\n",
            "Epoch 35/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4286, Acc: 0.9004\n",
            "  Val   - Loss: 0.2783, Acc: 0.9680, F1: 0.9679\n",
            "  LR: 0.000159\n",
            "  Epoch Time: 12.9s, Total: 484.0s\n",
            "‚úÖ Best model saved: F1=0.9679, Acc=0.9680\n",
            "\n",
            "Epoch 36/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3393, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5715, Acc: 0.8197, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2857, Acc: 0.9672, Time: 12.3s\n",
            "\n",
            "Epoch 36/40 Complete \n",
            "  Train - Loss: 0.4309, Acc: 0.8994\n",
            "  Val   - Loss: 0.2738, Acc: 0.9673, F1: 0.9672\n",
            "  LR: 0.000103\n",
            "  Epoch Time: 12.9s, Total: 497.7s\n",
            "\n",
            "Epoch 37/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4961, Acc: 0.9016, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.4456, Acc: 0.8852, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2698, Acc: 0.9672, Time: 12.3s\n",
            "\n",
            "Epoch 37/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.4249, Acc: 0.9049\n",
            "  Val   - Loss: 0.2722, Acc: 0.9720, F1: 0.9719\n",
            "  LR: 0.000058\n",
            "  Epoch Time: 12.9s, Total: 510.6s\n",
            "‚úÖ Best model saved: F1=0.9719, Acc=0.9720\n",
            "\n",
            "Epoch 38/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.3839, Acc: 0.9344, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5057, Acc: 0.8689, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2979, Acc: 0.9508, Time: 12.3s\n",
            "\n",
            "Epoch 38/40 Complete \n",
            "  Train - Loss: 0.4199, Acc: 0.9018\n",
            "  Val   - Loss: 0.2795, Acc: 0.9688, F1: 0.9687\n",
            "  LR: 0.000027\n",
            "  Epoch Time: 13.0s, Total: 524.3s\n",
            "\n",
            "Epoch 39/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.4311, Acc: 0.8852, Time: 4.1s\n",
            "  [Train] Batch  100 - Loss: 0.3717, Acc: 0.9344, Time: 7.4s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2576, Acc: 0.9672, Time: 12.4s\n",
            "\n",
            "Epoch 39/40 Complete \n",
            "  Train - Loss: 0.4086, Acc: 0.9103\n",
            "  Val   - Loss: 0.2848, Acc: 0.9692, F1: 0.9692\n",
            "  LR: 0.000007\n",
            "  Epoch Time: 13.0s, Total: 537.3s\n",
            "\n",
            "Epoch 40/40\n",
            "Training: 12,000 samples, 132 batches, batch_size: 61\n",
            "  [Train] Batch   50 - Loss: 0.6607, Acc: 0.7869, Time: 4.0s\n",
            "  [Train] Batch  100 - Loss: 0.5184, Acc: 0.8361, Time: 7.3s\n",
            "Validation: 12,000 samples, 66 batches, batch_size: 61\n",
            "  [Val] Batch   50 - Loss: 0.2324, Acc: 0.9836, Time: 12.3s\n",
            "\n",
            "Epoch 40/40 Complete \n",
            "  Train - Loss: 0.4116, Acc: 0.9104\n",
            "  Val   - Loss: 0.2669, Acc: 0.9650, F1: 0.9649\n",
            "  LR: 0.000001\n",
            "  Epoch Time: 12.9s, Total: 550.3s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating resnet50_fold_3...\n",
            "Evaluation Results for resnet50_fold_3:\n",
            "  Accuracy: 0.9698\n",
            "  F1-Macro: 0.9697\n",
            "  F1-Weighted: 0.9697\n",
            "  Misclassified: 121/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_3_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/resnet50_fold_3_training_data.pkl\n",
            "\n",
            "Training Summary for resnet50_fold_3:\n",
            "  Final Accuracy: 0.9698\n",
            "  Final F1 Score: 0.9697\n",
            "  Best Validation F1: 0.9719\n",
            "  Total Training Time: 557.4s\n",
            "  Final Epoch: 40/40\n",
            "\n",
            "Evaluating resnet50_fold_3...\n",
            "Evaluation Results for resnet50_fold_3:\n",
            "  Accuracy: 0.9840\n",
            "  F1-Macro: 0.9840\n",
            "  F1-Weighted: 0.9840\n",
            "  Misclassified: 48/3000\n",
            "Fold 3 completed - Acc: 0.9840, F1: 0.9840, Time: 558.2s\n",
            "\n",
            "K-fold Summary for resnet50:\n",
            "  Successful folds: 3/3\n",
            "  Mean Accuracy: 0.9882 ¬± 0.0030\n",
            "  Mean F1-Score: 0.9882 ¬± 0.0030\n",
            "  Total time: 1810.5s\n",
            "  Results saved to /content/drive/MyDrive/Hilsha/kfold_results/resnet50_kfold_results.json\n",
            "K-fold validation completed for resnet50\n",
            "Model saved for ensemble: resnet50\n",
            "‚úÖ resnet50 TRAINING COMPLETED!\n",
            "\n",
            "======================================================================\n",
            "TRAINING MODEL: efficientnet_b0\n",
            "======================================================================\n",
            "Using default parameters for efficientnet_b0\n",
            "\n",
            "EFFICIENTNET_B0 TRAINING PARAMETERS:\n",
            "  dropout: 0.102891\n",
            "  label_smoothing: 0.037148\n",
            "  lr: 0.004156\n",
            "  optimizer_type: adamw\n",
            "  scheduler_type: cosine\n",
            "  batch_size: 64\n",
            "  weight_decay: 0.000002\n",
            "\n",
            "Creating data loaders for efficientnet_b0...\n",
            "Train: 9000, Val: 3000, Test: 3000\n",
            "Using optimized batch size: 64\n",
            "Data loaders created successfully\n",
            "Train: 9000, Val: 3000, Test: 3000\n",
            "\n",
            "Creating model: efficientnet_b0\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 144MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created: 4,831,873 total params, 824,325 trainable\n",
            "\n",
            "Starting main model training for efficientnet_b0\n",
            "\n",
            "Training efficientnet_b0\n",
            "Training samples: 9,000\n",
            "Validation samples: 3,000\n",
            "Total epochs: 40\n",
            "Batch size: 64\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.3198, Acc: 0.5625, Time: 39.4s\n",
            "  [Train] Batch  100 - Loss: 1.3966, Acc: 0.5469, Time: 51.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.1945, Acc: 0.5756\n",
            "  Val   - Loss: 0.5008, Acc: 0.8757, F1: 0.8737\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 120.1s, Total: 120.1s\n",
            "‚úÖ Best model saved: F1=0.8737, Acc=0.8757\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.1365, Acc: 0.5469, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 1.1099, Acc: 0.6094, Time: 26.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 2/40 Complete \n",
            "  Train - Loss: 1.0558, Acc: 0.6268\n",
            "  Val   - Loss: 0.4990, Acc: 0.8753, F1: 0.8700\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 43.6s, Total: 163.7s\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.0848, Acc: 0.6094, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 1.0742, Acc: 0.6250, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9988, Acc: 0.6456\n",
            "  Val   - Loss: 0.4600, Acc: 0.8857, F1: 0.8836\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 43.7s, Total: 207.5s\n",
            "‚úÖ Best model saved: F1=0.8836, Acc=0.8857\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8663, Acc: 0.6719, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.8275, Acc: 0.6875, Time: 26.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9652, Acc: 0.6596\n",
            "  Val   - Loss: 0.4454, Acc: 0.8923, F1: 0.8911\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 43.6s, Total: 251.2s\n",
            "‚úÖ Best model saved: F1=0.8911, Acc=0.8923\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.9013, Acc: 0.6875, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7670, Acc: 0.7656, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 5/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9415, Acc: 0.6658\n",
            "  Val   - Loss: 0.3853, Acc: 0.9323, F1: 0.9311\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 44.0s, Total: 295.4s\n",
            "‚úÖ Best model saved: F1=0.9311, Acc=0.9323\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8544, Acc: 0.7188, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.7739, Acc: 0.7812, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9222, Acc: 0.6774\n",
            "  Val   - Loss: 0.3650, Acc: 0.9450, F1: 0.9447\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 44.0s, Total: 339.5s\n",
            "‚úÖ Best model saved: F1=0.9447, Acc=0.9450\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.0140, Acc: 0.6719, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7733, Acc: 0.7188, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 7/40 Complete \n",
            "  Train - Loss: 0.8962, Acc: 0.6879\n",
            "  Val   - Loss: 0.3781, Acc: 0.9300, F1: 0.9287\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 43.7s, Total: 383.4s\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7903, Acc: 0.7812, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.8688, Acc: 0.7031, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 8/40 Complete \n",
            "  Train - Loss: 0.8904, Acc: 0.6847\n",
            "  Val   - Loss: 0.3717, Acc: 0.9360, F1: 0.9352\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 43.8s, Total: 427.2s\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7395, Acc: 0.7031, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7517, Acc: 0.7344, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 9/40 Complete \n",
            "  Train - Loss: 0.8652, Acc: 0.6934\n",
            "  Val   - Loss: 0.3600, Acc: 0.9403, F1: 0.9398\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 43.8s, Total: 471.0s\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.9453, Acc: 0.6719, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.8363, Acc: 0.7344, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8549, Acc: 0.7014\n",
            "  Val   - Loss: 0.3467, Acc: 0.9517, F1: 0.9512\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 43.7s, Total: 514.7s\n",
            "‚úÖ Best model saved: F1=0.9512, Acc=0.9517\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 1.0474, Acc: 0.6250, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.8822, Acc: 0.7031, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 11/40 Complete \n",
            "  Train - Loss: 0.8561, Acc: 0.6959\n",
            "  Val   - Loss: 0.3433, Acc: 0.9497, F1: 0.9495\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 43.7s, Total: 558.6s\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8457, Acc: 0.6875, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.6778, Acc: 0.7812, Time: 26.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 12/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8326, Acc: 0.7140\n",
            "  Val   - Loss: 0.3281, Acc: 0.9590, F1: 0.9589\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 44.0s, Total: 602.6s\n",
            "‚úÖ Best model saved: F1=0.9589, Acc=0.9590\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7611, Acc: 0.7812, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.7772, Acc: 0.7344, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 13/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8198, Acc: 0.7176\n",
            "  Val   - Loss: 0.3199, Acc: 0.9623, F1: 0.9621\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 44.1s, Total: 646.8s\n",
            "‚úÖ Best model saved: F1=0.9621, Acc=0.9623\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8191, Acc: 0.6875, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.8207, Acc: 0.7031, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.8215, Acc: 0.7171\n",
            "  Val   - Loss: 0.3216, Acc: 0.9593, F1: 0.9592\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 43.6s, Total: 690.6s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.6547, Acc: 0.7969, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7118, Acc: 0.7656, Time: 26.1s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 15/40 Complete \n",
            "  Train - Loss: 0.8148, Acc: 0.7212\n",
            "  Val   - Loss: 0.3202, Acc: 0.9613, F1: 0.9611\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 43.7s, Total: 734.3s\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8480, Acc: 0.6875, Time: 13.5s\n",
            "  [Train] Batch  100 - Loss: 0.8638, Acc: 0.6719, Time: 26.4s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 16/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8158, Acc: 0.7181\n",
            "  Val   - Loss: 0.3121, Acc: 0.9653, F1: 0.9653\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 44.0s, Total: 778.3s\n",
            "‚úÖ Best model saved: F1=0.9653, Acc=0.9653\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8572, Acc: 0.7031, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.7624, Acc: 0.7656, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 17/40 Complete \n",
            "  Train - Loss: 0.8015, Acc: 0.7279\n",
            "  Val   - Loss: 0.3138, Acc: 0.9623, F1: 0.9622\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 43.8s, Total: 822.2s\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.6890, Acc: 0.7969, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7868, Acc: 0.7188, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 18/40 Complete \n",
            "  Train - Loss: 0.7998, Acc: 0.7276\n",
            "  Val   - Loss: 0.3131, Acc: 0.9643, F1: 0.9643\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 43.8s, Total: 866.0s\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8078, Acc: 0.7188, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.6884, Acc: 0.7500, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 19/40 Complete \n",
            "  Train - Loss: 0.8042, Acc: 0.7288\n",
            "  Val   - Loss: 0.3078, Acc: 0.9660, F1: 0.9658\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 43.8s, Total: 909.8s\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8122, Acc: 0.7500, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7448, Acc: 0.7344, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 20/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7940, Acc: 0.7358\n",
            "  Val   - Loss: 0.3046, Acc: 0.9703, F1: 0.9703\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 43.8s, Total: 953.6s\n",
            "‚úÖ Best model saved: F1=0.9703, Acc=0.9703\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8469, Acc: 0.6875, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.7477, Acc: 0.7188, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 21/40 Complete \n",
            "  Train - Loss: 0.7813, Acc: 0.7348\n",
            "  Val   - Loss: 0.3052, Acc: 0.9673, F1: 0.9673\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 43.8s, Total: 997.5s\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.6414, Acc: 0.8281, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.8481, Acc: 0.7188, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 22/40 Complete \n",
            "  Train - Loss: 0.7808, Acc: 0.7349\n",
            "  Val   - Loss: 0.2990, Acc: 0.9710, F1: 0.9709\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 43.8s, Total: 1041.3s\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.8026, Acc: 0.6719, Time: 13.4s\n",
            "  [Train] Batch  100 - Loss: 0.8198, Acc: 0.8125, Time: 26.3s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 23/40 Complete \n",
            "  Train - Loss: 0.7693, Acc: 0.7410\n",
            "  Val   - Loss: 0.3008, Acc: 0.9710, F1: 0.9709\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 44.0s, Total: 1085.3s\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 9,000 samples, 141 batches, batch_size: 64\n",
            "  [Train] Batch   50 - Loss: 0.7481, Acc: 0.7500, Time: 13.3s\n",
            "  [Train] Batch  100 - Loss: 0.7500, Acc: 0.7656, Time: 26.2s\n",
            "Validation: 3,000 samples, 47 batches, batch_size: 64\n",
            "\n",
            "Epoch 24/40 Complete \n",
            "  Train - Loss: 0.7680, Acc: 0.7449\n",
            "  Val   - Loss: 0.3055, Acc: 0.9653, F1: 0.9652\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 43.8s, Total: 1129.1s\n",
            "Early stopping at epoch 24\n",
            "Total training time: 1129.1s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating efficientnet_b0...\n",
            "Evaluation Results for efficientnet_b0:\n",
            "  Accuracy: 0.9073\n",
            "  F1-Macro: 0.9065\n",
            "  F1-Weighted: 0.9065\n",
            "  Misclassified: 278/3000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_training_data.pkl\n",
            "\n",
            "Training Summary for efficientnet_b0:\n",
            "  Final Accuracy: 0.9073\n",
            "  Final F1 Score: 0.9065\n",
            "  Best Validation F1: 0.9703\n",
            "  Total Training Time: 1211.0s\n",
            "  Final Epoch: 24/40\n",
            "Main model training completed for efficientnet_b0\n",
            "\n",
            "Starting K-fold cross-validation for efficientnet_b0\n",
            "\n",
            "K-fold Cross-Validation for efficientnet_b0 (3 folds)\n",
            "\n",
            "Training Fold 1/3\n",
            "\n",
            "Training efficientnet_b0_fold_1\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 42\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7986, Acc: 0.7381, Time: 28.2s\n",
            "  [Train] Batch  100 - Loss: 0.9283, Acc: 0.5952, Time: 29.6s\n",
            "  [Train] Batch  150 - Loss: 1.2301, Acc: 0.5952, Time: 31.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 1.0650, Acc: 0.6190, Time: 60.5s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0970, Acc: 0.6239\n",
            "  Val   - Loss: 1.0340, Acc: 0.6318, F1: 0.6310\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 87.5s, Total: 87.5s\n",
            "‚úÖ Best model saved: F1=0.6310, Acc=0.6318\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.4546, Acc: 0.4048, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 1.1096, Acc: 0.6667, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.9168, Acc: 0.6905, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7929, Acc: 0.7143, Time: 8.6s\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9375, Acc: 0.6954\n",
            "  Val   - Loss: 0.9241, Acc: 0.6755, F1: 0.6715\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 10.2s, Total: 97.8s\n",
            "‚úÖ Best model saved: F1=0.6715, Acc=0.6755\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7257, Acc: 0.8095, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7697, Acc: 0.7857, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.8598, Acc: 0.7143, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8430, Acc: 0.7143, Time: 8.7s\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8978, Acc: 0.6976\n",
            "  Val   - Loss: 0.8973, Acc: 0.7040, F1: 0.7029\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 10.1s, Total: 108.0s\n",
            "‚úÖ Best model saved: F1=0.7029, Acc=0.7040\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0217, Acc: 0.5952, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 1.0105, Acc: 0.7143, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.8083, Acc: 0.7381, Time: 5.3s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8959, Acc: 0.6905, Time: 8.9s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8403, Acc: 0.7154\n",
            "  Val   - Loss: 0.8509, Acc: 0.7222, F1: 0.7221\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 10.4s, Total: 118.6s\n",
            "‚úÖ Best model saved: F1=0.7221, Acc=0.7222\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0279, Acc: 0.6905, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.9259, Acc: 0.7143, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.9970, Acc: 0.5952, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8609, Acc: 0.6190, Time: 9.1s\n",
            "\n",
            "Epoch 5/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8314, Acc: 0.7146\n",
            "  Val   - Loss: 0.7786, Acc: 0.7292, F1: 0.7306\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 10.6s, Total: 129.3s\n",
            "‚úÖ Best model saved: F1=0.7306, Acc=0.7292\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6352, Acc: 0.7857, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.8597, Acc: 0.7143, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7413, Acc: 0.7381, Time: 4.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.9073, Acc: 0.6429, Time: 8.9s\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8015, Acc: 0.7330\n",
            "  Val   - Loss: 0.7859, Acc: 0.7420, F1: 0.7405\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 10.4s, Total: 139.9s\n",
            "‚úÖ Best model saved: F1=0.7405, Acc=0.7420\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7347, Acc: 0.7143, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.9976, Acc: 0.6429, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.9330, Acc: 0.7381, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7835, Acc: 0.7143, Time: 8.8s\n",
            "\n",
            "Epoch 7/40 Complete \n",
            "  Train - Loss: 0.7859, Acc: 0.7321\n",
            "  Val   - Loss: 0.7956, Acc: 0.7268, F1: 0.7262\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 10.6s, Total: 150.6s\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7686, Acc: 0.7857, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.6728, Acc: 0.7857, Time: 3.4s\n",
            "  [Train] Batch  150 - Loss: 1.0848, Acc: 0.6429, Time: 4.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8292, Acc: 0.6905, Time: 8.7s\n",
            "\n",
            "Epoch 8/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7655, Acc: 0.7460\n",
            "  Val   - Loss: 0.7634, Acc: 0.7542, F1: 0.7528\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 10.1s, Total: 160.7s\n",
            "‚úÖ Best model saved: F1=0.7528, Acc=0.7542\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7178, Acc: 0.8095, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.7555, Acc: 0.7619, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.7029, Acc: 0.8095, Time: 5.3s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6688, Acc: 0.7857, Time: 8.9s\n",
            "\n",
            "Epoch 9/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7375, Acc: 0.7588\n",
            "  Val   - Loss: 0.7435, Acc: 0.7570, F1: 0.7559\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 10.4s, Total: 171.2s\n",
            "‚úÖ Best model saved: F1=0.7559, Acc=0.7570\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6467, Acc: 0.8095, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.6313, Acc: 0.7857, Time: 3.8s\n",
            "  [Train] Batch  150 - Loss: 0.6943, Acc: 0.8095, Time: 5.3s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7480, Acc: 0.7619, Time: 8.9s\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7421, Acc: 0.7530\n",
            "  Val   - Loss: 0.7186, Acc: 0.7742, F1: 0.7748\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 10.5s, Total: 181.8s\n",
            "‚úÖ Best model saved: F1=0.7748, Acc=0.7742\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9714, Acc: 0.6190, Time: 1.9s\n",
            "  [Train] Batch  100 - Loss: 0.6876, Acc: 0.7381, Time: 3.4s\n",
            "  [Train] Batch  150 - Loss: 0.6902, Acc: 0.7143, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6798, Acc: 0.7857, Time: 8.8s\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7108, Acc: 0.7704\n",
            "  Val   - Loss: 0.7017, Acc: 0.7833, F1: 0.7827\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 10.3s, Total: 192.2s\n",
            "‚úÖ Best model saved: F1=0.7827, Acc=0.7833\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7358, Acc: 0.7619, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.7246, Acc: 0.7619, Time: 3.4s\n",
            "  [Train] Batch  150 - Loss: 0.6376, Acc: 0.7857, Time: 4.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8423, Acc: 0.7143, Time: 8.7s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.7128, Acc: 0.7690\n",
            "  Val   - Loss: 0.7521, Acc: 0.7482, F1: 0.7499\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 10.1s, Total: 202.5s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7393, Acc: 0.7143, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.5720, Acc: 0.7857, Time: 3.4s\n",
            "  [Train] Batch  150 - Loss: 0.7431, Acc: 0.7619, Time: 4.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7788, Acc: 0.7619, Time: 8.5s\n",
            "\n",
            "Epoch 13/40 Complete \n",
            "  Train - Loss: 0.7113, Acc: 0.7712\n",
            "  Val   - Loss: 0.7284, Acc: 0.7658, F1: 0.7640\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 10.0s, Total: 212.5s\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7169, Acc: 0.8095, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.8068, Acc: 0.7619, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7994, Acc: 0.7381, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.5699, Acc: 0.8333, Time: 8.7s\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.7087, Acc: 0.7724\n",
            "  Val   - Loss: 0.7053, Acc: 0.7760, F1: 0.7731\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 10.2s, Total: 222.7s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6771, Acc: 0.8095, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6060, Acc: 0.8333, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.8841, Acc: 0.6905, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7994, Acc: 0.7381, Time: 8.8s\n",
            "\n",
            "Epoch 15/40 Complete \n",
            "  Train - Loss: 0.6975, Acc: 0.7740\n",
            "  Val   - Loss: 0.7014, Acc: 0.7722, F1: 0.7743\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 10.2s, Total: 232.9s\n",
            "Early stopping at epoch 15\n",
            "Total training time: 232.9s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating efficientnet_b0_fold_1...\n",
            "Evaluation Results for efficientnet_b0_fold_1:\n",
            "  Accuracy: 0.7080\n",
            "  F1-Macro: 0.7081\n",
            "  F1-Weighted: 0.7075\n",
            "  Misclassified: 1168/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_1_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_1_training_data.pkl\n",
            "\n",
            "Training Summary for efficientnet_b0_fold_1:\n",
            "  Final Accuracy: 0.7080\n",
            "  Final F1 Score: 0.7081\n",
            "  Best Validation F1: 0.7827\n",
            "  Total Training Time: 309.8s\n",
            "  Final Epoch: 15/40\n",
            "\n",
            "Evaluating efficientnet_b0_fold_1...\n",
            "Evaluation Results for efficientnet_b0_fold_1:\n",
            "  Accuracy: 0.8997\n",
            "  F1-Macro: 0.8988\n",
            "  F1-Weighted: 0.8988\n",
            "  Misclassified: 301/3000\n",
            "Fold 1 completed - Acc: 0.8997, F1: 0.8988, Time: 310.3s\n",
            "\n",
            "Training Fold 2/3\n",
            "\n",
            "Training efficientnet_b0_fold_2\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 42\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.4132, Acc: 0.4524, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 1.2156, Acc: 0.6190, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 1.1172, Acc: 0.6429, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 1.0505, Acc: 0.7143, Time: 9.0s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.1227, Acc: 0.6162\n",
            "  Val   - Loss: 0.9548, Acc: 0.6795, F1: 0.6761\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 10.4s, Total: 10.4s\n",
            "‚úÖ Best model saved: F1=0.6761, Acc=0.6795\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9029, Acc: 0.6905, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7062, Acc: 0.7619, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.8645, Acc: 0.7143, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 1.1407, Acc: 0.6429, Time: 9.1s\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9302, Acc: 0.6879\n",
            "  Val   - Loss: 0.9199, Acc: 0.6895, F1: 0.6885\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 10.7s, Total: 21.2s\n",
            "‚úÖ Best model saved: F1=0.6885, Acc=0.6895\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8384, Acc: 0.7381, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.8194, Acc: 0.6429, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.9869, Acc: 0.6905, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.9101, Acc: 0.7381, Time: 9.0s\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8845, Acc: 0.6973\n",
            "  Val   - Loss: 0.8978, Acc: 0.6975, F1: 0.6976\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 10.4s, Total: 31.7s\n",
            "‚úÖ Best model saved: F1=0.6976, Acc=0.6975\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.2565, Acc: 0.6190, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.9988, Acc: 0.6667, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.8363, Acc: 0.7381, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8298, Acc: 0.7381, Time: 8.9s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8647, Acc: 0.7106\n",
            "  Val   - Loss: 0.8647, Acc: 0.7320, F1: 0.7320\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 10.4s, Total: 42.3s\n",
            "‚úÖ Best model saved: F1=0.7320, Acc=0.7320\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0616, Acc: 0.7143, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.8446, Acc: 0.7143, Time: 3.8s\n",
            "  [Train] Batch  150 - Loss: 0.8507, Acc: 0.7143, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8393, Acc: 0.6429, Time: 9.0s\n",
            "\n",
            "Epoch 5/40 Complete \n",
            "  Train - Loss: 0.8368, Acc: 0.7167\n",
            "  Val   - Loss: 0.8230, Acc: 0.6973, F1: 0.7059\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 10.6s, Total: 53.0s\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6435, Acc: 0.7857, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.7714, Acc: 0.7857, Time: 3.8s\n",
            "  [Train] Batch  150 - Loss: 0.7969, Acc: 0.7143, Time: 5.4s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7333, Acc: 0.7857, Time: 9.1s\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7882, Acc: 0.7431\n",
            "  Val   - Loss: 0.8090, Acc: 0.7350, F1: 0.7341\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 10.6s, Total: 63.6s\n",
            "‚úÖ Best model saved: F1=0.7341, Acc=0.7350\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7828, Acc: 0.6905, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.8666, Acc: 0.7143, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.8783, Acc: 0.7381, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8042, Acc: 0.6667, Time: 9.0s\n",
            "\n",
            "Epoch 7/40 Complete \n",
            "  Train - Loss: 0.7847, Acc: 0.7375\n",
            "  Val   - Loss: 0.8118, Acc: 0.7100, F1: 0.7110\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 10.6s, Total: 74.3s\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6812, Acc: 0.7619, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.9364, Acc: 0.7143, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.5989, Acc: 0.8095, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7922, Acc: 0.7381, Time: 9.0s\n",
            "\n",
            "Epoch 8/40 Complete \n",
            "  Train - Loss: 0.7690, Acc: 0.7446\n",
            "  Val   - Loss: 0.7864, Acc: 0.7282, F1: 0.7293\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 10.6s, Total: 84.8s\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9144, Acc: 0.7381, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.9574, Acc: 0.6190, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 1.0320, Acc: 0.5714, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8135, Acc: 0.6667, Time: 9.1s\n",
            "\n",
            "Epoch 9/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7475, Acc: 0.7521\n",
            "  Val   - Loss: 0.7502, Acc: 0.7522, F1: 0.7493\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 10.7s, Total: 95.6s\n",
            "‚úÖ Best model saved: F1=0.7493, Acc=0.7522\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.5860, Acc: 0.8333, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6849, Acc: 0.7857, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.6780, Acc: 0.8333, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6663, Acc: 0.7857, Time: 8.8s\n",
            "\n",
            "Epoch 10/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7320, Acc: 0.7602\n",
            "  Val   - Loss: 0.7410, Acc: 0.7542, F1: 0.7513\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 10.5s, Total: 106.2s\n",
            "‚úÖ Best model saved: F1=0.7513, Acc=0.7542\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7662, Acc: 0.7143, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7916, Acc: 0.7143, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.7553, Acc: 0.8095, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7777, Acc: 0.6667, Time: 8.9s\n",
            "\n",
            "Epoch 11/40 Complete \n",
            "  Train - Loss: 0.7282, Acc: 0.7642\n",
            "  Val   - Loss: 0.7609, Acc: 0.7525, F1: 0.7517\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 10.4s, Total: 116.7s\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6149, Acc: 0.8095, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.7983, Acc: 0.7143, Time: 3.8s\n",
            "  [Train] Batch  150 - Loss: 0.8168, Acc: 0.7143, Time: 5.3s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8660, Acc: 0.6905, Time: 9.1s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.7120, Acc: 0.7691\n",
            "  Val   - Loss: 0.7401, Acc: 0.7535, F1: 0.7499\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 10.6s, Total: 127.3s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7081, Acc: 0.8333, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.5614, Acc: 0.8571, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 1.1268, Acc: 0.5952, Time: 5.4s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7916, Acc: 0.7619, Time: 9.1s\n",
            "\n",
            "Epoch 13/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.7179, Acc: 0.7724\n",
            "  Val   - Loss: 0.7228, Acc: 0.7692, F1: 0.7671\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 10.7s, Total: 138.0s\n",
            "‚úÖ Best model saved: F1=0.7671, Acc=0.7692\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6761, Acc: 0.8095, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7578, Acc: 0.8095, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7459, Acc: 0.7619, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7789, Acc: 0.7381, Time: 8.9s\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.7030, Acc: 0.7795\n",
            "  Val   - Loss: 0.7161, Acc: 0.7648, F1: 0.7602\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 10.3s, Total: 148.5s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6351, Acc: 0.8333, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.7859, Acc: 0.7857, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7537, Acc: 0.7857, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8758, Acc: 0.6905, Time: 9.0s\n",
            "\n",
            "Epoch 15/40 Complete \n",
            "  Train - Loss: 0.7000, Acc: 0.7759\n",
            "  Val   - Loss: 0.7232, Acc: 0.7670, F1: 0.7644\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 10.5s, Total: 159.0s\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7432, Acc: 0.7619, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6032, Acc: 0.8095, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.5451, Acc: 0.8333, Time: 5.1s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7481, Acc: 0.7619, Time: 8.8s\n",
            "\n",
            "Epoch 16/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6906, Acc: 0.7798\n",
            "  Val   - Loss: 0.7083, Acc: 0.7735, F1: 0.7756\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 10.3s, Total: 169.4s\n",
            "‚úÖ Best model saved: F1=0.7756, Acc=0.7735\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7480, Acc: 0.7143, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.5357, Acc: 0.8810, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.5903, Acc: 0.8333, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8696, Acc: 0.7381, Time: 8.9s\n",
            "\n",
            "Epoch 17/40 Complete \n",
            "  Train - Loss: 0.6811, Acc: 0.7844\n",
            "  Val   - Loss: 0.7265, Acc: 0.7628, F1: 0.7642\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 10.4s, Total: 179.9s\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7041, Acc: 0.7619, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7310, Acc: 0.8095, Time: 3.8s\n",
            "  [Train] Batch  150 - Loss: 0.5724, Acc: 0.8571, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7993, Acc: 0.7381, Time: 9.1s\n",
            "\n",
            "Epoch 18/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6823, Acc: 0.7865\n",
            "  Val   - Loss: 0.6778, Acc: 0.7853, F1: 0.7831\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 10.6s, Total: 190.5s\n",
            "‚úÖ Best model saved: F1=0.7831, Acc=0.7853\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.5767, Acc: 0.8095, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.6750, Acc: 0.8333, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.5971, Acc: 0.8571, Time: 5.4s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7165, Acc: 0.7857, Time: 9.1s\n",
            "\n",
            "Epoch 19/40 Complete \n",
            "  Train - Loss: 0.6794, Acc: 0.7876\n",
            "  Val   - Loss: 0.7010, Acc: 0.7840, F1: 0.7825\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 10.6s, Total: 201.2s\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8783, Acc: 0.6667, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6680, Acc: 0.8095, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.5555, Acc: 0.8333, Time: 5.3s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6856, Acc: 0.8095, Time: 9.1s\n",
            "\n",
            "Epoch 20/40 Complete \n",
            "  Train - Loss: 0.6745, Acc: 0.7876\n",
            "  Val   - Loss: 0.6847, Acc: 0.7758, F1: 0.7755\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 10.7s, Total: 211.8s\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7201, Acc: 0.7857, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.7484, Acc: 0.7619, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7964, Acc: 0.7143, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7374, Acc: 0.7143, Time: 8.9s\n",
            "\n",
            "Epoch 21/40 Complete \n",
            "  Train - Loss: 0.6661, Acc: 0.7929\n",
            "  Val   - Loss: 0.6967, Acc: 0.7760, F1: 0.7757\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 10.5s, Total: 222.3s\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.5699, Acc: 0.8333, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.4696, Acc: 0.9048, Time: 3.9s\n",
            "  [Train] Batch  150 - Loss: 0.7179, Acc: 0.7857, Time: 5.5s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.8074, Acc: 0.6905, Time: 9.2s\n",
            "\n",
            "Epoch 22/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6755, Acc: 0.7834\n",
            "  Val   - Loss: 0.6798, Acc: 0.7937, F1: 0.7927\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 10.7s, Total: 233.1s\n",
            "‚úÖ Best model saved: F1=0.7927, Acc=0.7937\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7439, Acc: 0.7143, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.6520, Acc: 0.8095, Time: 3.5s\n",
            "  [Train] Batch  150 - Loss: 0.7233, Acc: 0.6905, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6691, Acc: 0.8095, Time: 8.8s\n",
            "\n",
            "Epoch 23/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.6631, Acc: 0.7894\n",
            "  Val   - Loss: 0.6713, Acc: 0.7947, F1: 0.7949\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 10.2s, Total: 243.4s\n",
            "‚úÖ Best model saved: F1=0.7949, Acc=0.7947\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.5560, Acc: 0.8333, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.5220, Acc: 0.8095, Time: 3.9s\n",
            "  [Train] Batch  150 - Loss: 0.6314, Acc: 0.7857, Time: 5.4s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6163, Acc: 0.7857, Time: 9.1s\n",
            "\n",
            "Epoch 24/40 Complete \n",
            "  Train - Loss: 0.6515, Acc: 0.7991\n",
            "  Val   - Loss: 0.6825, Acc: 0.7910, F1: 0.7895\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 10.6s, Total: 254.1s\n",
            "\n",
            "Epoch 25/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6867, Acc: 0.8095, Time: 2.0s\n",
            "  [Train] Batch  100 - Loss: 0.5617, Acc: 0.8095, Time: 3.7s\n",
            "  [Train] Batch  150 - Loss: 0.5992, Acc: 0.8095, Time: 5.4s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7819, Acc: 0.7381, Time: 9.1s\n",
            "\n",
            "Epoch 25/40 Complete \n",
            "  Train - Loss: 0.6482, Acc: 0.7975\n",
            "  Val   - Loss: 0.6900, Acc: 0.7867, F1: 0.7849\n",
            "  LR: 0.001283\n",
            "  Epoch Time: 10.6s, Total: 264.7s\n",
            "\n",
            "Epoch 26/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.6154, Acc: 0.8333, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6115, Acc: 0.8095, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.5330, Acc: 0.8333, Time: 5.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.7364, Acc: 0.7619, Time: 8.9s\n",
            "\n",
            "Epoch 26/40 Complete \n",
            "  Train - Loss: 0.6437, Acc: 0.8017\n",
            "  Val   - Loss: 0.6844, Acc: 0.7893, F1: 0.7883\n",
            "  LR: 0.001135\n",
            "  Epoch Time: 10.4s, Total: 275.1s\n",
            "\n",
            "Epoch 27/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8156, Acc: 0.6429, Time: 2.1s\n",
            "  [Train] Batch  100 - Loss: 0.6628, Acc: 0.7857, Time: 3.6s\n",
            "  [Train] Batch  150 - Loss: 0.5163, Acc: 0.9048, Time: 5.2s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6362, Acc: 0.8333, Time: 9.0s\n",
            "\n",
            "Epoch 27/40 Complete \n",
            "  Train - Loss: 0.6470, Acc: 0.8000\n",
            "  Val   - Loss: 0.6832, Acc: 0.7845, F1: 0.7822\n",
            "  LR: 0.000993\n",
            "  Epoch Time: 10.7s, Total: 285.8s\n",
            "Early stopping at epoch 27\n",
            "Total training time: 285.8s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating efficientnet_b0_fold_2...\n",
            "Evaluation Results for efficientnet_b0_fold_2:\n",
            "  Accuracy: 0.7202\n",
            "  F1-Macro: 0.7193\n",
            "  F1-Weighted: 0.7201\n",
            "  Misclassified: 1119/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_2_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_2_training_data.pkl\n",
            "\n",
            "Training Summary for efficientnet_b0_fold_2:\n",
            "  Final Accuracy: 0.7202\n",
            "  Final F1 Score: 0.7193\n",
            "  Best Validation F1: 0.7949\n",
            "  Total Training Time: 290.7s\n",
            "  Final Epoch: 27/40\n",
            "\n",
            "Evaluating efficientnet_b0_fold_2...\n",
            "Evaluation Results for efficientnet_b0_fold_2:\n",
            "  Accuracy: 0.9153\n",
            "  F1-Macro: 0.9144\n",
            "  F1-Weighted: 0.9144\n",
            "  Misclassified: 254/3000\n",
            "Fold 2 completed - Acc: 0.9153, F1: 0.9144, Time: 291.2s\n",
            "\n",
            "Training Fold 3/3\n",
            "\n",
            "Training efficientnet_b0_fold_3\n",
            "Training samples: 12,000\n",
            "Validation samples: 12,000\n",
            "Total epochs: 40\n",
            "Batch size: 42\n",
            "\n",
            "Epoch 1/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0046, Acc: 0.6667, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 1.3905, Acc: 0.3810, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 1.1104, Acc: 0.5714, Time: 5.7s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6477, Acc: 0.7619, Time: 9.0s\n",
            "\n",
            "Epoch 1/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.2586, Acc: 0.5536\n",
            "  Val   - Loss: 0.6258, Acc: 0.8167, F1: 0.8149\n",
            "  LR: 0.004149\n",
            "  Epoch Time: 10.1s, Total: 10.1s\n",
            "‚úÖ Best model saved: F1=0.8149, Acc=0.8167\n",
            "\n",
            "Epoch 2/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9905, Acc: 0.6190, Time: 2.5s\n",
            "  [Train] Batch  100 - Loss: 0.9313, Acc: 0.5714, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 1.4464, Acc: 0.4762, Time: 5.7s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6297, Acc: 0.8333, Time: 9.1s\n",
            "\n",
            "Epoch 2/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0972, Acc: 0.6042\n",
            "  Val   - Loss: 0.5565, Acc: 0.8472, F1: 0.8462\n",
            "  LR: 0.004130\n",
            "  Epoch Time: 10.1s, Total: 20.4s\n",
            "‚úÖ Best model saved: F1=0.8462, Acc=0.8472\n",
            "\n",
            "Epoch 3/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9657, Acc: 0.6190, Time: 2.5s\n",
            "  [Train] Batch  100 - Loss: 0.9787, Acc: 0.7143, Time: 4.4s\n",
            "  [Train] Batch  150 - Loss: 1.0366, Acc: 0.5714, Time: 6.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.5631, Acc: 0.8571, Time: 9.5s\n",
            "\n",
            "Epoch 3/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0539, Acc: 0.6214\n",
            "  Val   - Loss: 0.5550, Acc: 0.8530, F1: 0.8521\n",
            "  LR: 0.004098\n",
            "  Epoch Time: 10.5s, Total: 31.0s\n",
            "‚úÖ Best model saved: F1=0.8521, Acc=0.8530\n",
            "\n",
            "Epoch 4/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.1520, Acc: 0.5952, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 1.0564, Acc: 0.5952, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8958, Acc: 0.6905, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3773, Acc: 0.9286, Time: 9.2s\n",
            "\n",
            "Epoch 4/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 1.0128, Acc: 0.6395\n",
            "  Val   - Loss: 0.4928, Acc: 0.8770, F1: 0.8762\n",
            "  LR: 0.004054\n",
            "  Epoch Time: 10.2s, Total: 41.6s\n",
            "‚úÖ Best model saved: F1=0.8762, Acc=0.8770\n",
            "\n",
            "Epoch 5/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9781, Acc: 0.5714, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 0.9345, Acc: 0.7143, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 1.0184, Acc: 0.6429, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6442, Acc: 0.8333, Time: 9.4s\n",
            "\n",
            "Epoch 5/40 Complete \n",
            "  Train - Loss: 0.9723, Acc: 0.6519\n",
            "  Val   - Loss: 0.4954, Acc: 0.8765, F1: 0.8761\n",
            "  LR: 0.003998\n",
            "  Epoch Time: 10.6s, Total: 52.3s\n",
            "\n",
            "Epoch 6/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9844, Acc: 0.6190, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 1.0421, Acc: 0.6429, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 1.0309, Acc: 0.6905, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.5541, Acc: 0.8571, Time: 9.4s\n",
            "\n",
            "Epoch 6/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9531, Acc: 0.6630\n",
            "  Val   - Loss: 0.4659, Acc: 0.8882, F1: 0.8868\n",
            "  LR: 0.003929\n",
            "  Epoch Time: 10.4s, Total: 62.7s\n",
            "‚úÖ Best model saved: F1=0.8868, Acc=0.8882\n",
            "\n",
            "Epoch 7/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7602, Acc: 0.7143, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 0.8989, Acc: 0.7143, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.9192, Acc: 0.6667, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4570, Acc: 0.8810, Time: 9.2s\n",
            "\n",
            "Epoch 7/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.9332, Acc: 0.6656\n",
            "  Val   - Loss: 0.4672, Acc: 0.8935, F1: 0.8930\n",
            "  LR: 0.003850\n",
            "  Epoch Time: 10.3s, Total: 73.2s\n",
            "‚úÖ Best model saved: F1=0.8930, Acc=0.8935\n",
            "\n",
            "Epoch 8/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.1418, Acc: 0.5238, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 0.8272, Acc: 0.7381, Time: 4.2s\n",
            "  [Train] Batch  150 - Loss: 0.8983, Acc: 0.6429, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.6466, Acc: 0.7619, Time: 9.3s\n",
            "\n",
            "Epoch 8/40 Complete \n",
            "  Train - Loss: 0.9139, Acc: 0.6763\n",
            "  Val   - Loss: 0.4594, Acc: 0.8940, F1: 0.8937\n",
            "  LR: 0.003759\n",
            "  Epoch Time: 10.4s, Total: 83.7s\n",
            "\n",
            "Epoch 9/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8327, Acc: 0.6905, Time: 2.5s\n",
            "  [Train] Batch  100 - Loss: 0.9637, Acc: 0.6429, Time: 4.4s\n",
            "  [Train] Batch  150 - Loss: 0.7202, Acc: 0.7381, Time: 6.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.5418, Acc: 0.8571, Time: 9.3s\n",
            "\n",
            "Epoch 9/40 Complete \n",
            "  Train - Loss: 0.9031, Acc: 0.6790\n",
            "  Val   - Loss: 0.4502, Acc: 0.8952, F1: 0.8929\n",
            "  LR: 0.003658\n",
            "  Epoch Time: 10.4s, Total: 94.1s\n",
            "\n",
            "Epoch 10/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8441, Acc: 0.6667, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 1.0090, Acc: 0.6667, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8968, Acc: 0.6667, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4027, Acc: 0.9286, Time: 9.4s\n",
            "\n",
            "Epoch 10/40 Complete \n",
            "  Train - Loss: 0.8881, Acc: 0.6860\n",
            "  Val   - Loss: 0.4522, Acc: 0.8915, F1: 0.8898\n",
            "  LR: 0.003547\n",
            "  Epoch Time: 10.5s, Total: 104.6s\n",
            "\n",
            "Epoch 11/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0320, Acc: 0.6190, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.9979, Acc: 0.6667, Time: 4.0s\n",
            "  [Train] Batch  150 - Loss: 0.8269, Acc: 0.7143, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3328, Acc: 0.9762, Time: 9.3s\n",
            "\n",
            "Epoch 11/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8745, Acc: 0.6921\n",
            "  Val   - Loss: 0.4229, Acc: 0.9153, F1: 0.9146\n",
            "  LR: 0.003427\n",
            "  Epoch Time: 10.4s, Total: 115.0s\n",
            "‚úÖ Best model saved: F1=0.9146, Acc=0.9153\n",
            "\n",
            "Epoch 12/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9100, Acc: 0.5952, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.9586, Acc: 0.6429, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.9986, Acc: 0.7143, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3089, Acc: 0.9762, Time: 9.3s\n",
            "\n",
            "Epoch 12/40 Complete \n",
            "  Train - Loss: 0.8609, Acc: 0.6969\n",
            "  Val   - Loss: 0.4207, Acc: 0.9105, F1: 0.9101\n",
            "  LR: 0.003299\n",
            "  Epoch Time: 10.3s, Total: 125.4s\n",
            "\n",
            "Epoch 13/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7564, Acc: 0.7619, Time: 2.2s\n",
            "  [Train] Batch  100 - Loss: 0.8077, Acc: 0.7143, Time: 4.0s\n",
            "  [Train] Batch  150 - Loss: 0.8188, Acc: 0.6905, Time: 5.6s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4098, Acc: 0.9286, Time: 9.0s\n",
            "\n",
            "Epoch 13/40 Complete \n",
            "  Train - Loss: 0.8653, Acc: 0.6960\n",
            "  Val   - Loss: 0.4231, Acc: 0.9093, F1: 0.9090\n",
            "  LR: 0.003164\n",
            "  Epoch Time: 10.0s, Total: 135.5s\n",
            "\n",
            "Epoch 14/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9507, Acc: 0.7143, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 0.8052, Acc: 0.7619, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8725, Acc: 0.6667, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3298, Acc: 0.9524, Time: 9.1s\n",
            "\n",
            "Epoch 14/40 Complete \n",
            "  Train - Loss: 0.8643, Acc: 0.6916\n",
            "  Val   - Loss: 0.4131, Acc: 0.9165, F1: 0.9155\n",
            "  LR: 0.003021\n",
            "  Epoch Time: 10.2s, Total: 145.7s\n",
            "\n",
            "Epoch 15/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 1.0585, Acc: 0.6667, Time: 2.5s\n",
            "  [Train] Batch  100 - Loss: 0.8240, Acc: 0.7143, Time: 4.3s\n",
            "  [Train] Batch  150 - Loss: 0.6940, Acc: 0.8095, Time: 6.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3527, Acc: 0.9524, Time: 9.4s\n",
            "\n",
            "Epoch 15/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8357, Acc: 0.7137\n",
            "  Val   - Loss: 0.4076, Acc: 0.9210, F1: 0.9208\n",
            "  LR: 0.002873\n",
            "  Epoch Time: 10.6s, Total: 156.2s\n",
            "‚úÖ Best model saved: F1=0.9208, Acc=0.9210\n",
            "\n",
            "Epoch 16/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7599, Acc: 0.7143, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.9882, Acc: 0.6667, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.7729, Acc: 0.7143, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3442, Acc: 0.9524, Time: 9.3s\n",
            "\n",
            "Epoch 16/40 Complete \n",
            "  Train - Loss: 0.8386, Acc: 0.7040\n",
            "  Val   - Loss: 0.4194, Acc: 0.9150, F1: 0.9148\n",
            "  LR: 0.002720\n",
            "  Epoch Time: 10.5s, Total: 166.9s\n",
            "\n",
            "Epoch 17/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8683, Acc: 0.6905, Time: 2.4s\n",
            "  [Train] Batch  100 - Loss: 0.7435, Acc: 0.6905, Time: 4.2s\n",
            "  [Train] Batch  150 - Loss: 0.8674, Acc: 0.7619, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.5621, Acc: 0.8810, Time: 9.4s\n",
            "\n",
            "Epoch 17/40 Complete \n",
            "  Train - Loss: 0.8439, Acc: 0.7067\n",
            "  Val   - Loss: 0.4146, Acc: 0.9213, F1: 0.9207\n",
            "  LR: 0.002563\n",
            "  Epoch Time: 10.4s, Total: 177.3s\n",
            "\n",
            "Epoch 18/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9257, Acc: 0.6667, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 1.0628, Acc: 0.6190, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8070, Acc: 0.7381, Time: 5.7s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3069, Acc: 0.9762, Time: 9.2s\n",
            "\n",
            "Epoch 18/40 Complete \n",
            "  Train - Loss: 0.8412, Acc: 0.7126\n",
            "  Val   - Loss: 0.4078, Acc: 0.9210, F1: 0.9206\n",
            "  LR: 0.002403\n",
            "  Epoch Time: 10.4s, Total: 187.7s\n",
            "\n",
            "Epoch 19/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.7065, Acc: 0.7619, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.8045, Acc: 0.7381, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8165, Acc: 0.7857, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4321, Acc: 0.9048, Time: 9.3s\n",
            "\n",
            "Epoch 19/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8352, Acc: 0.7130\n",
            "  Val   - Loss: 0.4020, Acc: 0.9245, F1: 0.9243\n",
            "  LR: 0.002241\n",
            "  Epoch Time: 10.3s, Total: 198.0s\n",
            "‚úÖ Best model saved: F1=0.9243, Acc=0.9245\n",
            "\n",
            "Epoch 20/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8372, Acc: 0.7857, Time: 2.6s\n",
            "  [Train] Batch  100 - Loss: 0.6440, Acc: 0.8333, Time: 4.4s\n",
            "  [Train] Batch  150 - Loss: 0.7240, Acc: 0.7143, Time: 5.9s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4163, Acc: 0.8810, Time: 9.3s\n",
            "\n",
            "Epoch 20/40 Complete üåü NEW BEST!\n",
            "  Train - Loss: 0.8211, Acc: 0.7179\n",
            "  Val   - Loss: 0.3920, Acc: 0.9295, F1: 0.9293\n",
            "  LR: 0.002078\n",
            "  Epoch Time: 10.4s, Total: 208.6s\n",
            "‚úÖ Best model saved: F1=0.9293, Acc=0.9295\n",
            "\n",
            "Epoch 21/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9926, Acc: 0.6905, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.7720, Acc: 0.7381, Time: 4.3s\n",
            "  [Train] Batch  150 - Loss: 0.7046, Acc: 0.7619, Time: 6.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3138, Acc: 0.9286, Time: 9.4s\n",
            "\n",
            "Epoch 21/40 Complete \n",
            "  Train - Loss: 0.8046, Acc: 0.7204\n",
            "  Val   - Loss: 0.3923, Acc: 0.9255, F1: 0.9251\n",
            "  LR: 0.001915\n",
            "  Epoch Time: 10.5s, Total: 219.4s\n",
            "\n",
            "Epoch 22/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.9974, Acc: 0.6190, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.6975, Acc: 0.7381, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.8576, Acc: 0.7619, Time: 6.0s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.3346, Acc: 0.9524, Time: 9.5s\n",
            "\n",
            "Epoch 22/40 Complete \n",
            "  Train - Loss: 0.8095, Acc: 0.7214\n",
            "  Val   - Loss: 0.3985, Acc: 0.9215, F1: 0.9214\n",
            "  LR: 0.001753\n",
            "  Epoch Time: 10.6s, Total: 229.9s\n",
            "\n",
            "Epoch 23/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8909, Acc: 0.6905, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.8173, Acc: 0.7619, Time: 4.1s\n",
            "  [Train] Batch  150 - Loss: 0.7274, Acc: 0.7381, Time: 5.8s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4099, Acc: 0.9286, Time: 9.3s\n",
            "\n",
            "Epoch 23/40 Complete \n",
            "  Train - Loss: 0.8101, Acc: 0.7282\n",
            "  Val   - Loss: 0.3956, Acc: 0.9207, F1: 0.9204\n",
            "  LR: 0.001593\n",
            "  Epoch Time: 10.4s, Total: 240.3s\n",
            "\n",
            "Epoch 24/40\n",
            "Training: 12,000 samples, 191 batches, batch_size: 42\n",
            "  [Train] Batch   50 - Loss: 0.8191, Acc: 0.6905, Time: 2.3s\n",
            "  [Train] Batch  100 - Loss: 0.8275, Acc: 0.6667, Time: 4.0s\n",
            "  [Train] Batch  150 - Loss: 0.6823, Acc: 0.7381, Time: 5.7s\n",
            "Validation: 12,000 samples, 96 batches, batch_size: 42\n",
            "  [Val] Batch   50 - Loss: 0.4081, Acc: 0.9048, Time: 9.2s\n",
            "\n",
            "Epoch 24/40 Complete \n",
            "  Train - Loss: 0.8009, Acc: 0.7294\n",
            "  Val   - Loss: 0.3885, Acc: 0.9300, F1: 0.9299\n",
            "  LR: 0.001436\n",
            "  Epoch Time: 10.3s, Total: 250.6s\n",
            "Early stopping at epoch 24\n",
            "Total training time: 250.6s\n",
            "‚úÖ Loaded best model for evaluation\n",
            "\n",
            "Evaluating efficientnet_b0_fold_3...\n",
            "Evaluation Results for efficientnet_b0_fold_3:\n",
            "  Accuracy: 0.8765\n",
            "  F1-Macro: 0.8762\n",
            "  F1-Weighted: 0.8764\n",
            "  Misclassified: 494/4000\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_3_training_data.json\n",
            "‚úÖ Training data saved to /content/drive/MyDrive/Hilsha/training_results/efficientnet_b0_fold_3_training_data.pkl\n",
            "\n",
            "Training Summary for efficientnet_b0_fold_3:\n",
            "  Final Accuracy: 0.8765\n",
            "  Final F1 Score: 0.8762\n",
            "  Best Validation F1: 0.9293\n",
            "  Total Training Time: 254.8s\n",
            "  Final Epoch: 24/40\n",
            "\n",
            "Evaluating efficientnet_b0_fold_3...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Step 10: Evaluation and Plot's"
      ],
      "metadata": {
        "id": "rQ_reeX1cZz4"
      },
      "id": "rQ_reeX1cZz4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåç Real-World Data Test üîéüìä\n"
      ],
      "metadata": {
        "id": "_X2tyIMkfV_q"
      },
      "id": "_X2tyIMkfV_q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this import at the top\n",
        "# from your_model_module import ModelFactory  # Replace with your actual import\n",
        "\n",
        "# Simple Image Predictor for Google Colab\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Setup - Replace these with your values\n",
        "CLASS_NAMES = ['Class1', 'Class2', 'Class3', 'Class4', 'Class5']  # Replace with your 5 classes\n",
        "MODEL_PATH = \"/content/output/best_model/your_model_name_best.pt\"  # Replace with your model path\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load model\n",
        "def load_model():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
        "\n",
        "    # Get model info from checkpoint\n",
        "    model_name = checkpoint['model_name']\n",
        "    hyperparameters = checkpoint.get('hyperparameters', {})\n",
        "\n",
        "    # Create model using your ModelFactory\n",
        "    model = ModelFactory.create_model(\n",
        "        model_name,\n",
        "        params=hyperparameters,\n",
        "        num_classes=len(CLASS_NAMES)\n",
        "    )\n",
        "\n",
        "    # Load the trained weights\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"Loaded model: {model_name}\")\n",
        "    return model, device\n",
        "\n",
        "# Predict function\n",
        "def predict_image(image_path_or_pil):\n",
        "    model, device = load_model()\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Load and preprocess image\n",
        "    if isinstance(image_path_or_pil, str):\n",
        "        image = Image.open(image_path_or_pil)\n",
        "    else:\n",
        "        image = image_path_or_pil\n",
        "\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
        "\n",
        "    predicted_class = CLASS_NAMES[predicted_idx.item()]\n",
        "    confidence_score = confidence.item()\n",
        "\n",
        "    # Show results\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title('Input Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    probs = probabilities[0].cpu().numpy()\n",
        "    colors = ['green' if i == predicted_idx else 'skyblue' for i in range(len(CLASS_NAMES))]\n",
        "    plt.bar(CLASS_NAMES, probs, color=colors)\n",
        "    plt.title('Predictions')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"üéØ Predicted: {predicted_class}\")\n",
        "    print(f\"üìä Confidence: {confidence_score:.2%}\")\n",
        "\n",
        "    return predicted_class, confidence_score\n",
        "\n",
        "# Predict from URL\n",
        "def predict_from_url(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        return predict_image(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Predict from uploaded file\n",
        "def predict_uploaded():\n",
        "    print(\"Upload an image file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        result = predict_image(filename)\n",
        "        os.remove(filename)  # Clean up\n",
        "        return result\n",
        "    else:\n",
        "        print(\"No file uploaded\")\n",
        "\n",
        "# Example usage:\n",
        "print(\"üöÄ Simple Image Predictor Ready!\")\n",
        "print(\"\\nHow to use:\")\n",
        "print(\"1. First, replace MODEL_PATH and CLASS_NAMES above\")\n",
        "print(\"2. Fix the load_model() function with your actual model\")\n",
        "print(\"3. Then run:\")\n",
        "print(\"   predict_uploaded()  # To upload image\")\n",
        "print(\"   predict_from_url('http://example.com/image.jpg')  # To predict from URL\")\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision import transforms, models\n",
        "# from PIL import Image\n",
        "# import requests\n",
        "# from io import BytesIO\n",
        "\n",
        "# # ---------------------------\n",
        "# # CONFIG\n",
        "# # ---------------------------\n",
        "# MODEL_PATH = \"best_model.pth\"   # ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ trained model file\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ dataset ‡¶è‡¶∞ class ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü (‡¶®‡¶ø‡¶ú‡ßá‡¶∞ dataset ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶¨‡¶¶‡¶≤‡¶æ‡¶¨‡ßá)\n",
        "# CLASS_NAMES = [\"ilish\", \"chandana\", \"sardin\", \"sardinella\", \"punctatus\"]\n",
        "\n",
        "# # ---------------------------\n",
        "# # Load Model\n",
        "# # ---------------------------\n",
        "# def load_model():\n",
        "#     model = models.resnet50(weights=None)   # ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡ßã ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡¶∏‡¶æ‡¶ì\n",
        "#     num_features = model.fc.in_features\n",
        "#     model.fc = nn.Linear(num_features, len(CLASS_NAMES))\n",
        "\n",
        "#     checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "#     model.load_state_dict(checkpoint[\"model_state_dict\"])  # ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ save format ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ adjust ‡¶ï‡¶∞‡ßã\n",
        "#     model.to(DEVICE)\n",
        "#     model.eval()\n",
        "#     return model\n",
        "\n",
        "# # ---------------------------\n",
        "# # Preprocess\n",
        "# # ---------------------------\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),    # training ‡¶∏‡¶Æ‡ßü ‡¶Ø‡¶æ ‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßã, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶Æ‡ßá‡¶≤‡¶æ‡¶§‡ßá ‡¶π‡¶¨‡ßá\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406],\n",
        "#                          [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# def load_image(img_path=None, img_url=None):\n",
        "#     if img_path:\n",
        "#         image = Image.open(img_path).convert(\"RGB\")\n",
        "#     elif img_url:\n",
        "#         response = requests.get(img_url)\n",
        "#         image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "#     else:\n",
        "#         raise ValueError(\"Provide either img_path or img_url\")\n",
        "#     return image\n",
        "\n",
        "# # ---------------------------\n",
        "# # Prediction\n",
        "# # ---------------------------\n",
        "# def predict_image(model, image):\n",
        "#     img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(img_tensor)\n",
        "#         probs = torch.softmax(outputs, dim=1)\n",
        "#         conf, pred = torch.max(probs, 1)\n",
        "#     return CLASS_NAMES[pred.item()], conf.item()\n",
        "\n",
        "# # ---------------------------\n",
        "# # Main\n",
        "# # ---------------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     model = load_model()\n",
        "\n",
        "#     # Option 1: ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá\n",
        "#     img_path = \"test_fish.jpg\"\n",
        "#     image = load_image(img_path=img_path)\n",
        "#     label, confidence = predict_image(model, image)\n",
        "#     print(f\"Prediction: {label} ({confidence:.2f})\")\n",
        "\n",
        "#     # Option 2: URL ‡¶•‡ßá‡¶ï‡ßá\n",
        "#     img_url = \"https://example.com/sample_fish.jpg\"\n",
        "#     image = load_image(img_url=img_url)\n",
        "#     label, confidence = predict_image(model, image)\n",
        "#     print(f\"Prediction: {label} ({confidence:.2f})\")\n"
      ],
      "metadata": {
        "id": "kcr-u7s4phe3"
      },
      "id": "kcr-u7s4phe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cdd48bb6",
      "metadata": {
        "id": "cdd48bb6"
      },
      "source": [
        "#End"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}